{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9f3bd0d0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 1,048,575 rows × 80 columns for D:\\Canada\\Subjects\\Semester -2\\AIDI-2005-02 CAPSTONE TERM ll\\Bot_detector\\data\\processed\\Friday-16-02-2018_TrafficForML_CICFlowMeter.parquet\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "sample = \"D:\\Canada\\Subjects\\Semester -2\\AIDI-2005-02 CAPSTONE TERM ll\\Bot_detector\\data\\processed\\Friday-16-02-2018_TrafficForML_CICFlowMeter.parquet\"\n",
    "df = pd.read_parquet(sample, engine=\"pyarrow\")\n",
    "print(f\"Loaded {len(df):,} rows × {df.shape[1]} columns for {sample}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2fb9a0e6",
   "metadata": {},
   "source": [
    "Step 1: Exploratory Data Analysis (EDA)\n",
    "Goals:\n",
    "\n",
    "Understand how many classes are present in each file.\n",
    "\n",
    "Identify which files contain both Benign and attack labels.\n",
    "\n",
    "See if there’s class imbalance.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ec32ea7c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File: Friday-02-03-2018_TrafficForML_CICFlowMeter.parquet\n",
      "Label\n",
      "Benign    762384\n",
      "Attack    286191\n",
      "Name: count, dtype: int64\n",
      "----------------------------------------\n",
      "File: Friday-16-02-2018_TrafficForML_CICFlowMeter.parquet\n",
      "Label\n",
      "Attack    601803\n",
      "Benign    446772\n",
      "Name: count, dtype: int64\n",
      "----------------------------------------\n",
      "File: Friday-23-02-2018_TrafficForML_CICFlowMeter.parquet\n",
      "Label\n",
      "Benign    1048009\n",
      "Attack        566\n",
      "Name: count, dtype: int64\n",
      "----------------------------------------\n",
      "File: Thuesday-20-02-2018_TrafficForML_CICFlowMeter.parquet\n",
      "Label\n",
      "Benign    7372557\n",
      "Attack     576191\n",
      "Name: count, dtype: int64\n",
      "----------------------------------------\n",
      "File: Thursday-01-03-2018_TrafficForML_CICFlowMeter.parquet\n",
      "Label\n",
      "Benign    238037\n",
      "Attack     93088\n",
      "Name: count, dtype: int64\n",
      "----------------------------------------\n",
      "File: Thursday-15-02-2018_TrafficForML_CICFlowMeter.parquet\n",
      "Label\n",
      "Benign    996077\n",
      "Attack     52498\n",
      "Name: count, dtype: int64\n",
      "----------------------------------------\n",
      "File: Thursday-22-02-2018_TrafficForML_CICFlowMeter.parquet\n",
      "Label\n",
      "Benign    1048213\n",
      "Attack        362\n",
      "Name: count, dtype: int64\n",
      "----------------------------------------\n",
      "File: Wednesday-14-02-2018_TrafficForML_CICFlowMeter.parquet\n",
      "Label\n",
      "Benign    667626\n",
      "Attack    380949\n",
      "Name: count, dtype: int64\n",
      "----------------------------------------\n",
      "File: Wednesday-21-02-2018_TrafficForML_CICFlowMeter.parquet\n",
      "Label\n",
      "Attack    687742\n",
      "Benign    360833\n",
      "Name: count, dtype: int64\n",
      "----------------------------------------\n",
      "File: Wednesday-28-02-2018_TrafficForML_CICFlowMeter.parquet\n",
      "Label\n",
      "Benign    544200\n",
      "Attack     68904\n",
      "Name: count, dtype: int64\n",
      "----------------------------------------\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "data_dir = \"D:/Canada/Subjects/Semester -2/AIDI-2005-02 CAPSTONE TERM ll/Bot_detector/data/processed\"\n",
    "parquet_files = [f for f in os.listdir(data_dir) if f.endswith(\".parquet\")]\n",
    "\n",
    "for pf in parquet_files:\n",
    "    df = pd.read_parquet(os.path.join(data_dir, pf))\n",
    "    print(f\"File: {pf}\")\n",
    "    print(df['Label'].value_counts())\n",
    "    print(\"-\" * 40)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "faed0463",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 10 Parquet files.\n",
      "\n",
      "File: Friday-02-03-2018_TrafficForML_CICFlowMeter.parquet\n",
      "  Shape: 1,048,575 rows × 80 columns\n",
      "  Label counts:\n",
      "Label\n",
      "Benign    762384\n",
      "Bot       286191\n",
      "Name: count, dtype: int64\n",
      "  Columns with missing values:\n",
      "Flow Byts/s    2558\n",
      "dtype: int64\n",
      "------------------------------------------------------------\n",
      "File: Friday-16-02-2018_TrafficForML_CICFlowMeter.parquet\n",
      "  Shape: 1,048,575 rows × 80 columns\n",
      "  Label counts:\n",
      "Label\n",
      "DoS attacks-Hulk            461912\n",
      "Benign                      446772\n",
      "DoS attacks-SlowHTTPTest    139890\n",
      "Label                            1\n",
      "Name: count, dtype: int64\n",
      "  Columns with missing values:\n",
      "Dst Port         1\n",
      "Protocol         1\n",
      "Flow Duration    1\n",
      "Tot Fwd Pkts     1\n",
      "Tot Bwd Pkts     1\n",
      "                ..\n",
      "Active Min       1\n",
      "Idle Mean        1\n",
      "Idle Std         1\n",
      "Idle Max         1\n",
      "Idle Min         1\n",
      "Length: 78, dtype: int64\n",
      "------------------------------------------------------------\n",
      "File: Friday-23-02-2018_TrafficForML_CICFlowMeter.parquet\n",
      "  Shape: 1,048,575 rows × 80 columns\n",
      "  Label counts:\n",
      "Label\n",
      "Benign              1048009\n",
      "Brute Force -Web        362\n",
      "Brute Force -XSS        151\n",
      "SQL Injection            53\n",
      "Name: count, dtype: int64\n",
      "  Columns with missing values:\n",
      "Flow Byts/s    3754\n",
      "dtype: int64\n",
      "------------------------------------------------------------\n",
      "File: Thuesday-20-02-2018_TrafficForML_CICFlowMeter.parquet\n",
      "  Shape: 7,948,748 rows × 84 columns\n",
      "  Label counts:\n",
      "Label\n",
      "Benign                    7372557\n",
      "DDoS attacks-LOIC-HTTP     576191\n",
      "Name: count, dtype: int64\n",
      "  Columns with missing values:\n",
      "Flow Byts/s    36767\n",
      "dtype: int64\n",
      "------------------------------------------------------------\n",
      "File: Thursday-01-03-2018_TrafficForML_CICFlowMeter.parquet\n",
      "  Shape: 331,125 rows × 80 columns\n",
      "  Label counts:\n",
      "Label\n",
      "Benign           238037\n",
      "Infilteration     93063\n",
      "Label                25\n",
      "Name: count, dtype: int64\n",
      "  Columns with missing values:\n",
      "Dst Port         25\n",
      "Protocol         25\n",
      "Flow Duration    25\n",
      "Tot Fwd Pkts     25\n",
      "Tot Bwd Pkts     25\n",
      "                 ..\n",
      "Active Min       25\n",
      "Idle Mean        25\n",
      "Idle Std         25\n",
      "Idle Max         25\n",
      "Idle Min         25\n",
      "Length: 78, dtype: int64\n",
      "------------------------------------------------------------\n",
      "File: Thursday-15-02-2018_TrafficForML_CICFlowMeter.parquet\n",
      "  Shape: 1,048,575 rows × 80 columns\n",
      "  Label counts:\n",
      "Label\n",
      "Benign                   996077\n",
      "DoS attacks-GoldenEye     41508\n",
      "DoS attacks-Slowloris     10990\n",
      "Name: count, dtype: int64\n",
      "  Columns with missing values:\n",
      "Flow Byts/s    4921\n",
      "dtype: int64\n",
      "------------------------------------------------------------\n",
      "File: Thursday-22-02-2018_TrafficForML_CICFlowMeter.parquet\n",
      "  Shape: 1,048,575 rows × 80 columns\n",
      "  Label counts:\n",
      "Label\n",
      "Benign              1048213\n",
      "Brute Force -Web        249\n",
      "Brute Force -XSS         79\n",
      "SQL Injection            34\n",
      "Name: count, dtype: int64\n",
      "  Columns with missing values:\n",
      "Flow Byts/s    3569\n",
      "dtype: int64\n",
      "------------------------------------------------------------\n",
      "File: Wednesday-14-02-2018_TrafficForML_CICFlowMeter.parquet\n",
      "  Shape: 1,048,575 rows × 80 columns\n",
      "  Label counts:\n",
      "Label\n",
      "Benign            667626\n",
      "FTP-BruteForce    193360\n",
      "SSH-Bruteforce    187589\n",
      "Name: count, dtype: int64\n",
      "  Columns with missing values:\n",
      "Flow Byts/s    2277\n",
      "dtype: int64\n",
      "------------------------------------------------------------\n",
      "File: Wednesday-21-02-2018_TrafficForML_CICFlowMeter.parquet\n",
      "  Shape: 1,048,575 rows × 80 columns\n",
      "  Label counts:\n",
      "Label\n",
      "DDOS attack-HOIC        686012\n",
      "Benign                  360833\n",
      "DDOS attack-LOIC-UDP      1730\n",
      "Name: count, dtype: int64\n",
      "  No missing values found.\n",
      "------------------------------------------------------------\n",
      "File: Wednesday-28-02-2018_TrafficForML_CICFlowMeter.parquet\n",
      "  Shape: 613,104 rows × 80 columns\n",
      "  Label counts:\n",
      "Label\n",
      "Benign           544200\n",
      "Infilteration     68871\n",
      "Label                33\n",
      "Name: count, dtype: int64\n",
      "  Columns with missing values:\n",
      "Dst Port         33\n",
      "Protocol         33\n",
      "Flow Duration    33\n",
      "Tot Fwd Pkts     33\n",
      "Tot Bwd Pkts     33\n",
      "                 ..\n",
      "Active Min       33\n",
      "Idle Mean        33\n",
      "Idle Std         33\n",
      "Idle Max         33\n",
      "Idle Min         33\n",
      "Length: 78, dtype: int64\n",
      "------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "# Path to Parquet files\n",
    "parquet_dir = r\"D:/Canada/Subjects/Semester -2/AIDI-2005-02 CAPSTONE TERM ll/Bot_detector/data/processed\"\n",
    "parquet_files = [f for f in os.listdir(parquet_dir) if f.endswith(\".parquet\")]\n",
    "\n",
    "print(f\"Found {len(parquet_files)} Parquet files.\\n\")\n",
    "\n",
    "for pf in sorted(parquet_files):\n",
    "    fpath = os.path.join(parquet_dir, pf)\n",
    "    df = pd.read_parquet(fpath)\n",
    "    print(f\"File: {pf}\")\n",
    "    print(f\"  Shape: {df.shape[0]:,} rows × {df.shape[1]} columns\")\n",
    "    print(f\"  Label counts:\\n{df['Label'].value_counts(dropna=False)}\")\n",
    "    missing = df.isnull().sum()\n",
    "    n_missing = missing[missing > 0]\n",
    "    if not n_missing.empty:\n",
    "        print(f\"  Columns with missing values:\\n{n_missing}\")\n",
    "    else:\n",
    "        print(\"  No missing values found.\")\n",
    "    print(\"-\" * 60)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d551fcdd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing Friday-02-03-2018_TrafficForML_CICFlowMeter.parquet...\n",
      "Saved cleaned file: D:/Canada/Subjects/Semester -2/AIDI-2005-02 CAPSTONE TERM ll/Bot_detector/data/cleaned\\Friday-02-03-2018_TrafficForML_CICFlowMeter.parquet (Rows: 1048575)\n",
      "Processing Friday-16-02-2018_TrafficForML_CICFlowMeter.parquet...\n",
      "Saved cleaned file: D:/Canada/Subjects/Semester -2/AIDI-2005-02 CAPSTONE TERM ll/Bot_detector/data/cleaned\\Friday-16-02-2018_TrafficForML_CICFlowMeter.parquet (Rows: 1048574)\n",
      "Processing Friday-23-02-2018_TrafficForML_CICFlowMeter.parquet...\n",
      "Saved cleaned file: D:/Canada/Subjects/Semester -2/AIDI-2005-02 CAPSTONE TERM ll/Bot_detector/data/cleaned\\Friday-23-02-2018_TrafficForML_CICFlowMeter.parquet (Rows: 1048575)\n",
      "Processing Thuesday-20-02-2018_TrafficForML_CICFlowMeter.parquet...\n",
      "Saved cleaned file: D:/Canada/Subjects/Semester -2/AIDI-2005-02 CAPSTONE TERM ll/Bot_detector/data/cleaned\\Thuesday-20-02-2018_TrafficForML_CICFlowMeter.parquet (Rows: 7948748)\n",
      "Processing Thursday-01-03-2018_TrafficForML_CICFlowMeter.parquet...\n",
      "Saved cleaned file: D:/Canada/Subjects/Semester -2/AIDI-2005-02 CAPSTONE TERM ll/Bot_detector/data/cleaned\\Thursday-01-03-2018_TrafficForML_CICFlowMeter.parquet (Rows: 331100)\n",
      "Processing Thursday-15-02-2018_TrafficForML_CICFlowMeter.parquet...\n",
      "Saved cleaned file: D:/Canada/Subjects/Semester -2/AIDI-2005-02 CAPSTONE TERM ll/Bot_detector/data/cleaned\\Thursday-15-02-2018_TrafficForML_CICFlowMeter.parquet (Rows: 1048575)\n",
      "Processing Thursday-22-02-2018_TrafficForML_CICFlowMeter.parquet...\n",
      "Saved cleaned file: D:/Canada/Subjects/Semester -2/AIDI-2005-02 CAPSTONE TERM ll/Bot_detector/data/cleaned\\Thursday-22-02-2018_TrafficForML_CICFlowMeter.parquet (Rows: 1048575)\n",
      "Processing Wednesday-14-02-2018_TrafficForML_CICFlowMeter.parquet...\n",
      "Saved cleaned file: D:/Canada/Subjects/Semester -2/AIDI-2005-02 CAPSTONE TERM ll/Bot_detector/data/cleaned\\Wednesday-14-02-2018_TrafficForML_CICFlowMeter.parquet (Rows: 1048575)\n",
      "Processing Wednesday-21-02-2018_TrafficForML_CICFlowMeter.parquet...\n",
      "Saved cleaned file: D:/Canada/Subjects/Semester -2/AIDI-2005-02 CAPSTONE TERM ll/Bot_detector/data/cleaned\\Wednesday-21-02-2018_TrafficForML_CICFlowMeter.parquet (Rows: 1048575)\n",
      "Processing Wednesday-28-02-2018_TrafficForML_CICFlowMeter.parquet...\n",
      "Saved cleaned file: D:/Canada/Subjects/Semester -2/AIDI-2005-02 CAPSTONE TERM ll/Bot_detector/data/cleaned\\Wednesday-28-02-2018_TrafficForML_CICFlowMeter.parquet (Rows: 613071)\n",
      "All files cleaned and harmonized for binary classification!\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Paths\n",
    "PARQUET_DIR = 'D:/Canada/Subjects/Semester -2/AIDI-2005-02 CAPSTONE TERM ll/Bot_detector/data/processed'\n",
    "CLEANED_DIR = 'D:/Canada/Subjects/Semester -2/AIDI-2005-02 CAPSTONE TERM ll/Bot_detector/data/cleaned'\n",
    "os.makedirs(CLEANED_DIR, exist_ok=True)\n",
    "\n",
    "# Columns to always drop if present\n",
    "drop_cols = ['Timestamp', 'Flow ID', 'Src IP', 'Dst IP', 'Src Port', 'Label']  # 'Label' dropped after relabeling\n",
    "\n",
    "# Harmonized set of feature columns \n",
    "feature_cols = None\n",
    "\n",
    "for fname in sorted(os.listdir(PARQUET_DIR)):\n",
    "    if not fname.endswith('.parquet'):\n",
    "        continue\n",
    "    fpath = os.path.join(PARQUET_DIR, fname)\n",
    "    print(f\"Processing {fname}...\")\n",
    "    df = pd.read_parquet(fpath)\n",
    "    \n",
    "    # 1. Remove rows \n",
    "    df = df[df['Label'].notna()]\n",
    "    df = df[df['Label'] != 'Label']\n",
    "    \n",
    "    # 2. Standardize labels: Benign=0, attack=1\n",
    "    df['Label'] = df['Label'].apply(lambda x: 0 if str(x).strip().lower() == 'benign' else 1)\n",
    "    \n",
    "    # 3. Drop unneeded columns (keep 'Label')\n",
    "    cols_to_drop = [col for col in drop_cols if col != 'Label' and col in df.columns]\n",
    "    if cols_to_drop:\n",
    "        df = df.drop(columns=cols_to_drop, errors='ignore')\n",
    "    \n",
    "    # 4. Set/verify harmonized columns\n",
    "    if feature_cols is None:\n",
    "        feature_cols = [col for col in df.columns if col != 'Label']\n",
    "    else:\n",
    "        missing = [c for c in feature_cols if c not in df.columns]\n",
    "        extra = [c for c in df.columns if c not in feature_cols and c != 'Label']\n",
    "        # Add missing with NaN, drop extras\n",
    "        if missing:\n",
    "            for m in missing:\n",
    "                df[m] = np.nan\n",
    "        if extra:\n",
    "            df = df.drop(columns=extra, errors='ignore')\n",
    "        df = df[feature_cols + ['Label']]\n",
    "    \n",
    "    # 5. Convert all feature columns to float\n",
    "    for col in feature_cols:\n",
    "        df[col] = pd.to_numeric(df[col], errors='coerce')\n",
    "    \n",
    "    # 6. Fill missing values\n",
    "    df[feature_cols] = df[feature_cols].fillna(df[feature_cols].median())\n",
    "    \n",
    "    # 7. Save cleaned parquet\n",
    "    out_path = os.path.join(CLEANED_DIR, fname)\n",
    "    df.to_parquet(out_path, index=False)\n",
    "    print(f\"Saved cleaned file: {out_path} (Rows: {len(df)})\")\n",
    "\n",
    "\n",
    "print(\"All files cleaned and harmonized for binary classification!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "8f37bb23",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cleaned\\Friday-02-03-2018_TrafficForML_CICFlowMeter.parquet - Label counts:\n",
      "Label\n",
      "0    762384\n",
      "1    286191\n",
      "Name: count, dtype: int64\n",
      "\n",
      "Missing values:\n",
      "Series([], dtype: int64)\n",
      "\n",
      "cleaned\\Friday-16-02-2018_TrafficForML_CICFlowMeter.parquet - Label counts:\n",
      "Label\n",
      "1    601802\n",
      "0    446772\n",
      "Name: count, dtype: int64\n",
      "\n",
      "Missing values:\n",
      "Series([], dtype: int64)\n",
      "\n",
      "cleaned\\Friday-23-02-2018_TrafficForML_CICFlowMeter.parquet - Label counts:\n",
      "Label\n",
      "0    1048009\n",
      "1        566\n",
      "Name: count, dtype: int64\n",
      "\n",
      "Missing values:\n",
      "Series([], dtype: int64)\n",
      "\n",
      "cleaned\\Thuesday-20-02-2018_TrafficForML_CICFlowMeter.parquet - Label counts:\n",
      "Label\n",
      "0    7372557\n",
      "1     576191\n",
      "Name: count, dtype: int64\n",
      "\n",
      "Missing values:\n",
      "Series([], dtype: int64)\n",
      "\n",
      "cleaned\\Thursday-01-03-2018_TrafficForML_CICFlowMeter.parquet - Label counts:\n",
      "Label\n",
      "0    238037\n",
      "1     93063\n",
      "Name: count, dtype: int64\n",
      "\n",
      "Missing values:\n",
      "Series([], dtype: int64)\n",
      "\n",
      "cleaned\\Thursday-15-02-2018_TrafficForML_CICFlowMeter.parquet - Label counts:\n",
      "Label\n",
      "0    996077\n",
      "1     52498\n",
      "Name: count, dtype: int64\n",
      "\n",
      "Missing values:\n",
      "Series([], dtype: int64)\n",
      "\n",
      "cleaned\\Thursday-22-02-2018_TrafficForML_CICFlowMeter.parquet - Label counts:\n",
      "Label\n",
      "0    1048213\n",
      "1        362\n",
      "Name: count, dtype: int64\n",
      "\n",
      "Missing values:\n",
      "Series([], dtype: int64)\n",
      "\n",
      "cleaned\\Wednesday-14-02-2018_TrafficForML_CICFlowMeter.parquet - Label counts:\n",
      "Label\n",
      "0    667626\n",
      "1    380949\n",
      "Name: count, dtype: int64\n",
      "\n",
      "Missing values:\n",
      "Series([], dtype: int64)\n",
      "\n",
      "cleaned\\Wednesday-21-02-2018_TrafficForML_CICFlowMeter.parquet - Label counts:\n",
      "Label\n",
      "1    687742\n",
      "0    360833\n",
      "Name: count, dtype: int64\n",
      "\n",
      "Missing values:\n",
      "Series([], dtype: int64)\n",
      "\n",
      "cleaned\\Wednesday-28-02-2018_TrafficForML_CICFlowMeter.parquet - Label counts:\n",
      "Label\n",
      "0    544200\n",
      "1     68871\n",
      "Name: count, dtype: int64\n",
      "\n",
      "Missing values:\n",
      "Series([], dtype: int64)\n",
      "\n",
      "=== OVERALL LABEL COUNTS ===\n",
      "Label\n",
      "0    13484708.0\n",
      "1     2748235.0\n",
      "dtype: float64\n",
      "\n",
      "=== OVERALL MISSING VALUES (any feature) ===\n",
      "Series([], dtype: float64)\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import glob\n",
    "\n",
    "cleaned_dir = \"D:/Canada/Subjects/Semester -2/AIDI-2005-02 CAPSTONE TERM ll/Bot_detector/data/cleaned\"\n",
    "file_list = sorted(glob.glob(f\"{cleaned_dir}/*.parquet\"))\n",
    "\n",
    "# Aggregate for overall class balance and missingness\n",
    "total_counts = pd.Series(dtype=int)\n",
    "total_missing = pd.Series(dtype=int)\n",
    "\n",
    "for path in file_list:\n",
    "    df = pd.read_parquet(path)\n",
    "    print(f\"{path.split('/')[-1]} - Label counts:\\n{df['Label'].value_counts()}\\n\")\n",
    "    total_counts = total_counts.add(df['Label'].value_counts(), fill_value=0)\n",
    "    missing = df.isnull().sum()\n",
    "    print(f\"Missing values:\\n{missing[missing>0]}\\n\")\n",
    "    total_missing = total_missing.add(missing, fill_value=0)\n",
    "\n",
    "print(\"=== OVERALL LABEL COUNTS ===\")\n",
    "print(total_counts)\n",
    "print(\"\\n=== OVERALL MISSING VALUES (any feature) ===\")\n",
    "print(total_missing[total_missing > 0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3200388",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train files: ['D:\\\\Canada\\\\Subjects\\\\Semester -2\\\\AIDI-2005-02 CAPSTONE TERM ll\\\\Bot_detector\\\\data\\\\cleaned\\\\Friday-02-03-2018_TrafficForML_CICFlowMeter.parquet', 'D:\\\\Canada\\\\Subjects\\\\Semester -2\\\\AIDI-2005-02 CAPSTONE TERM ll\\\\Bot_detector\\\\data\\\\cleaned\\\\Friday-16-02-2018_TrafficForML_CICFlowMeter.parquet', 'D:\\\\Canada\\\\Subjects\\\\Semester -2\\\\AIDI-2005-02 CAPSTONE TERM ll\\\\Bot_detector\\\\data\\\\cleaned\\\\Friday-23-02-2018_TrafficForML_CICFlowMeter.parquet', 'D:\\\\Canada\\\\Subjects\\\\Semester -2\\\\AIDI-2005-02 CAPSTONE TERM ll\\\\Bot_detector\\\\data\\\\cleaned\\\\Thuesday-20-02-2018_TrafficForML_CICFlowMeter.parquet', 'D:\\\\Canada\\\\Subjects\\\\Semester -2\\\\AIDI-2005-02 CAPSTONE TERM ll\\\\Bot_detector\\\\data\\\\cleaned\\\\Thursday-01-03-2018_TrafficForML_CICFlowMeter.parquet', 'D:\\\\Canada\\\\Subjects\\\\Semester -2\\\\AIDI-2005-02 CAPSTONE TERM ll\\\\Bot_detector\\\\data\\\\cleaned\\\\Thursday-15-02-2018_TrafficForML_CICFlowMeter.parquet', 'D:\\\\Canada\\\\Subjects\\\\Semester -2\\\\AIDI-2005-02 CAPSTONE TERM ll\\\\Bot_detector\\\\data\\\\cleaned\\\\Thursday-22-02-2018_TrafficForML_CICFlowMeter.parquet', 'D:\\\\Canada\\\\Subjects\\\\Semester -2\\\\AIDI-2005-02 CAPSTONE TERM ll\\\\Bot_detector\\\\data\\\\cleaned\\\\Wednesday-14-02-2018_TrafficForML_CICFlowMeter.parquet', 'D:\\\\Canada\\\\Subjects\\\\Semester -2\\\\AIDI-2005-02 CAPSTONE TERM ll\\\\Bot_detector\\\\data\\\\cleaned\\\\Wednesday-21-02-2018_TrafficForML_CICFlowMeter.parquet']\n",
      "Test file: D:\\Canada\\Subjects\\Semester -2\\AIDI-2005-02 CAPSTONE TERM ll\\Bot_detector\\data\\cleaned\\Wednesday-28-02-2018_TrafficForML_CICFlowMeter.parquet\n",
      "\n",
      "Calculating scale_pos_weight...\n",
      "scale_pos_weight: 4.830\n",
      "\n",
      "Processing: D:\\Canada\\Subjects\\Semester -2\\AIDI-2005-02 CAPSTONE TERM ll\\Bot_detector\\data\\cleaned\\Friday-02-03-2018_TrafficForML_CICFlowMeter.parquet\n",
      "  Training new model …\n",
      "  Continuing training … (batch 2)\n",
      "  Continuing training … (batch 3)\n",
      "  Continuing training … (batch 4)\n",
      "  Continuing training … (batch 5)\n",
      "  Continuing training … (batch 6)\n",
      "  Continuing training … (batch 7)\n",
      "  Continuing training … (batch 8)\n",
      "  Continuing training … (batch 9)\n",
      "  Continuing training … (batch 10)\n",
      "  Continuing training … (batch 11)\n",
      "\n",
      "Processing: D:\\Canada\\Subjects\\Semester -2\\AIDI-2005-02 CAPSTONE TERM ll\\Bot_detector\\data\\cleaned\\Friday-16-02-2018_TrafficForML_CICFlowMeter.parquet\n",
      "  Continuing training … (batch 1)\n",
      "  Continuing training … (batch 2)\n",
      "  Continuing training … (batch 3)\n",
      "  Continuing training … (batch 4)\n",
      "  Continuing training … (batch 5)\n",
      "  Continuing training … (batch 6)\n",
      "  Continuing training … (batch 7)\n",
      "  Continuing training … (batch 8)\n",
      "  Continuing training … (batch 9)\n",
      "  Continuing training … (batch 10)\n",
      "  Continuing training … (batch 11)\n",
      "\n",
      "Processing: D:\\Canada\\Subjects\\Semester -2\\AIDI-2005-02 CAPSTONE TERM ll\\Bot_detector\\data\\cleaned\\Friday-23-02-2018_TrafficForML_CICFlowMeter.parquet\n",
      "  Continuing training … (batch 1)\n",
      "  Continuing training … (batch 2)\n",
      "  Continuing training … (batch 3)\n",
      "  Continuing training … (batch 4)\n",
      "  Continuing training … (batch 5)\n",
      "  Continuing training … (batch 6)\n",
      "  Continuing training … (batch 7)\n",
      "  Continuing training … (batch 8)\n",
      "  Continuing training … (batch 9)\n",
      "  Continuing training … (batch 10)\n",
      "  Continuing training … (batch 11)\n",
      "\n",
      "Processing: D:\\Canada\\Subjects\\Semester -2\\AIDI-2005-02 CAPSTONE TERM ll\\Bot_detector\\data\\cleaned\\Thuesday-20-02-2018_TrafficForML_CICFlowMeter.parquet\n",
      "  Continuing training … (batch 1)\n",
      "  Continuing training … (batch 2)\n",
      "  Continuing training … (batch 3)\n",
      "  Continuing training … (batch 4)\n",
      "  Continuing training … (batch 5)\n",
      "  Continuing training … (batch 6)\n",
      "  Continuing training … (batch 7)\n",
      "  Continuing training … (batch 8)\n",
      "  Continuing training … (batch 9)\n",
      "  Continuing training … (batch 10)\n",
      "  Continuing training … (batch 11)\n",
      "  Continuing training … (batch 12)\n",
      "  Continuing training … (batch 13)\n",
      "  Continuing training … (batch 14)\n",
      "  Continuing training … (batch 15)\n",
      "  Continuing training … (batch 16)\n",
      "  Continuing training … (batch 17)\n",
      "  Continuing training … (batch 18)\n",
      "  Continuing training … (batch 19)\n",
      "  Continuing training … (batch 20)\n",
      "  Continuing training … (batch 21)\n",
      "  Continuing training … (batch 22)\n",
      "  Continuing training … (batch 23)\n",
      "  Continuing training … (batch 24)\n",
      "  Continuing training … (batch 25)\n",
      "  Continuing training … (batch 26)\n",
      "  Continuing training … (batch 27)\n",
      "  Continuing training … (batch 28)\n",
      "  Continuing training … (batch 29)\n",
      "  Continuing training … (batch 30)\n",
      "  Continuing training … (batch 31)\n",
      "  Continuing training … (batch 32)\n",
      "  Continuing training … (batch 33)\n",
      "  Continuing training … (batch 34)\n",
      "  Continuing training … (batch 35)\n",
      "  Continuing training … (batch 36)\n",
      "  Continuing training … (batch 37)\n",
      "  Continuing training … (batch 38)\n",
      "  Continuing training … (batch 39)\n",
      "  Continuing training … (batch 40)\n",
      "  Continuing training … (batch 41)\n",
      "  Continuing training … (batch 42)\n",
      "  Continuing training … (batch 43)\n",
      "  Continuing training … (batch 44)\n",
      "  Continuing training … (batch 45)\n",
      "  Continuing training … (batch 46)\n",
      "  Continuing training … (batch 47)\n",
      "  Continuing training … (batch 48)\n",
      "  Continuing training … (batch 49)\n",
      "  Continuing training … (batch 50)\n",
      "  Continuing training … (batch 51)\n",
      "  Continuing training … (batch 52)\n",
      "  Continuing training … (batch 53)\n",
      "  Continuing training … (batch 54)\n",
      "  Continuing training … (batch 55)\n",
      "  Continuing training … (batch 56)\n",
      "  Continuing training … (batch 57)\n",
      "  Continuing training … (batch 58)\n",
      "  Continuing training … (batch 59)\n",
      "  Continuing training … (batch 60)\n",
      "  Continuing training … (batch 61)\n",
      "  Continuing training … (batch 62)\n",
      "  Continuing training … (batch 63)\n",
      "  Continuing training … (batch 64)\n",
      "  Continuing training … (batch 65)\n",
      "  Continuing training … (batch 66)\n",
      "  Continuing training … (batch 67)\n",
      "  Continuing training … (batch 68)\n",
      "  Continuing training … (batch 69)\n",
      "  Continuing training … (batch 70)\n",
      "  Continuing training … (batch 71)\n",
      "  Continuing training … (batch 72)\n",
      "  Continuing training … (batch 73)\n",
      "  Continuing training … (batch 74)\n",
      "  Continuing training … (batch 75)\n",
      "  Continuing training … (batch 76)\n",
      "  Continuing training … (batch 77)\n",
      "  Continuing training … (batch 78)\n",
      "  Continuing training … (batch 79)\n",
      "  Continuing training … (batch 80)\n",
      "\n",
      "Processing: D:\\Canada\\Subjects\\Semester -2\\AIDI-2005-02 CAPSTONE TERM ll\\Bot_detector\\data\\cleaned\\Thursday-01-03-2018_TrafficForML_CICFlowMeter.parquet\n",
      "  Continuing training … (batch 1)\n",
      "  Continuing training … (batch 2)\n",
      "  Continuing training … (batch 3)\n",
      "  Continuing training … (batch 4)\n",
      "\n",
      "Processing: D:\\Canada\\Subjects\\Semester -2\\AIDI-2005-02 CAPSTONE TERM ll\\Bot_detector\\data\\cleaned\\Thursday-15-02-2018_TrafficForML_CICFlowMeter.parquet\n",
      "  Continuing training … (batch 1)\n",
      "  Continuing training … (batch 2)\n",
      "  Continuing training … (batch 3)\n",
      "  Continuing training … (batch 4)\n",
      "  Continuing training … (batch 5)\n",
      "  Continuing training … (batch 6)\n",
      "  Continuing training … (batch 7)\n",
      "  Continuing training … (batch 8)\n",
      "  Continuing training … (batch 9)\n",
      "  Continuing training … (batch 10)\n",
      "  Continuing training … (batch 11)\n",
      "\n",
      "Processing: D:\\Canada\\Subjects\\Semester -2\\AIDI-2005-02 CAPSTONE TERM ll\\Bot_detector\\data\\cleaned\\Thursday-22-02-2018_TrafficForML_CICFlowMeter.parquet\n",
      "  Continuing training … (batch 1)\n",
      "  Continuing training … (batch 2)\n",
      "  Continuing training … (batch 3)\n",
      "  Continuing training … (batch 4)\n",
      "  Continuing training … (batch 5)\n",
      "  Continuing training … (batch 6)\n",
      "  Continuing training … (batch 7)\n",
      "  Continuing training … (batch 8)\n",
      "  Continuing training … (batch 9)\n",
      "  Continuing training … (batch 10)\n",
      "  Continuing training … (batch 11)\n",
      "\n",
      "Processing: D:\\Canada\\Subjects\\Semester -2\\AIDI-2005-02 CAPSTONE TERM ll\\Bot_detector\\data\\cleaned\\Wednesday-14-02-2018_TrafficForML_CICFlowMeter.parquet\n",
      "  Continuing training … (batch 1)\n",
      "  Continuing training … (batch 2)\n",
      "  Continuing training … (batch 3)\n",
      "  Continuing training … (batch 4)\n",
      "  Continuing training … (batch 5)\n",
      "  Continuing training … (batch 6)\n",
      "  Continuing training … (batch 7)\n",
      "  Continuing training … (batch 8)\n",
      "  Continuing training … (batch 9)\n",
      "  Continuing training … (batch 10)\n",
      "  Continuing training … (batch 11)\n",
      "\n",
      "Processing: D:\\Canada\\Subjects\\Semester -2\\AIDI-2005-02 CAPSTONE TERM ll\\Bot_detector\\data\\cleaned\\Wednesday-21-02-2018_TrafficForML_CICFlowMeter.parquet\n",
      "  Continuing training … (batch 1)\n",
      "  Continuing training … (batch 2)\n",
      "  Continuing training … (batch 3)\n",
      "  Continuing training … (batch 4)\n",
      "  Continuing training … (batch 5)\n",
      "  Continuing training … (batch 6)\n",
      "  Continuing training … (batch 7)\n",
      "  Continuing training … (batch 8)\n",
      "  Continuing training … (batch 9)\n",
      "  Continuing training … (batch 10)\n",
      "  Continuing training … (batch 11)\n",
      "\n",
      "Model saved as: final_lightgbm.txt\n",
      "\n",
      "Evaluating on: D:\\Canada\\Subjects\\Semester -2\\AIDI-2005-02 CAPSTONE TERM ll\\Bot_detector\\data\\cleaned\\Wednesday-28-02-2018_TrafficForML_CICFlowMeter.parquet\n",
      "\n",
      "=== Results for threshold=0.5 ===\n",
      "Accuracy: 0.769393757003675\n",
      "Confusion Matrix:\n",
      " [[459859  84341]\n",
      " [ 57037  11834]]\n",
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0     0.8897    0.8450    0.8668    544200\n",
      "           1     0.1230    0.1718    0.1434     68871\n",
      "\n",
      "    accuracy                         0.7694    613071\n",
      "   macro avg     0.5064    0.5084    0.5051    613071\n",
      "weighted avg     0.8035    0.7694    0.7855    613071\n",
      "\n",
      "\n",
      "=== Results for threshold=0.4 ===\n",
      "Accuracy: 0.769393757003675\n",
      "Confusion Matrix:\n",
      " [[459859  84341]\n",
      " [ 57037  11834]]\n",
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0     0.8897    0.8450    0.8668    544200\n",
      "           1     0.1230    0.1718    0.1434     68871\n",
      "\n",
      "    accuracy                         0.7694    613071\n",
      "   macro avg     0.5064    0.5084    0.5051    613071\n",
      "weighted avg     0.8035    0.7694    0.7855    613071\n",
      "\n",
      "\n",
      "=== Results for threshold=0.3 ===\n",
      "Accuracy: 0.769393757003675\n",
      "Confusion Matrix:\n",
      " [[459859  84341]\n",
      " [ 57037  11834]]\n",
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0     0.8897    0.8450    0.8668    544200\n",
      "           1     0.1230    0.1718    0.1434     68871\n",
      "\n",
      "    accuracy                         0.7694    613071\n",
      "   macro avg     0.5064    0.5084    0.5051    613071\n",
      "weighted avg     0.8035    0.7694    0.7855    613071\n",
      "\n",
      "\n",
      "=== Results for threshold=0.2 ===\n",
      "Accuracy: 0.769393757003675\n",
      "Confusion Matrix:\n",
      " [[459859  84341]\n",
      " [ 57037  11834]]\n",
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0     0.8897    0.8450    0.8668    544200\n",
      "           1     0.1230    0.1718    0.1434     68871\n",
      "\n",
      "    accuracy                         0.7694    613071\n",
      "   macro avg     0.5064    0.5084    0.5051    613071\n",
      "weighted avg     0.8035    0.7694    0.7855    613071\n",
      "\n",
      "\n",
      "Done! The model and thresholded evaluation results are printed.\n"
     ]
    }
   ],
   "source": [
    "import pyarrow.parquet as pq\n",
    "import lightgbm as lgb\n",
    "import numpy as np\n",
    "import os\n",
    "from glob import glob\n",
    "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
    "\n",
    "# === CONFIGURATION ===\n",
    "CLEANED_DIR = r\"D:\\Canada\\Subjects\\Semester -2\\AIDI-2005-02 CAPSTONE TERM ll\\Bot_detector\\data\\cleaned\"\n",
    "MODEL_PATH = \"final_lightgbm.txt\"\n",
    "BATCH_SIZE = 100_000\n",
    "\n",
    "# 1. Auto-detect all cleaned parquet files, split train/test (last one as test for demo)\n",
    "file_list = sorted(glob(os.path.join(CLEANED_DIR, \"*.parquet\")))\n",
    "train_files = file_list[:-1]\n",
    "test_file = file_list[-1]\n",
    "\n",
    "print(\"Train files:\", train_files)\n",
    "print(\"Test file:\", test_file)\n",
    "\n",
    "# === CALCULATE scale_pos_weight ===\n",
    "print(\"\\nCalculating scale_pos_weight...\")\n",
    "total_0, total_1 = 0, 0\n",
    "for f in train_files:\n",
    "    parquet = pq.ParquetFile(f)\n",
    "    for batch in parquet.iter_batches(batch_size=BATCH_SIZE, columns=['Label']):\n",
    "        arr = batch.to_pandas()['Label'].value_counts()\n",
    "        total_0 += arr.get(0, 0)\n",
    "        total_1 += arr.get(1, 0)\n",
    "\n",
    "scale_pos_weight = total_0 / total_1\n",
    "print(f\"scale_pos_weight: {scale_pos_weight:.3f}\")\n",
    "\n",
    "# 2. Pick feature columns (from any file)\n",
    "sample = pq.read_table(train_files[0], columns=None).to_pandas().iloc[:1]\n",
    "feature_cols = [col for col in sample.columns if col != 'Label']\n",
    "\n",
    "# 3. LightGBM parameters\n",
    "params = {\n",
    "    'objective': 'binary',\n",
    "    'boosting_type': 'gbdt',\n",
    "    'metric': 'binary_logloss',\n",
    "    'learning_rate': 0.05,\n",
    "    'num_leaves': 31,\n",
    "    'verbose': -1,\n",
    "    'scale_pos_weight': scale_pos_weight\n",
    "}\n",
    "\n",
    "# === TRAINING ===\n",
    "booster = None\n",
    "for file_path in train_files:\n",
    "    print(f\"\\nProcessing: {file_path}\")\n",
    "    parquet_file = pq.ParquetFile(file_path)\n",
    "    batch_idx = 1\n",
    "    for batch in parquet_file.iter_batches(batch_size=BATCH_SIZE):\n",
    "        df = batch.to_pandas()\n",
    "        X = df[feature_cols].astype(np.float32)\n",
    "        y = df['Label'].astype(np.int32)\n",
    "        lgb_train = lgb.Dataset(X, y, free_raw_data=True)\n",
    "        if booster is None:\n",
    "            print(\"  Training new model …\")\n",
    "            booster = lgb.train(params, lgb_train, num_boost_round=100)\n",
    "        else:\n",
    "            print(f\"  Continuing training … (batch {batch_idx})\")\n",
    "            booster = lgb.train(params, lgb_train, num_boost_round=100, init_model=booster)\n",
    "        del df, X, y, lgb_train  # Free memory\n",
    "        batch_idx += 1\n",
    "\n",
    "booster.save_model(MODEL_PATH)\n",
    "print(\"\\nModel saved as:\", MODEL_PATH)\n",
    "\n",
    "# === EVALUATION ===\n",
    "print(f\"\\nEvaluating on: {test_file}\")\n",
    "test_parquet = pq.ParquetFile(test_file)\n",
    "y_true_all, y_pred_prob = [], []\n",
    "for batch in test_parquet.iter_batches(batch_size=BATCH_SIZE):\n",
    "    df = batch.to_pandas()\n",
    "    X = df[feature_cols].astype(np.float32)\n",
    "    y = df['Label'].astype(np.int32)\n",
    "    y_prob = booster.predict(X)\n",
    "    y_true_all.extend(y.tolist())\n",
    "    y_pred_prob.extend(y_prob.tolist())\n",
    "    del df, X, y, y_prob\n",
    "\n",
    "# --- Threshold sweep\n",
    "thresholds = [0.5, 0.4, 0.3, 0.2]\n",
    "for threshold in thresholds:\n",
    "    y_pred = (np.array(y_pred_prob) > threshold).astype(int)\n",
    "    print(f\"\\n=== Results for threshold={threshold} ===\")\n",
    "    print(\"Accuracy:\", accuracy_score(y_true_all, y_pred))\n",
    "    print(\"Confusion Matrix:\\n\", confusion_matrix(y_true_all, y_pred))\n",
    "    print(\"Classification Report:\\n\", classification_report(y_true_all, y_pred, digits=4))\n",
    "\n",
    "print(\"\\nDone! The model and thresholded evaluation results are printed.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a7f7fb8",
   "metadata": {},
   "source": [
    "### Updated Lightbgm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "afa9fe0e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train files: ['D:\\\\Canada\\\\Subjects\\\\Semester -2\\\\AIDI-2005-02 CAPSTONE TERM ll\\\\Bot_detector\\\\data\\\\cleaned\\\\Friday-02-03-2018_TrafficForML_CICFlowMeter.parquet', 'D:\\\\Canada\\\\Subjects\\\\Semester -2\\\\AIDI-2005-02 CAPSTONE TERM ll\\\\Bot_detector\\\\data\\\\cleaned\\\\Friday-16-02-2018_TrafficForML_CICFlowMeter.parquet', 'D:\\\\Canada\\\\Subjects\\\\Semester -2\\\\AIDI-2005-02 CAPSTONE TERM ll\\\\Bot_detector\\\\data\\\\cleaned\\\\Friday-23-02-2018_TrafficForML_CICFlowMeter.parquet', 'D:\\\\Canada\\\\Subjects\\\\Semester -2\\\\AIDI-2005-02 CAPSTONE TERM ll\\\\Bot_detector\\\\data\\\\cleaned\\\\Thuesday-20-02-2018_TrafficForML_CICFlowMeter.parquet', 'D:\\\\Canada\\\\Subjects\\\\Semester -2\\\\AIDI-2005-02 CAPSTONE TERM ll\\\\Bot_detector\\\\data\\\\cleaned\\\\Thursday-01-03-2018_TrafficForML_CICFlowMeter.parquet', 'D:\\\\Canada\\\\Subjects\\\\Semester -2\\\\AIDI-2005-02 CAPSTONE TERM ll\\\\Bot_detector\\\\data\\\\cleaned\\\\Thursday-15-02-2018_TrafficForML_CICFlowMeter.parquet', 'D:\\\\Canada\\\\Subjects\\\\Semester -2\\\\AIDI-2005-02 CAPSTONE TERM ll\\\\Bot_detector\\\\data\\\\cleaned\\\\Thursday-22-02-2018_TrafficForML_CICFlowMeter.parquet', 'D:\\\\Canada\\\\Subjects\\\\Semester -2\\\\AIDI-2005-02 CAPSTONE TERM ll\\\\Bot_detector\\\\data\\\\cleaned\\\\Wednesday-14-02-2018_TrafficForML_CICFlowMeter.parquet']\n",
      "Validation file: D:\\Canada\\Subjects\\Semester -2\\AIDI-2005-02 CAPSTONE TERM ll\\Bot_detector\\data\\cleaned\\Wednesday-21-02-2018_TrafficForML_CICFlowMeter.parquet\n",
      "Test file: D:\\Canada\\Subjects\\Semester -2\\AIDI-2005-02 CAPSTONE TERM ll\\Bot_detector\\data\\cleaned\\Wednesday-28-02-2018_TrafficForML_CICFlowMeter.parquet\n",
      "\n",
      "Calculating scale_pos_weight from train files...\n",
      "scale_pos_weight: 6.316\n",
      "\n",
      "Starting batch-wise training with early stopping...\n",
      "\n",
      "Processing: D:\\Canada\\Subjects\\Semester -2\\AIDI-2005-02 CAPSTONE TERM ll\\Bot_detector\\data\\cleaned\\Friday-02-03-2018_TrafficForML_CICFlowMeter.parquet\n",
      "  Training new model with early stopping …\n",
      "Training until validation scores don't improve for 25 rounds\n",
      "[25]\tvalidation's auc: 0.951711\n",
      "Early stopping, best iteration is:\n",
      "[12]\tvalidation's auc: 0.95566\n",
      "  Continuing training … (batch 2)\n",
      "Training until validation scores don't improve for 25 rounds\n",
      "[25]\tvalidation's auc: 0.921198\n",
      "Early stopping, best iteration is:\n",
      "[13]\tvalidation's auc: 0.955255\n",
      "  Continuing training … (batch 3)\n",
      "Training until validation scores don't improve for 25 rounds\n",
      "[25]\tvalidation's auc: 0.934673\n",
      "[50]\tvalidation's auc: 0.947047\n",
      "Early stopping, best iteration is:\n",
      "[49]\tvalidation's auc: 0.947786\n",
      "  Continuing training … (batch 4)\n",
      "[50]\tvalidation's auc: 0.947009\n",
      "Training until validation scores don't improve for 25 rounds\n",
      "[75]\tvalidation's auc: 0.954825\n",
      "[100]\tvalidation's auc: 0.937553\n",
      "Early stopping, best iteration is:\n",
      "[81]\tvalidation's auc: 0.957409\n",
      "  Continuing training … (batch 5)\n",
      "Training until validation scores don't improve for 25 rounds\n",
      "[100]\tvalidation's auc: 0.957409\n",
      "Early stopping, best iteration is:\n",
      "[82]\tvalidation's auc: 0.957409\n",
      "  Continuing training … (batch 6)\n",
      "Training until validation scores don't improve for 25 rounds\n",
      "[100]\tvalidation's auc: 0.957409\n",
      "Early stopping, best iteration is:\n",
      "[82]\tvalidation's auc: 0.957409\n",
      "  Continuing training … (batch 7)\n",
      "Training until validation scores don't improve for 25 rounds\n",
      "[100]\tvalidation's auc: 0.957409\n",
      "Early stopping, best iteration is:\n",
      "[82]\tvalidation's auc: 0.957409\n",
      "  Continuing training … (batch 8)\n",
      "Training until validation scores don't improve for 25 rounds\n",
      "[100]\tvalidation's auc: 0.957409\n",
      "Early stopping, best iteration is:\n",
      "[82]\tvalidation's auc: 0.957409\n",
      "  Continuing training … (batch 9)\n",
      "Training until validation scores don't improve for 25 rounds\n",
      "[100]\tvalidation's auc: 0.957409\n",
      "Early stopping, best iteration is:\n",
      "[82]\tvalidation's auc: 0.957409\n",
      "  Continuing training … (batch 10)\n",
      "Training until validation scores don't improve for 25 rounds\n",
      "[100]\tvalidation's auc: 0.957409\n",
      "Early stopping, best iteration is:\n",
      "[82]\tvalidation's auc: 0.957409\n",
      "  Continuing training … (batch 11)\n",
      "Training until validation scores don't improve for 25 rounds\n",
      "[100]\tvalidation's auc: 0.957409\n",
      "Early stopping, best iteration is:\n",
      "[82]\tvalidation's auc: 0.957409\n",
      "\n",
      "Processing: D:\\Canada\\Subjects\\Semester -2\\AIDI-2005-02 CAPSTONE TERM ll\\Bot_detector\\data\\cleaned\\Friday-16-02-2018_TrafficForML_CICFlowMeter.parquet\n",
      "  Continuing training … (batch 1)\n",
      "Training until validation scores don't improve for 25 rounds\n",
      "[100]\tvalidation's auc: 0.98196\n",
      "[125]\tvalidation's auc: 0.987392\n",
      "[150]\tvalidation's auc: 0.988728\n",
      "[175]\tvalidation's auc: 0.994268\n",
      "[200]\tvalidation's auc: 0.995035\n",
      "[225]\tvalidation's auc: 0.995563\n",
      "[250]\tvalidation's auc: 0.996361\n",
      "[275]\tvalidation's auc: 0.996978\n",
      "Early stopping, best iteration is:\n",
      "[266]\tvalidation's auc: 0.997516\n",
      "  Continuing training … (batch 2)\n",
      "Training until validation scores don't improve for 25 rounds\n",
      "[275]\tvalidation's auc: 0.997531\n",
      "Early stopping, best iteration is:\n",
      "[270]\tvalidation's auc: 0.997534\n",
      "  Continuing training … (batch 3)\n",
      "Training until validation scores don't improve for 25 rounds\n",
      "[275]\tvalidation's auc: 0.997678\n",
      "[300]\tvalidation's auc: 0.997861\n",
      "Early stopping, best iteration is:\n",
      "[291]\tvalidation's auc: 0.997957\n",
      "  Continuing training … (batch 4)\n",
      "Training until validation scores don't improve for 25 rounds\n",
      "[300]\tvalidation's auc: 0.997955\n",
      "Early stopping, best iteration is:\n",
      "[294]\tvalidation's auc: 0.997955\n",
      "  Continuing training … (batch 5)\n",
      "Training until validation scores don't improve for 25 rounds\n",
      "[300]\tvalidation's auc: 0.998003\n",
      "[325]\tvalidation's auc: 0.997921\n",
      "Early stopping, best iteration is:\n",
      "[305]\tvalidation's auc: 0.998011\n",
      "  Continuing training … (batch 6)\n",
      "Training until validation scores don't improve for 25 rounds\n",
      "[325]\tvalidation's auc: 0.997925\n",
      "Early stopping, best iteration is:\n",
      "[306]\tvalidation's auc: 0.997954\n",
      "  Continuing training … (batch 7)\n",
      "Training until validation scores don't improve for 25 rounds\n",
      "[325]\tvalidation's auc: 0.997925\n",
      "Early stopping, best iteration is:\n",
      "[307]\tvalidation's auc: 0.997943\n",
      "  Continuing training … (batch 8)\n",
      "Training until validation scores don't improve for 25 rounds\n",
      "[325]\tvalidation's auc: 0.99792\n",
      "Early stopping, best iteration is:\n",
      "[308]\tvalidation's auc: 0.997936\n",
      "  Continuing training … (batch 9)\n",
      "Training until validation scores don't improve for 25 rounds\n",
      "[325]\tvalidation's auc: 0.997777\n",
      "Early stopping, best iteration is:\n",
      "[311]\tvalidation's auc: 0.997926\n",
      "  Continuing training … (batch 10)\n",
      "Training until validation scores don't improve for 25 rounds\n",
      "[325]\tvalidation's auc: 0.997931\n",
      "Early stopping, best iteration is:\n",
      "[314]\tvalidation's auc: 0.997933\n",
      "  Continuing training … (batch 11)\n",
      "Training until validation scores don't improve for 25 rounds\n",
      "[325]\tvalidation's auc: 0.997748\n",
      "Early stopping, best iteration is:\n",
      "[315]\tvalidation's auc: 0.997913\n",
      "\n",
      "Processing: D:\\Canada\\Subjects\\Semester -2\\AIDI-2005-02 CAPSTONE TERM ll\\Bot_detector\\data\\cleaned\\Friday-23-02-2018_TrafficForML_CICFlowMeter.parquet\n",
      "  Continuing training … (batch 1)\n",
      "Training until validation scores don't improve for 25 rounds\n",
      "[325]\tvalidation's auc: 0.994449\n",
      "Early stopping, best iteration is:\n",
      "[316]\tvalidation's auc: 0.996095\n",
      "  Continuing training … (batch 2)\n",
      "Training until validation scores don't improve for 25 rounds\n",
      "[325]\tvalidation's auc: 0.996095\n",
      "Early stopping, best iteration is:\n",
      "[317]\tvalidation's auc: 0.996095\n",
      "  Continuing training … (batch 3)\n",
      "Training until validation scores don't improve for 25 rounds\n",
      "[325]\tvalidation's auc: 0.996095\n",
      "Early stopping, best iteration is:\n",
      "[317]\tvalidation's auc: 0.996095\n",
      "  Continuing training … (batch 4)\n",
      "Training until validation scores don't improve for 25 rounds\n",
      "[325]\tvalidation's auc: 0.996095\n",
      "Early stopping, best iteration is:\n",
      "[317]\tvalidation's auc: 0.996095\n",
      "  Continuing training … (batch 5)\n",
      "Training until validation scores don't improve for 25 rounds\n",
      "[325]\tvalidation's auc: 0.996095\n",
      "Early stopping, best iteration is:\n",
      "[317]\tvalidation's auc: 0.996095\n",
      "  Continuing training … (batch 6)\n",
      "Training until validation scores don't improve for 25 rounds\n",
      "[325]\tvalidation's auc: 0.996095\n",
      "Early stopping, best iteration is:\n",
      "[317]\tvalidation's auc: 0.996095\n",
      "  Continuing training … (batch 7)\n",
      "Training until validation scores don't improve for 25 rounds\n",
      "[325]\tvalidation's auc: 0.996095\n",
      "Early stopping, best iteration is:\n",
      "[317]\tvalidation's auc: 0.996095\n",
      "  Continuing training … (batch 8)\n",
      "Training until validation scores don't improve for 25 rounds\n",
      "[325]\tvalidation's auc: 0.996095\n",
      "Early stopping, best iteration is:\n",
      "[317]\tvalidation's auc: 0.996095\n",
      "  Continuing training … (batch 9)\n",
      "Training until validation scores don't improve for 25 rounds\n",
      "[325]\tvalidation's auc: 0.996095\n",
      "Early stopping, best iteration is:\n",
      "[317]\tvalidation's auc: 0.996095\n",
      "  Continuing training … (batch 10)\n",
      "Training until validation scores don't improve for 25 rounds\n",
      "[325]\tvalidation's auc: 0.996095\n",
      "Early stopping, best iteration is:\n",
      "[317]\tvalidation's auc: 0.996095\n",
      "  Continuing training … (batch 11)\n",
      "Training until validation scores don't improve for 25 rounds\n",
      "[325]\tvalidation's auc: 0.996095\n",
      "Early stopping, best iteration is:\n",
      "[317]\tvalidation's auc: 0.996095\n",
      "\n",
      "Processing: D:\\Canada\\Subjects\\Semester -2\\AIDI-2005-02 CAPSTONE TERM ll\\Bot_detector\\data\\cleaned\\Thuesday-20-02-2018_TrafficForML_CICFlowMeter.parquet\n",
      "  Continuing training … (batch 1)\n",
      "Training until validation scores don't improve for 25 rounds\n",
      "[325]\tvalidation's auc: 0.994943\n",
      "Early stopping, best iteration is:\n",
      "[317]\tvalidation's auc: 0.995955\n",
      "  Continuing training … (batch 2)\n",
      "Training until validation scores don't improve for 25 rounds\n",
      "[325]\tvalidation's auc: 0.996096\n",
      "[350]\tvalidation's auc: 0.996422\n",
      "[375]\tvalidation's auc: 0.987981\n",
      "Early stopping, best iteration is:\n",
      "[358]\tvalidation's auc: 0.996764\n",
      "  Continuing training … (batch 3)\n",
      "Training until validation scores don't improve for 25 rounds\n",
      "[375]\tvalidation's auc: 0.996643\n",
      "Early stopping, best iteration is:\n",
      "[361]\tvalidation's auc: 0.99677\n",
      "  Continuing training … (batch 4)\n",
      "Training until validation scores don't improve for 25 rounds\n",
      "[375]\tvalidation's auc: 0.99017\n",
      "Early stopping, best iteration is:\n",
      "[362]\tvalidation's auc: 0.996714\n",
      "  Continuing training … (batch 5)\n",
      "Training until validation scores don't improve for 25 rounds\n",
      "[375]\tvalidation's auc: 0.998009\n",
      "[400]\tvalidation's auc: 0.998465\n",
      "Early stopping, best iteration is:\n",
      "[393]\tvalidation's auc: 0.998791\n",
      "  Continuing training … (batch 6)\n",
      "Training until validation scores don't improve for 25 rounds\n",
      "[400]\tvalidation's auc: 0.998981\n",
      "Early stopping, best iteration is:\n",
      "[394]\tvalidation's auc: 0.999192\n",
      "  Continuing training … (batch 7)\n",
      "Training until validation scores don't improve for 25 rounds\n",
      "[400]\tvalidation's auc: 0.999192\n",
      "Early stopping, best iteration is:\n",
      "[395]\tvalidation's auc: 0.999192\n",
      "  Continuing training … (batch 8)\n",
      "Training until validation scores don't improve for 25 rounds\n",
      "[400]\tvalidation's auc: 0.999192\n",
      "Early stopping, best iteration is:\n",
      "[395]\tvalidation's auc: 0.999192\n",
      "  Continuing training … (batch 9)\n",
      "Training until validation scores don't improve for 25 rounds\n",
      "[400]\tvalidation's auc: 0.999192\n",
      "Early stopping, best iteration is:\n",
      "[395]\tvalidation's auc: 0.999192\n",
      "  Continuing training … (batch 10)\n",
      "Training until validation scores don't improve for 25 rounds\n",
      "[400]\tvalidation's auc: 0.999192\n",
      "Early stopping, best iteration is:\n",
      "[395]\tvalidation's auc: 0.999192\n",
      "  Continuing training … (batch 11)\n",
      "Training until validation scores don't improve for 25 rounds\n",
      "[400]\tvalidation's auc: 0.999192\n",
      "Early stopping, best iteration is:\n",
      "[395]\tvalidation's auc: 0.999192\n",
      "  Continuing training … (batch 12)\n",
      "Training until validation scores don't improve for 25 rounds\n",
      "[400]\tvalidation's auc: 0.999192\n",
      "Early stopping, best iteration is:\n",
      "[395]\tvalidation's auc: 0.999192\n",
      "  Continuing training … (batch 13)\n",
      "Training until validation scores don't improve for 25 rounds\n",
      "[400]\tvalidation's auc: 0.999192\n",
      "Early stopping, best iteration is:\n",
      "[395]\tvalidation's auc: 0.999192\n",
      "  Continuing training … (batch 14)\n",
      "Training until validation scores don't improve for 25 rounds\n",
      "[400]\tvalidation's auc: 0.999192\n",
      "Early stopping, best iteration is:\n",
      "[395]\tvalidation's auc: 0.999192\n",
      "  Continuing training … (batch 15)\n",
      "Training until validation scores don't improve for 25 rounds\n",
      "[400]\tvalidation's auc: 0.999192\n",
      "Early stopping, best iteration is:\n",
      "[395]\tvalidation's auc: 0.999192\n",
      "  Continuing training … (batch 16)\n",
      "Training until validation scores don't improve for 25 rounds\n",
      "[400]\tvalidation's auc: 0.999192\n",
      "Early stopping, best iteration is:\n",
      "[395]\tvalidation's auc: 0.999192\n",
      "  Continuing training … (batch 17)\n",
      "Training until validation scores don't improve for 25 rounds\n",
      "[400]\tvalidation's auc: 0.999192\n",
      "Early stopping, best iteration is:\n",
      "[395]\tvalidation's auc: 0.999192\n",
      "  Continuing training … (batch 18)\n",
      "Training until validation scores don't improve for 25 rounds\n",
      "[400]\tvalidation's auc: 0.999192\n",
      "Early stopping, best iteration is:\n",
      "[395]\tvalidation's auc: 0.999192\n",
      "  Continuing training … (batch 19)\n",
      "Training until validation scores don't improve for 25 rounds\n",
      "[400]\tvalidation's auc: 0.999192\n",
      "Early stopping, best iteration is:\n",
      "[395]\tvalidation's auc: 0.999192\n",
      "  Continuing training … (batch 20)\n",
      "Training until validation scores don't improve for 25 rounds\n",
      "[400]\tvalidation's auc: 0.999192\n",
      "Early stopping, best iteration is:\n",
      "[395]\tvalidation's auc: 0.999192\n",
      "  Continuing training … (batch 21)\n",
      "Training until validation scores don't improve for 25 rounds\n",
      "[400]\tvalidation's auc: 0.999192\n",
      "Early stopping, best iteration is:\n",
      "[395]\tvalidation's auc: 0.999192\n",
      "  Continuing training … (batch 22)\n",
      "Training until validation scores don't improve for 25 rounds\n",
      "[400]\tvalidation's auc: 0.999192\n",
      "Early stopping, best iteration is:\n",
      "[395]\tvalidation's auc: 0.999192\n",
      "  Continuing training … (batch 23)\n",
      "Training until validation scores don't improve for 25 rounds\n",
      "[400]\tvalidation's auc: 0.999192\n",
      "Early stopping, best iteration is:\n",
      "[395]\tvalidation's auc: 0.999192\n",
      "  Continuing training … (batch 24)\n",
      "Training until validation scores don't improve for 25 rounds\n",
      "[400]\tvalidation's auc: 0.999192\n",
      "Early stopping, best iteration is:\n",
      "[395]\tvalidation's auc: 0.999192\n",
      "  Continuing training … (batch 25)\n",
      "Training until validation scores don't improve for 25 rounds\n",
      "[400]\tvalidation's auc: 0.999192\n",
      "Early stopping, best iteration is:\n",
      "[395]\tvalidation's auc: 0.999192\n",
      "  Continuing training … (batch 26)\n",
      "Training until validation scores don't improve for 25 rounds\n",
      "[400]\tvalidation's auc: 0.999192\n",
      "Early stopping, best iteration is:\n",
      "[395]\tvalidation's auc: 0.999192\n",
      "  Continuing training … (batch 27)\n",
      "Training until validation scores don't improve for 25 rounds\n",
      "[400]\tvalidation's auc: 0.999192\n",
      "Early stopping, best iteration is:\n",
      "[395]\tvalidation's auc: 0.999192\n",
      "  Continuing training … (batch 28)\n",
      "Training until validation scores don't improve for 25 rounds\n",
      "[400]\tvalidation's auc: 0.999192\n",
      "Early stopping, best iteration is:\n",
      "[395]\tvalidation's auc: 0.999192\n",
      "  Continuing training … (batch 29)\n",
      "Training until validation scores don't improve for 25 rounds\n",
      "[400]\tvalidation's auc: 0.999192\n",
      "Early stopping, best iteration is:\n",
      "[395]\tvalidation's auc: 0.999192\n",
      "  Continuing training … (batch 30)\n",
      "Training until validation scores don't improve for 25 rounds\n",
      "[400]\tvalidation's auc: 0.999192\n",
      "Early stopping, best iteration is:\n",
      "[395]\tvalidation's auc: 0.999192\n",
      "  Continuing training … (batch 31)\n",
      "Training until validation scores don't improve for 25 rounds\n",
      "[400]\tvalidation's auc: 0.999192\n",
      "Early stopping, best iteration is:\n",
      "[395]\tvalidation's auc: 0.999192\n",
      "  Continuing training … (batch 32)\n",
      "Training until validation scores don't improve for 25 rounds\n",
      "[400]\tvalidation's auc: 0.999192\n",
      "Early stopping, best iteration is:\n",
      "[395]\tvalidation's auc: 0.999192\n",
      "  Continuing training … (batch 33)\n",
      "Training until validation scores don't improve for 25 rounds\n",
      "[400]\tvalidation's auc: 0.999192\n",
      "Early stopping, best iteration is:\n",
      "[395]\tvalidation's auc: 0.999192\n",
      "  Continuing training … (batch 34)\n",
      "Training until validation scores don't improve for 25 rounds\n",
      "[400]\tvalidation's auc: 0.999192\n",
      "Early stopping, best iteration is:\n",
      "[395]\tvalidation's auc: 0.999192\n",
      "  Continuing training … (batch 35)\n",
      "Training until validation scores don't improve for 25 rounds\n",
      "[400]\tvalidation's auc: 0.999192\n",
      "Early stopping, best iteration is:\n",
      "[395]\tvalidation's auc: 0.999192\n",
      "  Continuing training … (batch 36)\n",
      "Training until validation scores don't improve for 25 rounds\n",
      "[400]\tvalidation's auc: 0.999192\n",
      "Early stopping, best iteration is:\n",
      "[395]\tvalidation's auc: 0.999192\n",
      "  Continuing training … (batch 37)\n",
      "Training until validation scores don't improve for 25 rounds\n",
      "[400]\tvalidation's auc: 0.999192\n",
      "Early stopping, best iteration is:\n",
      "[395]\tvalidation's auc: 0.999192\n",
      "  Continuing training … (batch 38)\n",
      "Training until validation scores don't improve for 25 rounds\n",
      "[400]\tvalidation's auc: 0.999192\n",
      "Early stopping, best iteration is:\n",
      "[395]\tvalidation's auc: 0.999192\n",
      "  Continuing training … (batch 39)\n",
      "Training until validation scores don't improve for 25 rounds\n",
      "[400]\tvalidation's auc: 0.999192\n",
      "Early stopping, best iteration is:\n",
      "[395]\tvalidation's auc: 0.999192\n",
      "  Continuing training … (batch 40)\n",
      "Training until validation scores don't improve for 25 rounds\n",
      "[400]\tvalidation's auc: 0.999192\n",
      "Early stopping, best iteration is:\n",
      "[395]\tvalidation's auc: 0.999192\n",
      "  Continuing training … (batch 41)\n",
      "Training until validation scores don't improve for 25 rounds\n",
      "[400]\tvalidation's auc: 0.999192\n",
      "Early stopping, best iteration is:\n",
      "[395]\tvalidation's auc: 0.999192\n",
      "  Continuing training … (batch 42)\n",
      "Training until validation scores don't improve for 25 rounds\n",
      "[400]\tvalidation's auc: 0.999192\n",
      "Early stopping, best iteration is:\n",
      "[395]\tvalidation's auc: 0.999192\n",
      "  Continuing training … (batch 43)\n",
      "Training until validation scores don't improve for 25 rounds\n",
      "[400]\tvalidation's auc: 0.999192\n",
      "Early stopping, best iteration is:\n",
      "[395]\tvalidation's auc: 0.999192\n",
      "  Continuing training … (batch 44)\n",
      "Training until validation scores don't improve for 25 rounds\n",
      "[400]\tvalidation's auc: 0.999192\n",
      "Early stopping, best iteration is:\n",
      "[395]\tvalidation's auc: 0.999192\n",
      "  Continuing training … (batch 45)\n",
      "Training until validation scores don't improve for 25 rounds\n",
      "[400]\tvalidation's auc: 0.999192\n",
      "Early stopping, best iteration is:\n",
      "[395]\tvalidation's auc: 0.999192\n",
      "  Continuing training … (batch 46)\n",
      "Training until validation scores don't improve for 25 rounds\n",
      "[400]\tvalidation's auc: 0.999192\n",
      "Early stopping, best iteration is:\n",
      "[395]\tvalidation's auc: 0.999192\n",
      "  Continuing training … (batch 47)\n",
      "Training until validation scores don't improve for 25 rounds\n",
      "[400]\tvalidation's auc: 0.999192\n",
      "Early stopping, best iteration is:\n",
      "[395]\tvalidation's auc: 0.999192\n",
      "  Continuing training … (batch 48)\n",
      "Training until validation scores don't improve for 25 rounds\n",
      "[400]\tvalidation's auc: 0.999192\n",
      "Early stopping, best iteration is:\n",
      "[395]\tvalidation's auc: 0.999192\n",
      "  Continuing training … (batch 49)\n",
      "Training until validation scores don't improve for 25 rounds\n",
      "[400]\tvalidation's auc: 0.999192\n",
      "Early stopping, best iteration is:\n",
      "[395]\tvalidation's auc: 0.999192\n",
      "  Continuing training … (batch 50)\n",
      "Training until validation scores don't improve for 25 rounds\n",
      "[400]\tvalidation's auc: 0.999192\n",
      "Early stopping, best iteration is:\n",
      "[395]\tvalidation's auc: 0.999192\n",
      "  Continuing training … (batch 51)\n",
      "Training until validation scores don't improve for 25 rounds\n",
      "[400]\tvalidation's auc: 0.999192\n",
      "Early stopping, best iteration is:\n",
      "[395]\tvalidation's auc: 0.999192\n",
      "  Continuing training … (batch 52)\n",
      "Training until validation scores don't improve for 25 rounds\n",
      "[400]\tvalidation's auc: 0.999192\n",
      "Early stopping, best iteration is:\n",
      "[395]\tvalidation's auc: 0.999192\n",
      "  Continuing training … (batch 53)\n",
      "Training until validation scores don't improve for 25 rounds\n",
      "[400]\tvalidation's auc: 0.999192\n",
      "Early stopping, best iteration is:\n",
      "[395]\tvalidation's auc: 0.999192\n",
      "  Continuing training … (batch 54)\n",
      "Training until validation scores don't improve for 25 rounds\n",
      "[400]\tvalidation's auc: 0.999192\n",
      "Early stopping, best iteration is:\n",
      "[395]\tvalidation's auc: 0.999192\n",
      "  Continuing training … (batch 55)\n",
      "Training until validation scores don't improve for 25 rounds\n",
      "[400]\tvalidation's auc: 0.999192\n",
      "Early stopping, best iteration is:\n",
      "[395]\tvalidation's auc: 0.999192\n",
      "  Continuing training … (batch 56)\n",
      "Training until validation scores don't improve for 25 rounds\n",
      "[400]\tvalidation's auc: 0.999192\n",
      "Early stopping, best iteration is:\n",
      "[395]\tvalidation's auc: 0.999192\n",
      "  Continuing training … (batch 57)\n",
      "Training until validation scores don't improve for 25 rounds\n",
      "[400]\tvalidation's auc: 0.999192\n",
      "Early stopping, best iteration is:\n",
      "[395]\tvalidation's auc: 0.999192\n",
      "  Continuing training … (batch 58)\n",
      "Training until validation scores don't improve for 25 rounds\n",
      "[400]\tvalidation's auc: 0.999192\n",
      "Early stopping, best iteration is:\n",
      "[395]\tvalidation's auc: 0.999192\n",
      "  Continuing training … (batch 59)\n",
      "Training until validation scores don't improve for 25 rounds\n",
      "[400]\tvalidation's auc: 0.999192\n",
      "Early stopping, best iteration is:\n",
      "[395]\tvalidation's auc: 0.999192\n",
      "  Continuing training … (batch 60)\n",
      "Training until validation scores don't improve for 25 rounds\n",
      "[400]\tvalidation's auc: 0.999192\n",
      "Early stopping, best iteration is:\n",
      "[395]\tvalidation's auc: 0.999192\n",
      "  Continuing training … (batch 61)\n",
      "Training until validation scores don't improve for 25 rounds\n",
      "[400]\tvalidation's auc: 0.999192\n",
      "Early stopping, best iteration is:\n",
      "[395]\tvalidation's auc: 0.999192\n",
      "  Continuing training … (batch 62)\n",
      "Training until validation scores don't improve for 25 rounds\n",
      "[400]\tvalidation's auc: 0.999192\n",
      "Early stopping, best iteration is:\n",
      "[395]\tvalidation's auc: 0.999192\n",
      "  Continuing training … (batch 63)\n",
      "Training until validation scores don't improve for 25 rounds\n",
      "[400]\tvalidation's auc: 0.999192\n",
      "Early stopping, best iteration is:\n",
      "[395]\tvalidation's auc: 0.999192\n",
      "  Continuing training … (batch 64)\n",
      "Training until validation scores don't improve for 25 rounds\n",
      "[400]\tvalidation's auc: 0.999192\n",
      "Early stopping, best iteration is:\n",
      "[395]\tvalidation's auc: 0.999192\n",
      "  Continuing training … (batch 65)\n",
      "Training until validation scores don't improve for 25 rounds\n",
      "[400]\tvalidation's auc: 0.999192\n",
      "Early stopping, best iteration is:\n",
      "[395]\tvalidation's auc: 0.999192\n",
      "  Continuing training … (batch 66)\n",
      "Training until validation scores don't improve for 25 rounds\n",
      "[400]\tvalidation's auc: 0.999192\n",
      "Early stopping, best iteration is:\n",
      "[395]\tvalidation's auc: 0.999192\n",
      "  Continuing training … (batch 67)\n",
      "Training until validation scores don't improve for 25 rounds\n",
      "[400]\tvalidation's auc: 0.999192\n",
      "Early stopping, best iteration is:\n",
      "[395]\tvalidation's auc: 0.999192\n",
      "  Continuing training … (batch 68)\n",
      "Training until validation scores don't improve for 25 rounds\n",
      "[400]\tvalidation's auc: 0.999192\n",
      "Early stopping, best iteration is:\n",
      "[395]\tvalidation's auc: 0.999192\n",
      "  Continuing training … (batch 69)\n",
      "Training until validation scores don't improve for 25 rounds\n",
      "[400]\tvalidation's auc: 0.999192\n",
      "Early stopping, best iteration is:\n",
      "[395]\tvalidation's auc: 0.999192\n",
      "  Continuing training … (batch 70)\n",
      "Training until validation scores don't improve for 25 rounds\n",
      "[400]\tvalidation's auc: 0.999192\n",
      "Early stopping, best iteration is:\n",
      "[395]\tvalidation's auc: 0.999192\n",
      "  Continuing training … (batch 71)\n",
      "Training until validation scores don't improve for 25 rounds\n",
      "[400]\tvalidation's auc: 0.999192\n",
      "Early stopping, best iteration is:\n",
      "[395]\tvalidation's auc: 0.999192\n",
      "  Continuing training … (batch 72)\n",
      "Training until validation scores don't improve for 25 rounds\n",
      "[400]\tvalidation's auc: 0.999192\n",
      "Early stopping, best iteration is:\n",
      "[395]\tvalidation's auc: 0.999192\n",
      "  Continuing training … (batch 73)\n",
      "Training until validation scores don't improve for 25 rounds\n",
      "[400]\tvalidation's auc: 0.999192\n",
      "Early stopping, best iteration is:\n",
      "[395]\tvalidation's auc: 0.999192\n",
      "  Continuing training … (batch 74)\n",
      "Training until validation scores don't improve for 25 rounds\n",
      "[400]\tvalidation's auc: 0.999192\n",
      "Early stopping, best iteration is:\n",
      "[395]\tvalidation's auc: 0.999192\n",
      "  Continuing training … (batch 75)\n",
      "Training until validation scores don't improve for 25 rounds\n",
      "[400]\tvalidation's auc: 0.999192\n",
      "Early stopping, best iteration is:\n",
      "[395]\tvalidation's auc: 0.999192\n",
      "  Continuing training … (batch 76)\n",
      "Training until validation scores don't improve for 25 rounds\n",
      "[400]\tvalidation's auc: 0.999192\n",
      "Early stopping, best iteration is:\n",
      "[395]\tvalidation's auc: 0.999192\n",
      "  Continuing training … (batch 77)\n",
      "Training until validation scores don't improve for 25 rounds\n",
      "[400]\tvalidation's auc: 0.999192\n",
      "Early stopping, best iteration is:\n",
      "[395]\tvalidation's auc: 0.999192\n",
      "  Continuing training … (batch 78)\n",
      "Training until validation scores don't improve for 25 rounds\n",
      "[400]\tvalidation's auc: 0.999192\n",
      "Early stopping, best iteration is:\n",
      "[395]\tvalidation's auc: 0.999192\n",
      "  Continuing training … (batch 79)\n",
      "Training until validation scores don't improve for 25 rounds\n",
      "[400]\tvalidation's auc: 0.999192\n",
      "Early stopping, best iteration is:\n",
      "[395]\tvalidation's auc: 0.999192\n",
      "  Continuing training … (batch 80)\n",
      "Training until validation scores don't improve for 25 rounds\n",
      "[400]\tvalidation's auc: 0.999192\n",
      "Early stopping, best iteration is:\n",
      "[395]\tvalidation's auc: 0.999192\n",
      "\n",
      "Processing: D:\\Canada\\Subjects\\Semester -2\\AIDI-2005-02 CAPSTONE TERM ll\\Bot_detector\\data\\cleaned\\Thursday-01-03-2018_TrafficForML_CICFlowMeter.parquet\n",
      "  Continuing training … (batch 1)\n",
      "Training until validation scores don't improve for 25 rounds\n",
      "[400]\tvalidation's auc: 0.999192\n",
      "Early stopping, best iteration is:\n",
      "[395]\tvalidation's auc: 0.999192\n",
      "  Continuing training … (batch 2)\n",
      "Training until validation scores don't improve for 25 rounds\n",
      "[400]\tvalidation's auc: 0.999134\n",
      "Early stopping, best iteration is:\n",
      "[396]\tvalidation's auc: 0.999198\n",
      "  Continuing training … (batch 3)\n",
      "Training until validation scores don't improve for 25 rounds\n",
      "[400]\tvalidation's auc: 0.99892\n",
      "Early stopping, best iteration is:\n",
      "[397]\tvalidation's auc: 0.999038\n",
      "  Continuing training … (batch 4)\n",
      "Training until validation scores don't improve for 25 rounds\n",
      "[400]\tvalidation's auc: 0.999038\n",
      "Early stopping, best iteration is:\n",
      "[398]\tvalidation's auc: 0.999038\n",
      "\n",
      "Processing: D:\\Canada\\Subjects\\Semester -2\\AIDI-2005-02 CAPSTONE TERM ll\\Bot_detector\\data\\cleaned\\Thursday-15-02-2018_TrafficForML_CICFlowMeter.parquet\n",
      "  Continuing training … (batch 1)\n",
      "Training until validation scores don't improve for 25 rounds\n",
      "[400]\tvalidation's auc: 0.996051\n",
      "Early stopping, best iteration is:\n",
      "[398]\tvalidation's auc: 0.9975\n",
      "  Continuing training … (batch 2)\n",
      "Training until validation scores don't improve for 25 rounds\n",
      "[400]\tvalidation's auc: 0.9975\n",
      "Early stopping, best iteration is:\n",
      "[399]\tvalidation's auc: 0.9975\n",
      "  Continuing training … (batch 3)\n",
      "Training until validation scores don't improve for 25 rounds\n",
      "[400]\tvalidation's auc: 0.9975\n",
      "Early stopping, best iteration is:\n",
      "[399]\tvalidation's auc: 0.9975\n",
      "  Continuing training … (batch 4)\n",
      "Training until validation scores don't improve for 25 rounds\n",
      "[400]\tvalidation's auc: 0.9975\n",
      "Early stopping, best iteration is:\n",
      "[399]\tvalidation's auc: 0.9975\n",
      "  Continuing training … (batch 5)\n",
      "Training until validation scores don't improve for 25 rounds\n",
      "[400]\tvalidation's auc: 0.9975\n",
      "Early stopping, best iteration is:\n",
      "[399]\tvalidation's auc: 0.9975\n",
      "  Continuing training … (batch 6)\n",
      "Training until validation scores don't improve for 25 rounds\n",
      "[400]\tvalidation's auc: 0.9975\n",
      "Early stopping, best iteration is:\n",
      "[399]\tvalidation's auc: 0.9975\n",
      "  Continuing training … (batch 7)\n",
      "Training until validation scores don't improve for 25 rounds\n",
      "[400]\tvalidation's auc: 0.9975\n",
      "Early stopping, best iteration is:\n",
      "[399]\tvalidation's auc: 0.9975\n",
      "  Continuing training … (batch 8)\n",
      "Training until validation scores don't improve for 25 rounds\n",
      "[400]\tvalidation's auc: 0.9975\n",
      "Early stopping, best iteration is:\n",
      "[399]\tvalidation's auc: 0.9975\n",
      "  Continuing training … (batch 9)\n",
      "Training until validation scores don't improve for 25 rounds\n",
      "[400]\tvalidation's auc: 0.9975\n",
      "Early stopping, best iteration is:\n",
      "[399]\tvalidation's auc: 0.9975\n",
      "  Continuing training … (batch 10)\n",
      "Training until validation scores don't improve for 25 rounds\n",
      "[400]\tvalidation's auc: 0.9975\n",
      "Early stopping, best iteration is:\n",
      "[399]\tvalidation's auc: 0.9975\n",
      "  Continuing training … (batch 11)\n",
      "Training until validation scores don't improve for 25 rounds\n",
      "[400]\tvalidation's auc: 0.9975\n",
      "Early stopping, best iteration is:\n",
      "[399]\tvalidation's auc: 0.9975\n",
      "\n",
      "Processing: D:\\Canada\\Subjects\\Semester -2\\AIDI-2005-02 CAPSTONE TERM ll\\Bot_detector\\data\\cleaned\\Thursday-22-02-2018_TrafficForML_CICFlowMeter.parquet\n",
      "  Continuing training … (batch 1)\n",
      "Training until validation scores don't improve for 25 rounds\n",
      "[400]\tvalidation's auc: 0.9972\n",
      "[425]\tvalidation's auc: 0.996343\n",
      "Early stopping, best iteration is:\n",
      "[402]\tvalidation's auc: 0.997258\n",
      "  Continuing training … (batch 2)\n",
      "Training until validation scores don't improve for 25 rounds\n",
      "[425]\tvalidation's auc: 0.997258\n",
      "Early stopping, best iteration is:\n",
      "[403]\tvalidation's auc: 0.997258\n",
      "  Continuing training … (batch 3)\n",
      "Training until validation scores don't improve for 25 rounds\n",
      "[425]\tvalidation's auc: 0.997258\n",
      "Early stopping, best iteration is:\n",
      "[403]\tvalidation's auc: 0.997258\n",
      "  Continuing training … (batch 4)\n",
      "Training until validation scores don't improve for 25 rounds\n",
      "[425]\tvalidation's auc: 0.997258\n",
      "Early stopping, best iteration is:\n",
      "[403]\tvalidation's auc: 0.997258\n",
      "  Continuing training … (batch 5)\n",
      "Training until validation scores don't improve for 25 rounds\n",
      "[425]\tvalidation's auc: 0.997258\n",
      "Early stopping, best iteration is:\n",
      "[403]\tvalidation's auc: 0.997258\n",
      "  Continuing training … (batch 6)\n",
      "Training until validation scores don't improve for 25 rounds\n",
      "[425]\tvalidation's auc: 0.997258\n",
      "Early stopping, best iteration is:\n",
      "[403]\tvalidation's auc: 0.997258\n",
      "  Continuing training … (batch 7)\n",
      "Training until validation scores don't improve for 25 rounds\n",
      "[425]\tvalidation's auc: 0.997258\n",
      "Early stopping, best iteration is:\n",
      "[403]\tvalidation's auc: 0.997258\n",
      "  Continuing training … (batch 8)\n",
      "Training until validation scores don't improve for 25 rounds\n",
      "[425]\tvalidation's auc: 0.997258\n",
      "Early stopping, best iteration is:\n",
      "[403]\tvalidation's auc: 0.997258\n",
      "  Continuing training … (batch 9)\n",
      "Training until validation scores don't improve for 25 rounds\n",
      "[425]\tvalidation's auc: 0.997258\n",
      "Early stopping, best iteration is:\n",
      "[403]\tvalidation's auc: 0.997258\n",
      "  Continuing training … (batch 10)\n",
      "Training until validation scores don't improve for 25 rounds\n",
      "[425]\tvalidation's auc: 0.997258\n",
      "Early stopping, best iteration is:\n",
      "[403]\tvalidation's auc: 0.997258\n",
      "  Continuing training … (batch 11)\n",
      "Training until validation scores don't improve for 25 rounds\n",
      "[425]\tvalidation's auc: 0.997258\n",
      "Early stopping, best iteration is:\n",
      "[403]\tvalidation's auc: 0.997258\n",
      "\n",
      "Processing: D:\\Canada\\Subjects\\Semester -2\\AIDI-2005-02 CAPSTONE TERM ll\\Bot_detector\\data\\cleaned\\Wednesday-14-02-2018_TrafficForML_CICFlowMeter.parquet\n",
      "  Continuing training … (batch 1)\n",
      "Training until validation scores don't improve for 25 rounds\n",
      "[425]\tvalidation's auc: 0.99706\n",
      "Early stopping, best iteration is:\n",
      "[403]\tvalidation's auc: 0.997255\n",
      "  Continuing training … (batch 2)\n",
      "Training until validation scores don't improve for 25 rounds\n",
      "[425]\tvalidation's auc: 0.996729\n",
      "Early stopping, best iteration is:\n",
      "[404]\tvalidation's auc: 0.997216\n",
      "  Continuing training … (batch 3)\n",
      "Training until validation scores don't improve for 25 rounds\n",
      "[425]\tvalidation's auc: 0.997195\n",
      "Early stopping, best iteration is:\n",
      "[420]\tvalidation's auc: 0.997243\n",
      "  Continuing training … (batch 4)\n",
      "Training until validation scores don't improve for 25 rounds\n",
      "[425]\tvalidation's auc: 0.995559\n",
      "Early stopping, best iteration is:\n",
      "[421]\tvalidation's auc: 0.996863\n",
      "  Continuing training … (batch 5)\n",
      "Training until validation scores don't improve for 25 rounds\n",
      "[425]\tvalidation's auc: 0.996863\n",
      "Early stopping, best iteration is:\n",
      "[422]\tvalidation's auc: 0.996863\n",
      "  Continuing training … (batch 6)\n",
      "Training until validation scores don't improve for 25 rounds\n",
      "[425]\tvalidation's auc: 0.996863\n",
      "Early stopping, best iteration is:\n",
      "[422]\tvalidation's auc: 0.996863\n",
      "  Continuing training … (batch 7)\n",
      "Training until validation scores don't improve for 25 rounds\n",
      "[425]\tvalidation's auc: 0.996863\n",
      "Early stopping, best iteration is:\n",
      "[422]\tvalidation's auc: 0.996863\n",
      "  Continuing training … (batch 8)\n",
      "Training until validation scores don't improve for 25 rounds\n",
      "[425]\tvalidation's auc: 0.996863\n",
      "Early stopping, best iteration is:\n",
      "[422]\tvalidation's auc: 0.996863\n",
      "  Continuing training … (batch 9)\n",
      "Training until validation scores don't improve for 25 rounds\n",
      "[425]\tvalidation's auc: 0.996863\n",
      "Early stopping, best iteration is:\n",
      "[422]\tvalidation's auc: 0.996863\n",
      "  Continuing training … (batch 10)\n",
      "Training until validation scores don't improve for 25 rounds\n",
      "[425]\tvalidation's auc: 0.996863\n",
      "Early stopping, best iteration is:\n",
      "[422]\tvalidation's auc: 0.996863\n",
      "  Continuing training … (batch 11)\n",
      "Training until validation scores don't improve for 25 rounds\n",
      "[425]\tvalidation's auc: 0.996863\n",
      "Early stopping, best iteration is:\n",
      "[422]\tvalidation's auc: 0.996863\n",
      "\n",
      "Model saved as: final_lightgbm.txt\n",
      "\n",
      "Evaluating on: D:\\Canada\\Subjects\\Semester -2\\AIDI-2005-02 CAPSTONE TERM ll\\Bot_detector\\data\\cleaned\\Wednesday-28-02-2018_TrafficForML_CICFlowMeter.parquet\n",
      "\n",
      "=== Results for threshold=0.5 ===\n",
      "Accuracy: 0.6556744651108926\n",
      "Confusion Matrix:\n",
      " [[379312 164888]\n",
      " [ 46208  22663]]\n",
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0     0.8914    0.6970    0.7823    544200\n",
      "           1     0.1208    0.3291    0.1768     68871\n",
      "\n",
      "    accuracy                         0.6557    613071\n",
      "   macro avg     0.5061    0.5130    0.4795    613071\n",
      "weighted avg     0.8048    0.6557    0.7143    613071\n",
      "\n",
      "\n",
      "=== Results for threshold=0.4 ===\n",
      "Accuracy: 0.5863953767181942\n",
      "Confusion Matrix:\n",
      " [[331255 212945]\n",
      " [ 40624  28247]]\n",
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0     0.8908    0.6087    0.7232    544200\n",
      "           1     0.1171    0.4101    0.1822     68871\n",
      "\n",
      "    accuracy                         0.5864    613071\n",
      "   macro avg     0.5039    0.5094    0.4527    613071\n",
      "weighted avg     0.8039    0.5864    0.6624    613071\n",
      "\n",
      "\n",
      "=== Results for threshold=0.3 ===\n",
      "Accuracy: 0.4911176682635453\n",
      "Confusion Matrix:\n",
      " [[264449 279751]\n",
      " [ 32230  36641]]\n",
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0     0.8914    0.4859    0.6290    544200\n",
      "           1     0.1158    0.5320    0.1902     68871\n",
      "\n",
      "    accuracy                         0.4911    613071\n",
      "   macro avg     0.5036    0.5090    0.4096    613071\n",
      "weighted avg     0.8042    0.4911    0.5797    613071\n",
      "\n",
      "\n",
      "=== Results for threshold=0.2 ===\n",
      "Accuracy: 0.25412717287231007\n",
      "Confusion Matrix:\n",
      " [[ 99411 444789]\n",
      " [ 12484  56387]]\n",
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0     0.8884    0.1827    0.3030    544200\n",
      "           1     0.1125    0.8187    0.1978     68871\n",
      "\n",
      "    accuracy                         0.2541    613071\n",
      "   macro avg     0.5005    0.5007    0.2504    613071\n",
      "weighted avg     0.8013    0.2541    0.2912    613071\n",
      "\n",
      "\n",
      "=== Results for threshold=0.1 ===\n",
      "Accuracy: 0.1791766369637448\n",
      "Confusion Matrix:\n",
      " [[ 46728 497472]\n",
      " [  5751  63120]]\n",
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0     0.8904    0.0859    0.1566    544200\n",
      "           1     0.1126    0.9165    0.2006     68871\n",
      "\n",
      "    accuracy                         0.1792    613071\n",
      "   macro avg     0.5015    0.5012    0.1786    613071\n",
      "weighted avg     0.8030    0.1792    0.1616    613071\n",
      "\n",
      "\n",
      "=== Results for threshold=0.05 ===\n",
      "Accuracy: 0.14278117868892837\n",
      "Confusion Matrix:\n",
      " [[ 21126 523074]\n",
      " [  2462  66409]]\n",
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0     0.8956    0.0388    0.0744    544200\n",
      "           1     0.1127    0.9643    0.2017     68871\n",
      "\n",
      "    accuracy                         0.1428    613071\n",
      "   macro avg     0.5041    0.5015    0.1381    613071\n",
      "weighted avg     0.8077    0.1428    0.0887    613071\n",
      "\n",
      "\n",
      "=== Results for threshold=0.01 ===\n",
      "Accuracy: 0.11269983411383021\n",
      "Confusion Matrix:\n",
      " [[   256 543944]\n",
      " [    34  68837]]\n",
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0     0.8828    0.0005    0.0009    544200\n",
      "           1     0.1123    0.9995    0.2020     68871\n",
      "\n",
      "    accuracy                         0.1127    613071\n",
      "   macro avg     0.4975    0.5000    0.1015    613071\n",
      "weighted avg     0.7962    0.1127    0.0235    613071\n",
      "\n",
      "\n",
      "Done! The model and thresholded evaluation results are printed.\n"
     ]
    }
   ],
   "source": [
    "import pyarrow.parquet as pq\n",
    "import lightgbm as lgb\n",
    "import numpy as np\n",
    "import os\n",
    "from glob import glob\n",
    "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
    "\n",
    "# === CONFIGURATION ===\n",
    "CLEANED_DIR = r\"D:\\Canada\\Subjects\\Semester -2\\AIDI-2005-02 CAPSTONE TERM ll\\Bot_detector\\data\\cleaned\"\n",
    "MODEL_PATH = \"final_lightgbm.txt\"\n",
    "BATCH_SIZE = 100_000\n",
    "\n",
    "file_list = sorted(glob(os.path.join(CLEANED_DIR, \"*.parquet\")))\n",
    "if len(file_list) < 3:\n",
    "    raise Exception(\"Need at least 3 files for train/val/test split!\")\n",
    "train_files = file_list[:-2]\n",
    "val_file = file_list[-2]\n",
    "test_file = file_list[-1]\n",
    "\n",
    "print(\"Train files:\", train_files)\n",
    "print(\"Validation file:\", val_file)\n",
    "print(\"Test file:\", test_file)\n",
    "\n",
    "print(\"\\nCalculating scale_pos_weight from train files...\")\n",
    "total_0, total_1 = 0, 0\n",
    "for f in train_files:\n",
    "    parquet = pq.ParquetFile(f)\n",
    "    for batch in parquet.iter_batches(batch_size=BATCH_SIZE, columns=['Label']):\n",
    "        arr = batch.to_pandas()['Label'].value_counts()\n",
    "        total_0 += arr.get(0, 0)\n",
    "        total_1 += arr.get(1, 0)\n",
    "\n",
    "scale_pos_weight = total_0 / total_1\n",
    "print(f\"scale_pos_weight: {scale_pos_weight:.3f}\")\n",
    "\n",
    "sample = pq.read_table(train_files[0], columns=None).to_pandas().iloc[:1]\n",
    "feature_cols = [col for col in sample.columns if col != 'Label']\n",
    "\n",
    "params = {\n",
    "    'objective': 'binary',\n",
    "    'boosting_type': 'gbdt',\n",
    "    'metric': 'auc',\n",
    "    'learning_rate': 0.02,\n",
    "    'num_leaves': 64,\n",
    "    'max_depth': 7,\n",
    "    'min_child_samples': 40,\n",
    "    'subsample': 0.8,\n",
    "    'colsample_bytree': 0.8,\n",
    "    'scale_pos_weight': scale_pos_weight,\n",
    "    'verbose': -1\n",
    "}\n",
    "\n",
    "print(\"\\nStarting batch-wise training with early stopping...\")\n",
    "val_parquet = pq.ParquetFile(val_file)\n",
    "val_batch = next(val_parquet.iter_batches(batch_size=BATCH_SIZE))\n",
    "df_val = val_batch.to_pandas()\n",
    "X_val = df_val[feature_cols].astype(np.float32)\n",
    "y_val = df_val['Label'].astype(np.int32)\n",
    "lgb_val = lgb.Dataset(X_val, y_val, free_raw_data=False)  # <--- Fix here\n",
    "\n",
    "booster = None\n",
    "for file_path in train_files:\n",
    "    print(f\"\\nProcessing: {file_path}\")\n",
    "    parquet_file = pq.ParquetFile(file_path)\n",
    "    batch_idx = 1\n",
    "    for batch in parquet_file.iter_batches(batch_size=BATCH_SIZE):\n",
    "        df = batch.to_pandas()\n",
    "        X = df[feature_cols].astype(np.float32)\n",
    "        y = df['Label'].astype(np.int32)\n",
    "        lgb_train = lgb.Dataset(X, y, free_raw_data=False)  # <--- Fix here\n",
    "        if booster is None:\n",
    "            print(\"  Training new model with early stopping …\")\n",
    "            booster = lgb.train(\n",
    "                params,\n",
    "                lgb_train,\n",
    "                num_boost_round=500,\n",
    "                valid_sets=[lgb_val],\n",
    "                valid_names=['validation'],\n",
    "                callbacks=[lgb.early_stopping(25), lgb.log_evaluation(25)]\n",
    "            )\n",
    "        else:\n",
    "            print(f\"  Continuing training … (batch {batch_idx})\")\n",
    "            booster = lgb.train(\n",
    "                params,\n",
    "                lgb_train,\n",
    "                num_boost_round=500,\n",
    "                init_model=booster,\n",
    "                valid_sets=[lgb_val],\n",
    "                valid_names=['validation'],\n",
    "                callbacks=[lgb.early_stopping(25), lgb.log_evaluation(25)]\n",
    "            )\n",
    "        del df, X, y, lgb_train\n",
    "        batch_idx += 1\n",
    "\n",
    "booster.save_model(MODEL_PATH)\n",
    "print(\"\\nModel saved as:\", MODEL_PATH)\n",
    "\n",
    "print(f\"\\nEvaluating on: {test_file}\")\n",
    "test_parquet = pq.ParquetFile(test_file)\n",
    "y_true_all, y_pred_prob = [], []\n",
    "for batch in test_parquet.iter_batches(batch_size=BATCH_SIZE):\n",
    "    df = batch.to_pandas()\n",
    "    X = df[feature_cols].astype(np.float32)\n",
    "    y = df['Label'].astype(np.int32)\n",
    "    y_prob = booster.predict(X)\n",
    "    y_true_all.extend(y.tolist())\n",
    "    y_pred_prob.extend(y_prob.tolist())\n",
    "    del df, X, y, y_prob\n",
    "\n",
    "thresholds = [0.5, 0.4, 0.3, 0.2, 0.1, 0.05, 0.01]\n",
    "for threshold in thresholds:\n",
    "    y_pred = (np.array(y_pred_prob) > threshold).astype(int)\n",
    "    print(f\"\\n=== Results for threshold={threshold} ===\")\n",
    "    print(\"Accuracy:\", accuracy_score(y_true_all, y_pred))\n",
    "    print(\"Confusion Matrix:\\n\", confusion_matrix(y_true_all, y_pred))\n",
    "    print(\"Classification Report:\\n\", classification_report(y_true_all, y_pred, digits=4))\n",
    "\n",
    "print(\"\\nDone! The model and thresholded evaluation results are printed.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12df5a6f",
   "metadata": {},
   "source": [
    "### Using optuna (Tuning)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "8f138e35",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\Canada\\Subjects\\Semester -2\\AIDI-2005-02 CAPSTONE TERM ll\\Bot_detector\\.venv\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "[I 2025-06-05 19:13:56,685] A new study created in memory with name: no-name-663e31c4-3b36-4269-b4c4-34b56448bee2\n",
      "C:\\Users\\aroma\\AppData\\Local\\Temp\\ipykernel_32672\\58112322.py:30: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  'learning_rate': trial.suggest_loguniform('learning_rate', 0.01, 0.2),\n",
      "C:\\Users\\aroma\\AppData\\Local\\Temp\\ipykernel_32672\\58112322.py:34: FutureWarning: suggest_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float instead.\n",
      "  'subsample': trial.suggest_uniform('subsample', 0.6, 1.0),\n",
      "C:\\Users\\aroma\\AppData\\Local\\Temp\\ipykernel_32672\\58112322.py:35: FutureWarning: suggest_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float instead.\n",
      "  'colsample_bytree': trial.suggest_uniform('colsample_bytree', 0.6, 1.0),\n",
      "C:\\Users\\aroma\\AppData\\Local\\Temp\\ipykernel_32672\\58112322.py:36: FutureWarning: suggest_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float instead.\n",
      "  'scale_pos_weight': trial.suggest_uniform('scale_pos_weight', 4.0, 8.0),  # Use around your computed value\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training until validation scores don't improve for 25 rounds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-06-05 19:13:57,569] Trial 0 finished with value: 0.9999164508313142 and parameters: {'learning_rate': 0.04560353410164083, 'num_leaves': 101, 'max_depth': 20, 'min_child_samples': 95, 'subsample': 0.7884208766149358, 'colsample_bytree': 0.6499557736337581, 'scale_pos_weight': 4.303589271823835}. Best is trial 0 with value: 0.9999164508313142.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping, best iteration is:\n",
      "[58]\tvalidation's auc: 1\n",
      "Training until validation scores don't improve for 25 rounds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\aroma\\AppData\\Local\\Temp\\ipykernel_32672\\58112322.py:30: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  'learning_rate': trial.suggest_loguniform('learning_rate', 0.01, 0.2),\n",
      "C:\\Users\\aroma\\AppData\\Local\\Temp\\ipykernel_32672\\58112322.py:34: FutureWarning: suggest_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float instead.\n",
      "  'subsample': trial.suggest_uniform('subsample', 0.6, 1.0),\n",
      "C:\\Users\\aroma\\AppData\\Local\\Temp\\ipykernel_32672\\58112322.py:35: FutureWarning: suggest_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float instead.\n",
      "  'colsample_bytree': trial.suggest_uniform('colsample_bytree', 0.6, 1.0),\n",
      "C:\\Users\\aroma\\AppData\\Local\\Temp\\ipykernel_32672\\58112322.py:36: FutureWarning: suggest_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float instead.\n",
      "  'scale_pos_weight': trial.suggest_uniform('scale_pos_weight', 4.0, 8.0),  # Use around your computed value\n",
      "[I 2025-06-05 19:13:57,996] Trial 1 finished with value: 0.9999331595481585 and parameters: {'learning_rate': 0.03598716260032116, 'num_leaves': 88, 'max_depth': 10, 'min_child_samples': 90, 'subsample': 0.8579528415284592, 'colsample_bytree': 0.7069807607708449, 'scale_pos_weight': 5.7485223337623985}. Best is trial 1 with value: 0.9999331595481585.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping, best iteration is:\n",
      "[83]\tvalidation's auc: 1\n",
      "Training until validation scores don't improve for 25 rounds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\aroma\\AppData\\Local\\Temp\\ipykernel_32672\\58112322.py:30: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  'learning_rate': trial.suggest_loguniform('learning_rate', 0.01, 0.2),\n",
      "C:\\Users\\aroma\\AppData\\Local\\Temp\\ipykernel_32672\\58112322.py:34: FutureWarning: suggest_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float instead.\n",
      "  'subsample': trial.suggest_uniform('subsample', 0.6, 1.0),\n",
      "C:\\Users\\aroma\\AppData\\Local\\Temp\\ipykernel_32672\\58112322.py:35: FutureWarning: suggest_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float instead.\n",
      "  'colsample_bytree': trial.suggest_uniform('colsample_bytree', 0.6, 1.0),\n",
      "C:\\Users\\aroma\\AppData\\Local\\Temp\\ipykernel_32672\\58112322.py:36: FutureWarning: suggest_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float instead.\n",
      "  'scale_pos_weight': trial.suggest_uniform('scale_pos_weight', 4.0, 8.0),  # Use around your computed value\n",
      "[I 2025-06-05 19:13:58,295] Trial 2 finished with value: 0.9986648865153538 and parameters: {'learning_rate': 0.013645793457541272, 'num_leaves': 150, 'max_depth': 12, 'min_child_samples': 36, 'subsample': 0.88420026359968, 'colsample_bytree': 0.8609711963632429, 'scale_pos_weight': 4.0334154974828405}. Best is trial 1 with value: 0.9999331595481585.\n",
      "C:\\Users\\aroma\\AppData\\Local\\Temp\\ipykernel_32672\\58112322.py:30: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  'learning_rate': trial.suggest_loguniform('learning_rate', 0.01, 0.2),\n",
      "C:\\Users\\aroma\\AppData\\Local\\Temp\\ipykernel_32672\\58112322.py:34: FutureWarning: suggest_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float instead.\n",
      "  'subsample': trial.suggest_uniform('subsample', 0.6, 1.0),\n",
      "C:\\Users\\aroma\\AppData\\Local\\Temp\\ipykernel_32672\\58112322.py:35: FutureWarning: suggest_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float instead.\n",
      "  'colsample_bytree': trial.suggest_uniform('colsample_bytree', 0.6, 1.0),\n",
      "C:\\Users\\aroma\\AppData\\Local\\Temp\\ipykernel_32672\\58112322.py:36: FutureWarning: suggest_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float instead.\n",
      "  'scale_pos_weight': trial.suggest_uniform('scale_pos_weight', 4.0, 8.0),  # Use around your computed value\n",
      "[I 2025-06-05 19:13:58,468] Trial 3 finished with value: 0.9987148889296861 and parameters: {'learning_rate': 0.017373010231835437, 'num_leaves': 34, 'max_depth': 7, 'min_child_samples': 71, 'subsample': 0.9328012656975175, 'colsample_bytree': 0.9197843003863173, 'scale_pos_weight': 5.010953408987704}. Best is trial 1 with value: 0.9999331595481585.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping, best iteration is:\n",
      "[39]\tvalidation's auc: 0.999999\n",
      "Training until validation scores don't improve for 25 rounds\n",
      "Early stopping, best iteration is:\n",
      "[18]\tvalidation's auc: 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\aroma\\AppData\\Local\\Temp\\ipykernel_32672\\58112322.py:30: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  'learning_rate': trial.suggest_loguniform('learning_rate', 0.01, 0.2),\n",
      "C:\\Users\\aroma\\AppData\\Local\\Temp\\ipykernel_32672\\58112322.py:34: FutureWarning: suggest_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float instead.\n",
      "  'subsample': trial.suggest_uniform('subsample', 0.6, 1.0),\n",
      "C:\\Users\\aroma\\AppData\\Local\\Temp\\ipykernel_32672\\58112322.py:35: FutureWarning: suggest_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float instead.\n",
      "  'colsample_bytree': trial.suggest_uniform('colsample_bytree', 0.6, 1.0),\n",
      "C:\\Users\\aroma\\AppData\\Local\\Temp\\ipykernel_32672\\58112322.py:36: FutureWarning: suggest_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float instead.\n",
      "  'scale_pos_weight': trial.suggest_uniform('scale_pos_weight', 4.0, 8.0),  # Use around your computed value\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training until validation scores don't improve for 25 rounds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-06-05 19:13:58,826] Trial 4 finished with value: 0.9996992883156804 and parameters: {'learning_rate': 0.020227367431839005, 'num_leaves': 87, 'max_depth': 10, 'min_child_samples': 82, 'subsample': 0.8558035135929238, 'colsample_bytree': 0.7404733235392386, 'scale_pos_weight': 5.8966306491353535}. Best is trial 1 with value: 0.9999331595481585.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping, best iteration is:\n",
      "[68]\tvalidation's auc: 1\n",
      "Training until validation scores don't improve for 25 rounds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\aroma\\AppData\\Local\\Temp\\ipykernel_32672\\58112322.py:30: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  'learning_rate': trial.suggest_loguniform('learning_rate', 0.01, 0.2),\n",
      "C:\\Users\\aroma\\AppData\\Local\\Temp\\ipykernel_32672\\58112322.py:34: FutureWarning: suggest_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float instead.\n",
      "  'subsample': trial.suggest_uniform('subsample', 0.6, 1.0),\n",
      "C:\\Users\\aroma\\AppData\\Local\\Temp\\ipykernel_32672\\58112322.py:35: FutureWarning: suggest_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float instead.\n",
      "  'colsample_bytree': trial.suggest_uniform('colsample_bytree', 0.6, 1.0),\n",
      "C:\\Users\\aroma\\AppData\\Local\\Temp\\ipykernel_32672\\58112322.py:36: FutureWarning: suggest_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float instead.\n",
      "  'scale_pos_weight': trial.suggest_uniform('scale_pos_weight', 4.0, 8.0),  # Use around your computed value\n",
      "[I 2025-06-05 19:13:59,302] Trial 5 finished with value: 0.9986982209018993 and parameters: {'learning_rate': 0.015784044450559406, 'num_leaves': 66, 'max_depth': 20, 'min_child_samples': 65, 'subsample': 0.8540364155147294, 'colsample_bytree': 0.7271861574722229, 'scale_pos_weight': 6.870844133607379}. Best is trial 1 with value: 0.9999331595481585.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping, best iteration is:\n",
      "[38]\tvalidation's auc: 1\n",
      "Training until validation scores don't improve for 25 rounds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\aroma\\AppData\\Local\\Temp\\ipykernel_32672\\58112322.py:30: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  'learning_rate': trial.suggest_loguniform('learning_rate', 0.01, 0.2),\n",
      "C:\\Users\\aroma\\AppData\\Local\\Temp\\ipykernel_32672\\58112322.py:34: FutureWarning: suggest_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float instead.\n",
      "  'subsample': trial.suggest_uniform('subsample', 0.6, 1.0),\n",
      "C:\\Users\\aroma\\AppData\\Local\\Temp\\ipykernel_32672\\58112322.py:35: FutureWarning: suggest_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float instead.\n",
      "  'colsample_bytree': trial.suggest_uniform('colsample_bytree', 0.6, 1.0),\n",
      "C:\\Users\\aroma\\AppData\\Local\\Temp\\ipykernel_32672\\58112322.py:36: FutureWarning: suggest_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float instead.\n",
      "  'scale_pos_weight': trial.suggest_uniform('scale_pos_weight', 4.0, 8.0),  # Use around your computed value\n",
      "[I 2025-06-05 19:13:59,541] Trial 6 finished with value: 0.9997994719427826 and parameters: {'learning_rate': 0.13189462139129365, 'num_leaves': 126, 'max_depth': 20, 'min_child_samples': 38, 'subsample': 0.9444090562742982, 'colsample_bytree': 0.723209894832436, 'scale_pos_weight': 4.517621677991876}. Best is trial 1 with value: 0.9999331595481585.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping, best iteration is:\n",
      "[8]\tvalidation's auc: 0.999887\n",
      "Training until validation scores don't improve for 25 rounds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\aroma\\AppData\\Local\\Temp\\ipykernel_32672\\58112322.py:30: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  'learning_rate': trial.suggest_loguniform('learning_rate', 0.01, 0.2),\n",
      "C:\\Users\\aroma\\AppData\\Local\\Temp\\ipykernel_32672\\58112322.py:34: FutureWarning: suggest_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float instead.\n",
      "  'subsample': trial.suggest_uniform('subsample', 0.6, 1.0),\n",
      "C:\\Users\\aroma\\AppData\\Local\\Temp\\ipykernel_32672\\58112322.py:35: FutureWarning: suggest_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float instead.\n",
      "  'colsample_bytree': trial.suggest_uniform('colsample_bytree', 0.6, 1.0),\n",
      "C:\\Users\\aroma\\AppData\\Local\\Temp\\ipykernel_32672\\58112322.py:36: FutureWarning: suggest_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float instead.\n",
      "  'scale_pos_weight': trial.suggest_uniform('scale_pos_weight', 4.0, 8.0),  # Use around your computed value\n",
      "[I 2025-06-05 19:13:59,887] Trial 7 finished with value: 0.9999665764230088 and parameters: {'learning_rate': 0.18512850854627738, 'num_leaves': 15, 'max_depth': 17, 'min_child_samples': 93, 'subsample': 0.6704129072515996, 'colsample_bytree': 0.68396826250353, 'scale_pos_weight': 6.469073069531305}. Best is trial 7 with value: 0.9999665764230088.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping, best iteration is:\n",
      "[89]\tvalidation's auc: 0.999951\n",
      "Training until validation scores don't improve for 25 rounds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\aroma\\AppData\\Local\\Temp\\ipykernel_32672\\58112322.py:30: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  'learning_rate': trial.suggest_loguniform('learning_rate', 0.01, 0.2),\n",
      "C:\\Users\\aroma\\AppData\\Local\\Temp\\ipykernel_32672\\58112322.py:34: FutureWarning: suggest_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float instead.\n",
      "  'subsample': trial.suggest_uniform('subsample', 0.6, 1.0),\n",
      "C:\\Users\\aroma\\AppData\\Local\\Temp\\ipykernel_32672\\58112322.py:35: FutureWarning: suggest_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float instead.\n",
      "  'colsample_bytree': trial.suggest_uniform('colsample_bytree', 0.6, 1.0),\n",
      "C:\\Users\\aroma\\AppData\\Local\\Temp\\ipykernel_32672\\58112322.py:36: FutureWarning: suggest_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float instead.\n",
      "  'scale_pos_weight': trial.suggest_uniform('scale_pos_weight', 4.0, 8.0),  # Use around your computed value\n",
      "[I 2025-06-05 19:14:00,152] Trial 8 finished with value: 0.9986815534304645 and parameters: {'learning_rate': 0.013819218915470395, 'num_leaves': 104, 'max_depth': 7, 'min_child_samples': 79, 'subsample': 0.7533288836864305, 'colsample_bytree': 0.6935952592019199, 'scale_pos_weight': 5.00130406488552}. Best is trial 7 with value: 0.9999665764230088.\n",
      "C:\\Users\\aroma\\AppData\\Local\\Temp\\ipykernel_32672\\58112322.py:30: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  'learning_rate': trial.suggest_loguniform('learning_rate', 0.01, 0.2),\n",
      "C:\\Users\\aroma\\AppData\\Local\\Temp\\ipykernel_32672\\58112322.py:34: FutureWarning: suggest_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float instead.\n",
      "  'subsample': trial.suggest_uniform('subsample', 0.6, 1.0),\n",
      "C:\\Users\\aroma\\AppData\\Local\\Temp\\ipykernel_32672\\58112322.py:35: FutureWarning: suggest_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float instead.\n",
      "  'colsample_bytree': trial.suggest_uniform('colsample_bytree', 0.6, 1.0),\n",
      "C:\\Users\\aroma\\AppData\\Local\\Temp\\ipykernel_32672\\58112322.py:36: FutureWarning: suggest_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float instead.\n",
      "  'scale_pos_weight': trial.suggest_uniform('scale_pos_weight', 4.0, 8.0),  # Use around your computed value\n",
      "[I 2025-06-05 19:14:00,302] Trial 9 finished with value: 0.9995989840930357 and parameters: {'learning_rate': 0.10279451451711513, 'num_leaves': 73, 'max_depth': 9, 'min_child_samples': 89, 'subsample': 0.8048307603839993, 'colsample_bytree': 0.703270860433661, 'scale_pos_weight': 7.98134901467273}. Best is trial 7 with value: 0.9999665764230088.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping, best iteration is:\n",
      "[41]\tvalidation's auc: 1\n",
      "Training until validation scores don't improve for 25 rounds\n",
      "Early stopping, best iteration is:\n",
      "[1]\tvalidation's auc: 0.993362\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\aroma\\AppData\\Local\\Temp\\ipykernel_32672\\58112322.py:30: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  'learning_rate': trial.suggest_loguniform('learning_rate', 0.01, 0.2),\n",
      "C:\\Users\\aroma\\AppData\\Local\\Temp\\ipykernel_32672\\58112322.py:34: FutureWarning: suggest_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float instead.\n",
      "  'subsample': trial.suggest_uniform('subsample', 0.6, 1.0),\n",
      "C:\\Users\\aroma\\AppData\\Local\\Temp\\ipykernel_32672\\58112322.py:35: FutureWarning: suggest_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float instead.\n",
      "  'colsample_bytree': trial.suggest_uniform('colsample_bytree', 0.6, 1.0),\n",
      "C:\\Users\\aroma\\AppData\\Local\\Temp\\ipykernel_32672\\58112322.py:36: FutureWarning: suggest_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float instead.\n",
      "  'scale_pos_weight': trial.suggest_uniform('scale_pos_weight', 4.0, 8.0),  # Use around your computed value\n",
      "[I 2025-06-05 19:14:00,512] Trial 10 finished with value: 0.9999164508313142 and parameters: {'learning_rate': 0.194930350411836, 'num_leaves': 16, 'max_depth': 16, 'min_child_samples': 47, 'subsample': 0.6218558885532086, 'colsample_bytree': 0.6063192725282336, 'scale_pos_weight': 6.946399595398768}. Best is trial 7 with value: 0.9999665764230088.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training until validation scores don't improve for 25 rounds\n",
      "Early stopping, best iteration is:\n",
      "[7]\tvalidation's auc: 1\n",
      "Training until validation scores don't improve for 25 rounds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\aroma\\AppData\\Local\\Temp\\ipykernel_32672\\58112322.py:30: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  'learning_rate': trial.suggest_loguniform('learning_rate', 0.01, 0.2),\n",
      "C:\\Users\\aroma\\AppData\\Local\\Temp\\ipykernel_32672\\58112322.py:34: FutureWarning: suggest_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float instead.\n",
      "  'subsample': trial.suggest_uniform('subsample', 0.6, 1.0),\n",
      "C:\\Users\\aroma\\AppData\\Local\\Temp\\ipykernel_32672\\58112322.py:35: FutureWarning: suggest_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float instead.\n",
      "  'colsample_bytree': trial.suggest_uniform('colsample_bytree', 0.6, 1.0),\n",
      "C:\\Users\\aroma\\AppData\\Local\\Temp\\ipykernel_32672\\58112322.py:36: FutureWarning: suggest_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float instead.\n",
      "  'scale_pos_weight': trial.suggest_uniform('scale_pos_weight', 4.0, 8.0),  # Use around your computed value\n",
      "[I 2025-06-05 19:14:00,733] Trial 11 finished with value: 0.9998496215475614 and parameters: {'learning_rate': 0.03531696764642827, 'num_leaves': 52, 'max_depth': 3, 'min_child_samples': 97, 'subsample': 0.6786593646845375, 'colsample_bytree': 0.8090468987933609, 'scale_pos_weight': 6.1204371586768795}. Best is trial 7 with value: 0.9999665764230088.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping, best iteration is:\n",
      "[48]\tvalidation's auc: 1\n",
      "Training until validation scores don't improve for 25 rounds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\aroma\\AppData\\Local\\Temp\\ipykernel_32672\\58112322.py:30: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  'learning_rate': trial.suggest_loguniform('learning_rate', 0.01, 0.2),\n",
      "C:\\Users\\aroma\\AppData\\Local\\Temp\\ipykernel_32672\\58112322.py:34: FutureWarning: suggest_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float instead.\n",
      "  'subsample': trial.suggest_uniform('subsample', 0.6, 1.0),\n",
      "C:\\Users\\aroma\\AppData\\Local\\Temp\\ipykernel_32672\\58112322.py:35: FutureWarning: suggest_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float instead.\n",
      "  'colsample_bytree': trial.suggest_uniform('colsample_bytree', 0.6, 1.0),\n",
      "C:\\Users\\aroma\\AppData\\Local\\Temp\\ipykernel_32672\\58112322.py:36: FutureWarning: suggest_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float instead.\n",
      "  'scale_pos_weight': trial.suggest_uniform('scale_pos_weight', 4.0, 8.0),  # Use around your computed value\n",
      "[I 2025-06-05 19:14:01,049] Trial 12 finished with value: 0.9998830350727689 and parameters: {'learning_rate': 0.05302827772792345, 'num_leaves': 45, 'max_depth': 14, 'min_child_samples': 56, 'subsample': 0.7125470926660693, 'colsample_bytree': 0.9993948500473785, 'scale_pos_weight': 5.843996815691067}. Best is trial 7 with value: 0.9999665764230088.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping, best iteration is:\n",
      "[29]\tvalidation's auc: 1\n",
      "Training until validation scores don't improve for 25 rounds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\aroma\\AppData\\Local\\Temp\\ipykernel_32672\\58112322.py:30: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  'learning_rate': trial.suggest_loguniform('learning_rate', 0.01, 0.2),\n",
      "C:\\Users\\aroma\\AppData\\Local\\Temp\\ipykernel_32672\\58112322.py:34: FutureWarning: suggest_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float instead.\n",
      "  'subsample': trial.suggest_uniform('subsample', 0.6, 1.0),\n",
      "C:\\Users\\aroma\\AppData\\Local\\Temp\\ipykernel_32672\\58112322.py:35: FutureWarning: suggest_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float instead.\n",
      "  'colsample_bytree': trial.suggest_uniform('colsample_bytree', 0.6, 1.0),\n",
      "C:\\Users\\aroma\\AppData\\Local\\Temp\\ipykernel_32672\\58112322.py:36: FutureWarning: suggest_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float instead.\n",
      "  'scale_pos_weight': trial.suggest_uniform('scale_pos_weight', 4.0, 8.0),  # Use around your computed value\n",
      "[I 2025-06-05 19:14:01,296] Trial 13 finished with value: 0.9997326917936381 and parameters: {'learning_rate': 0.06469817836933513, 'num_leaves': 22, 'max_depth': 16, 'min_child_samples': 25, 'subsample': 0.9933868916389614, 'colsample_bytree': 0.7903702347898184, 'scale_pos_weight': 6.5738343181317}. Best is trial 7 with value: 0.9999665764230088.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping, best iteration is:\n",
      "[8]\tvalidation's auc: 1\n",
      "Training until validation scores don't improve for 25 rounds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\aroma\\AppData\\Local\\Temp\\ipykernel_32672\\58112322.py:30: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  'learning_rate': trial.suggest_loguniform('learning_rate', 0.01, 0.2),\n",
      "C:\\Users\\aroma\\AppData\\Local\\Temp\\ipykernel_32672\\58112322.py:34: FutureWarning: suggest_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float instead.\n",
      "  'subsample': trial.suggest_uniform('subsample', 0.6, 1.0),\n",
      "C:\\Users\\aroma\\AppData\\Local\\Temp\\ipykernel_32672\\58112322.py:35: FutureWarning: suggest_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float instead.\n",
      "  'colsample_bytree': trial.suggest_uniform('colsample_bytree', 0.6, 1.0),\n",
      "C:\\Users\\aroma\\AppData\\Local\\Temp\\ipykernel_32672\\58112322.py:36: FutureWarning: suggest_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float instead.\n",
      "  'scale_pos_weight': trial.suggest_uniform('scale_pos_weight', 4.0, 8.0),  # Use around your computed value\n",
      "[I 2025-06-05 19:14:01,683] Trial 14 finished with value: 0.9996992883156804 and parameters: {'learning_rate': 0.02868031726099514, 'num_leaves': 92, 'max_depth': 16, 'min_child_samples': 80, 'subsample': 0.6284515164881406, 'colsample_bytree': 0.6006923532788192, 'scale_pos_weight': 7.539105060147962}. Best is trial 7 with value: 0.9999665764230088.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping, best iteration is:\n",
      "[44]\tvalidation's auc: 1\n",
      "Training until validation scores don't improve for 25 rounds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\aroma\\AppData\\Local\\Temp\\ipykernel_32672\\58112322.py:30: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  'learning_rate': trial.suggest_loguniform('learning_rate', 0.01, 0.2),\n",
      "C:\\Users\\aroma\\AppData\\Local\\Temp\\ipykernel_32672\\58112322.py:34: FutureWarning: suggest_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float instead.\n",
      "  'subsample': trial.suggest_uniform('subsample', 0.6, 1.0),\n",
      "C:\\Users\\aroma\\AppData\\Local\\Temp\\ipykernel_32672\\58112322.py:35: FutureWarning: suggest_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float instead.\n",
      "  'colsample_bytree': trial.suggest_uniform('colsample_bytree', 0.6, 1.0),\n",
      "C:\\Users\\aroma\\AppData\\Local\\Temp\\ipykernel_32672\\58112322.py:36: FutureWarning: suggest_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float instead.\n",
      "  'scale_pos_weight': trial.suggest_uniform('scale_pos_weight', 4.0, 8.0),  # Use around your computed value\n",
      "[I 2025-06-05 19:14:02,130] Trial 15 finished with value: 0.9999498654723508 and parameters: {'learning_rate': 0.08401670545588726, 'num_leaves': 118, 'max_depth': 13, 'min_child_samples': 100, 'subsample': 0.6878434514082071, 'colsample_bytree': 0.6530972972657778, 'scale_pos_weight': 5.546439616483521}. Best is trial 7 with value: 0.9999665764230088.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping, best iteration is:\n",
      "[61]\tvalidation's auc: 0.999934\n",
      "Training until validation scores don't improve for 25 rounds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\aroma\\AppData\\Local\\Temp\\ipykernel_32672\\58112322.py:30: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  'learning_rate': trial.suggest_loguniform('learning_rate', 0.01, 0.2),\n",
      "C:\\Users\\aroma\\AppData\\Local\\Temp\\ipykernel_32672\\58112322.py:34: FutureWarning: suggest_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float instead.\n",
      "  'subsample': trial.suggest_uniform('subsample', 0.6, 1.0),\n",
      "C:\\Users\\aroma\\AppData\\Local\\Temp\\ipykernel_32672\\58112322.py:35: FutureWarning: suggest_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float instead.\n",
      "  'colsample_bytree': trial.suggest_uniform('colsample_bytree', 0.6, 1.0),\n",
      "C:\\Users\\aroma\\AppData\\Local\\Temp\\ipykernel_32672\\58112322.py:36: FutureWarning: suggest_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float instead.\n",
      "  'scale_pos_weight': trial.suggest_uniform('scale_pos_weight', 4.0, 8.0),  # Use around your computed value\n",
      "[I 2025-06-05 19:14:02,515] Trial 16 finished with value: 0.9999498637966475 and parameters: {'learning_rate': 0.08598471164808877, 'num_leaves': 123, 'max_depth': 13, 'min_child_samples': 100, 'subsample': 0.681010992050926, 'colsample_bytree': 0.6530794371262499, 'scale_pos_weight': 5.285789422483324}. Best is trial 7 with value: 0.9999665764230088.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping, best iteration is:\n",
      "[49]\tvalidation's auc: 0.999934\n",
      "Training until validation scores don't improve for 25 rounds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\aroma\\AppData\\Local\\Temp\\ipykernel_32672\\58112322.py:30: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  'learning_rate': trial.suggest_loguniform('learning_rate', 0.01, 0.2),\n",
      "C:\\Users\\aroma\\AppData\\Local\\Temp\\ipykernel_32672\\58112322.py:34: FutureWarning: suggest_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float instead.\n",
      "  'subsample': trial.suggest_uniform('subsample', 0.6, 1.0),\n",
      "C:\\Users\\aroma\\AppData\\Local\\Temp\\ipykernel_32672\\58112322.py:35: FutureWarning: suggest_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float instead.\n",
      "  'colsample_bytree': trial.suggest_uniform('colsample_bytree', 0.6, 1.0),\n",
      "C:\\Users\\aroma\\AppData\\Local\\Temp\\ipykernel_32672\\58112322.py:36: FutureWarning: suggest_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float instead.\n",
      "  'scale_pos_weight': trial.suggest_uniform('scale_pos_weight', 4.0, 8.0),  # Use around your computed value\n",
      "[I 2025-06-05 19:14:02,940] Trial 17 finished with value: 0.9999665764230088 and parameters: {'learning_rate': 0.1957056175784135, 'num_leaves': 121, 'max_depth': 18, 'min_child_samples': 71, 'subsample': 0.6703481987655344, 'colsample_bytree': 0.7810196874715913, 'scale_pos_weight': 6.3768172374029986}. Best is trial 7 with value: 0.9999665764230088.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping, best iteration is:\n",
      "[69]\tvalidation's auc: 0.999974\n",
      "Training until validation scores don't improve for 25 rounds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\aroma\\AppData\\Local\\Temp\\ipykernel_32672\\58112322.py:30: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  'learning_rate': trial.suggest_loguniform('learning_rate', 0.01, 0.2),\n",
      "C:\\Users\\aroma\\AppData\\Local\\Temp\\ipykernel_32672\\58112322.py:34: FutureWarning: suggest_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float instead.\n",
      "  'subsample': trial.suggest_uniform('subsample', 0.6, 1.0),\n",
      "C:\\Users\\aroma\\AppData\\Local\\Temp\\ipykernel_32672\\58112322.py:35: FutureWarning: suggest_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float instead.\n",
      "  'colsample_bytree': trial.suggest_uniform('colsample_bytree', 0.6, 1.0),\n",
      "C:\\Users\\aroma\\AppData\\Local\\Temp\\ipykernel_32672\\58112322.py:36: FutureWarning: suggest_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float instead.\n",
      "  'scale_pos_weight': trial.suggest_uniform('scale_pos_weight', 4.0, 8.0),  # Use around your computed value\n",
      "[I 2025-06-05 19:14:03,253] Trial 18 finished with value: 0.9999498688234213 and parameters: {'learning_rate': 0.1963032129064966, 'num_leaves': 138, 'max_depth': 18, 'min_child_samples': 69, 'subsample': 0.6061656874723715, 'colsample_bytree': 0.7915081203709389, 'scale_pos_weight': 6.310399620775555}. Best is trial 7 with value: 0.9999665764230088.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping, best iteration is:\n",
      "[14]\tvalidation's auc: 1\n",
      "Training until validation scores don't improve for 25 rounds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\aroma\\AppData\\Local\\Temp\\ipykernel_32672\\58112322.py:30: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  'learning_rate': trial.suggest_loguniform('learning_rate', 0.01, 0.2),\n",
      "C:\\Users\\aroma\\AppData\\Local\\Temp\\ipykernel_32672\\58112322.py:34: FutureWarning: suggest_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float instead.\n",
      "  'subsample': trial.suggest_uniform('subsample', 0.6, 1.0),\n",
      "C:\\Users\\aroma\\AppData\\Local\\Temp\\ipykernel_32672\\58112322.py:35: FutureWarning: suggest_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float instead.\n",
      "  'colsample_bytree': trial.suggest_uniform('colsample_bytree', 0.6, 1.0),\n",
      "C:\\Users\\aroma\\AppData\\Local\\Temp\\ipykernel_32672\\58112322.py:36: FutureWarning: suggest_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float instead.\n",
      "  'scale_pos_weight': trial.suggest_uniform('scale_pos_weight', 4.0, 8.0),  # Use around your computed value\n",
      "[I 2025-06-05 19:14:03,539] Trial 19 finished with value: 0.9996992883156804 and parameters: {'learning_rate': 0.13626307139994254, 'num_leaves': 63, 'max_depth': 17, 'min_child_samples': 61, 'subsample': 0.7272972894907207, 'colsample_bytree': 0.8367649838454011, 'scale_pos_weight': 7.133609423579998}. Best is trial 7 with value: 0.9999665764230088.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping, best iteration is:\n",
      "[3]\tvalidation's auc: 0.99995\n",
      "Training until validation scores don't improve for 25 rounds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\aroma\\AppData\\Local\\Temp\\ipykernel_32672\\58112322.py:30: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  'learning_rate': trial.suggest_loguniform('learning_rate', 0.01, 0.2),\n",
      "C:\\Users\\aroma\\AppData\\Local\\Temp\\ipykernel_32672\\58112322.py:34: FutureWarning: suggest_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float instead.\n",
      "  'subsample': trial.suggest_uniform('subsample', 0.6, 1.0),\n",
      "C:\\Users\\aroma\\AppData\\Local\\Temp\\ipykernel_32672\\58112322.py:35: FutureWarning: suggest_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float instead.\n",
      "  'colsample_bytree': trial.suggest_uniform('colsample_bytree', 0.6, 1.0),\n",
      "C:\\Users\\aroma\\AppData\\Local\\Temp\\ipykernel_32672\\58112322.py:36: FutureWarning: suggest_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float instead.\n",
      "  'scale_pos_weight': trial.suggest_uniform('scale_pos_weight', 4.0, 8.0),  # Use around your computed value\n",
      "[I 2025-06-05 19:14:03,850] Trial 20 finished with value: 0.9999164508313142 and parameters: {'learning_rate': 0.13646736624320266, 'num_leaves': 112, 'max_depth': 18, 'min_child_samples': 53, 'subsample': 0.6546215238551097, 'colsample_bytree': 0.7639856888054799, 'scale_pos_weight': 6.377531195783911}. Best is trial 7 with value: 0.9999665764230088.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping, best iteration is:\n",
      "[12]\tvalidation's auc: 1\n",
      "Training until validation scores don't improve for 25 rounds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\aroma\\AppData\\Local\\Temp\\ipykernel_32672\\58112322.py:30: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  'learning_rate': trial.suggest_loguniform('learning_rate', 0.01, 0.2),\n",
      "C:\\Users\\aroma\\AppData\\Local\\Temp\\ipykernel_32672\\58112322.py:34: FutureWarning: suggest_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float instead.\n",
      "  'subsample': trial.suggest_uniform('subsample', 0.6, 1.0),\n",
      "C:\\Users\\aroma\\AppData\\Local\\Temp\\ipykernel_32672\\58112322.py:35: FutureWarning: suggest_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float instead.\n",
      "  'colsample_bytree': trial.suggest_uniform('colsample_bytree', 0.6, 1.0),\n",
      "C:\\Users\\aroma\\AppData\\Local\\Temp\\ipykernel_32672\\58112322.py:36: FutureWarning: suggest_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float instead.\n",
      "  'scale_pos_weight': trial.suggest_uniform('scale_pos_weight', 4.0, 8.0),  # Use around your computed value\n",
      "[I 2025-06-05 19:14:04,291] Trial 21 finished with value: 0.9999665764230088 and parameters: {'learning_rate': 0.19447904818602701, 'num_leaves': 150, 'max_depth': 18, 'min_child_samples': 72, 'subsample': 0.6018453830745306, 'colsample_bytree': 0.8768160699394809, 'scale_pos_weight': 6.321212530484546}. Best is trial 7 with value: 0.9999665764230088.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping, best iteration is:\n",
      "[69]\tvalidation's auc: 0.999975\n",
      "Training until validation scores don't improve for 25 rounds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\aroma\\AppData\\Local\\Temp\\ipykernel_32672\\58112322.py:30: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  'learning_rate': trial.suggest_loguniform('learning_rate', 0.01, 0.2),\n",
      "C:\\Users\\aroma\\AppData\\Local\\Temp\\ipykernel_32672\\58112322.py:34: FutureWarning: suggest_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float instead.\n",
      "  'subsample': trial.suggest_uniform('subsample', 0.6, 1.0),\n",
      "C:\\Users\\aroma\\AppData\\Local\\Temp\\ipykernel_32672\\58112322.py:35: FutureWarning: suggest_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float instead.\n",
      "  'colsample_bytree': trial.suggest_uniform('colsample_bytree', 0.6, 1.0),\n",
      "C:\\Users\\aroma\\AppData\\Local\\Temp\\ipykernel_32672\\58112322.py:36: FutureWarning: suggest_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float instead.\n",
      "  'scale_pos_weight': trial.suggest_uniform('scale_pos_weight', 4.0, 8.0),  # Use around your computed value\n",
      "[I 2025-06-05 19:14:04,544] Trial 22 finished with value: 0.9996156866676692 and parameters: {'learning_rate': 0.17075053127649106, 'num_leaves': 150, 'max_depth': 18, 'min_child_samples': 75, 'subsample': 0.6481728030618982, 'colsample_bytree': 0.8907592871910307, 'scale_pos_weight': 6.666068821937084}. Best is trial 7 with value: 0.9999665764230088.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping, best iteration is:\n",
      "[4]\tvalidation's auc: 0.99975\n",
      "Training until validation scores don't improve for 25 rounds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\aroma\\AppData\\Local\\Temp\\ipykernel_32672\\58112322.py:30: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  'learning_rate': trial.suggest_loguniform('learning_rate', 0.01, 0.2),\n",
      "C:\\Users\\aroma\\AppData\\Local\\Temp\\ipykernel_32672\\58112322.py:34: FutureWarning: suggest_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float instead.\n",
      "  'subsample': trial.suggest_uniform('subsample', 0.6, 1.0),\n",
      "C:\\Users\\aroma\\AppData\\Local\\Temp\\ipykernel_32672\\58112322.py:35: FutureWarning: suggest_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float instead.\n",
      "  'colsample_bytree': trial.suggest_uniform('colsample_bytree', 0.6, 1.0),\n",
      "C:\\Users\\aroma\\AppData\\Local\\Temp\\ipykernel_32672\\58112322.py:36: FutureWarning: suggest_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float instead.\n",
      "  'scale_pos_weight': trial.suggest_uniform('scale_pos_weight', 4.0, 8.0),  # Use around your computed value\n",
      "[I 2025-06-05 19:14:04,817] Trial 23 finished with value: 0.9999331595481585 and parameters: {'learning_rate': 0.10973009867990309, 'num_leaves': 134, 'max_depth': 15, 'min_child_samples': 82, 'subsample': 0.6033285052166177, 'colsample_bytree': 0.931801080268536, 'scale_pos_weight': 7.284828176237712}. Best is trial 7 with value: 0.9999665764230088.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping, best iteration is:\n",
      "[22]\tvalidation's auc: 1\n",
      "Training until validation scores don't improve for 25 rounds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\aroma\\AppData\\Local\\Temp\\ipykernel_32672\\58112322.py:30: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  'learning_rate': trial.suggest_loguniform('learning_rate', 0.01, 0.2),\n",
      "C:\\Users\\aroma\\AppData\\Local\\Temp\\ipykernel_32672\\58112322.py:34: FutureWarning: suggest_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float instead.\n",
      "  'subsample': trial.suggest_uniform('subsample', 0.6, 1.0),\n",
      "C:\\Users\\aroma\\AppData\\Local\\Temp\\ipykernel_32672\\58112322.py:35: FutureWarning: suggest_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float instead.\n",
      "  'colsample_bytree': trial.suggest_uniform('colsample_bytree', 0.6, 1.0),\n",
      "C:\\Users\\aroma\\AppData\\Local\\Temp\\ipykernel_32672\\58112322.py:36: FutureWarning: suggest_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float instead.\n",
      "  'scale_pos_weight': trial.suggest_uniform('scale_pos_weight', 4.0, 8.0),  # Use around your computed value\n",
      "[I 2025-06-05 19:14:05,289] Trial 24 finished with value: 0.9999665764230088 and parameters: {'learning_rate': 0.15926555275156568, 'num_leaves': 142, 'max_depth': 19, 'min_child_samples': 89, 'subsample': 0.7505209010601501, 'colsample_bytree': 0.8488433544920642, 'scale_pos_weight': 6.185883483730522}. Best is trial 7 with value: 0.9999665764230088.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping, best iteration is:\n",
      "[108]\tvalidation's auc: 0.999953\n",
      "Training until validation scores don't improve for 25 rounds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\aroma\\AppData\\Local\\Temp\\ipykernel_32672\\58112322.py:30: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  'learning_rate': trial.suggest_loguniform('learning_rate', 0.01, 0.2),\n",
      "C:\\Users\\aroma\\AppData\\Local\\Temp\\ipykernel_32672\\58112322.py:34: FutureWarning: suggest_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float instead.\n",
      "  'subsample': trial.suggest_uniform('subsample', 0.6, 1.0),\n",
      "C:\\Users\\aroma\\AppData\\Local\\Temp\\ipykernel_32672\\58112322.py:35: FutureWarning: suggest_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float instead.\n",
      "  'colsample_bytree': trial.suggest_uniform('colsample_bytree', 0.6, 1.0),\n",
      "C:\\Users\\aroma\\AppData\\Local\\Temp\\ipykernel_32672\\58112322.py:36: FutureWarning: suggest_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float instead.\n",
      "  'scale_pos_weight': trial.suggest_uniform('scale_pos_weight', 4.0, 8.0),  # Use around your computed value\n",
      "[I 2025-06-05 19:14:05,592] Trial 25 finished with value: 0.9996825874137557 and parameters: {'learning_rate': 0.07106649249071058, 'num_leaves': 130, 'max_depth': 15, 'min_child_samples': 66, 'subsample': 0.6561971426361681, 'colsample_bytree': 0.9712683098876507, 'scale_pos_weight': 6.607001997348869}. Best is trial 7 with value: 0.9999665764230088.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping, best iteration is:\n",
      "[10]\tvalidation's auc: 1\n",
      "Training until validation scores don't improve for 25 rounds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\aroma\\AppData\\Local\\Temp\\ipykernel_32672\\58112322.py:30: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  'learning_rate': trial.suggest_loguniform('learning_rate', 0.01, 0.2),\n",
      "C:\\Users\\aroma\\AppData\\Local\\Temp\\ipykernel_32672\\58112322.py:34: FutureWarning: suggest_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float instead.\n",
      "  'subsample': trial.suggest_uniform('subsample', 0.6, 1.0),\n",
      "C:\\Users\\aroma\\AppData\\Local\\Temp\\ipykernel_32672\\58112322.py:35: FutureWarning: suggest_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float instead.\n",
      "  'colsample_bytree': trial.suggest_uniform('colsample_bytree', 0.6, 1.0),\n",
      "C:\\Users\\aroma\\AppData\\Local\\Temp\\ipykernel_32672\\58112322.py:36: FutureWarning: suggest_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float instead.\n",
      "  'scale_pos_weight': trial.suggest_uniform('scale_pos_weight', 4.0, 8.0),  # Use around your computed value\n",
      "[I 2025-06-05 19:14:05,892] Trial 26 finished with value: 0.9996491286695294 and parameters: {'learning_rate': 0.10503330706273284, 'num_leaves': 112, 'max_depth': 17, 'min_child_samples': 48, 'subsample': 0.7173501524293386, 'colsample_bytree': 0.885094154476296, 'scale_pos_weight': 5.4976859155765}. Best is trial 7 with value: 0.9999665764230088.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping, best iteration is:\n",
      "[1]\tvalidation's auc: 0.993576\n",
      "Training until validation scores don't improve for 25 rounds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\aroma\\AppData\\Local\\Temp\\ipykernel_32672\\58112322.py:30: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  'learning_rate': trial.suggest_loguniform('learning_rate', 0.01, 0.2),\n",
      "C:\\Users\\aroma\\AppData\\Local\\Temp\\ipykernel_32672\\58112322.py:34: FutureWarning: suggest_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float instead.\n",
      "  'subsample': trial.suggest_uniform('subsample', 0.6, 1.0),\n",
      "C:\\Users\\aroma\\AppData\\Local\\Temp\\ipykernel_32672\\58112322.py:35: FutureWarning: suggest_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float instead.\n",
      "  'colsample_bytree': trial.suggest_uniform('colsample_bytree', 0.6, 1.0),\n",
      "C:\\Users\\aroma\\AppData\\Local\\Temp\\ipykernel_32672\\58112322.py:36: FutureWarning: suggest_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float instead.\n",
      "  'scale_pos_weight': trial.suggest_uniform('scale_pos_weight', 4.0, 8.0),  # Use around your computed value\n",
      "[I 2025-06-05 19:14:06,136] Trial 27 finished with value: 0.99968255559454 and parameters: {'learning_rate': 0.1509056349893212, 'num_leaves': 78, 'max_depth': 19, 'min_child_samples': 74, 'subsample': 0.6398400736593701, 'colsample_bytree': 0.7592323806750807, 'scale_pos_weight': 7.43753171053948}. Best is trial 7 with value: 0.9999665764230088.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping, best iteration is:\n",
      "[1]\tvalidation's auc: 0.993456\n",
      "Training until validation scores don't improve for 25 rounds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\aroma\\AppData\\Local\\Temp\\ipykernel_32672\\58112322.py:30: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  'learning_rate': trial.suggest_loguniform('learning_rate', 0.01, 0.2),\n",
      "C:\\Users\\aroma\\AppData\\Local\\Temp\\ipykernel_32672\\58112322.py:34: FutureWarning: suggest_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float instead.\n",
      "  'subsample': trial.suggest_uniform('subsample', 0.6, 1.0),\n",
      "C:\\Users\\aroma\\AppData\\Local\\Temp\\ipykernel_32672\\58112322.py:35: FutureWarning: suggest_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float instead.\n",
      "  'colsample_bytree': trial.suggest_uniform('colsample_bytree', 0.6, 1.0),\n",
      "C:\\Users\\aroma\\AppData\\Local\\Temp\\ipykernel_32672\\58112322.py:36: FutureWarning: suggest_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float instead.\n",
      "  'scale_pos_weight': trial.suggest_uniform('scale_pos_weight', 4.0, 8.0),  # Use around your computed value\n",
      "[I 2025-06-05 19:14:06,467] Trial 28 finished with value: 0.9998997426728603 and parameters: {'learning_rate': 0.197332540632944, 'num_leaves': 99, 'max_depth': 17, 'min_child_samples': 61, 'subsample': 0.7769349718886936, 'colsample_bytree': 0.8214045053779897, 'scale_pos_weight': 7.763969549339474}. Best is trial 7 with value: 0.9999665764230088.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping, best iteration is:\n",
      "[3]\tvalidation's auc: 0.999937\n",
      "Training until validation scores don't improve for 25 rounds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\aroma\\AppData\\Local\\Temp\\ipykernel_32672\\58112322.py:30: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  'learning_rate': trial.suggest_loguniform('learning_rate', 0.01, 0.2),\n",
      "C:\\Users\\aroma\\AppData\\Local\\Temp\\ipykernel_32672\\58112322.py:34: FutureWarning: suggest_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float instead.\n",
      "  'subsample': trial.suggest_uniform('subsample', 0.6, 1.0),\n",
      "C:\\Users\\aroma\\AppData\\Local\\Temp\\ipykernel_32672\\58112322.py:35: FutureWarning: suggest_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float instead.\n",
      "  'colsample_bytree': trial.suggest_uniform('colsample_bytree', 0.6, 1.0),\n",
      "C:\\Users\\aroma\\AppData\\Local\\Temp\\ipykernel_32672\\58112322.py:36: FutureWarning: suggest_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float instead.\n",
      "  'scale_pos_weight': trial.suggest_uniform('scale_pos_weight', 4.0, 8.0),  # Use around your computed value\n",
      "[I 2025-06-05 19:14:07,210] Trial 29 finished with value: 0.9987982374148752 and parameters: {'learning_rate': 0.010511745854879226, 'num_leaves': 31, 'max_depth': 20, 'min_child_samples': 93, 'subsample': 0.8092883014353324, 'colsample_bytree': 0.6824945011005659, 'scale_pos_weight': 7.031970981215996}. Best is trial 7 with value: 0.9999665764230088.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping, best iteration is:\n",
      "[114]\tvalidation's auc: 1\n",
      "Training until validation scores don't improve for 25 rounds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\aroma\\AppData\\Local\\Temp\\ipykernel_32672\\58112322.py:30: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  'learning_rate': trial.suggest_loguniform('learning_rate', 0.01, 0.2),\n",
      "C:\\Users\\aroma\\AppData\\Local\\Temp\\ipykernel_32672\\58112322.py:34: FutureWarning: suggest_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float instead.\n",
      "  'subsample': trial.suggest_uniform('subsample', 0.6, 1.0),\n",
      "C:\\Users\\aroma\\AppData\\Local\\Temp\\ipykernel_32672\\58112322.py:35: FutureWarning: suggest_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float instead.\n",
      "  'colsample_bytree': trial.suggest_uniform('colsample_bytree', 0.6, 1.0),\n",
      "C:\\Users\\aroma\\AppData\\Local\\Temp\\ipykernel_32672\\58112322.py:36: FutureWarning: suggest_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float instead.\n",
      "  'scale_pos_weight': trial.suggest_uniform('scale_pos_weight', 4.0, 8.0),  # Use around your computed value\n",
      "[I 2025-06-05 19:14:07,778] Trial 30 finished with value: 0.9999498654723508 and parameters: {'learning_rate': 0.12769661047135278, 'num_leaves': 139, 'max_depth': 14, 'min_child_samples': 86, 'subsample': 0.686014004961469, 'colsample_bytree': 0.7687573431399413, 'scale_pos_weight': 6.365870953807143}. Best is trial 7 with value: 0.9999665764230088.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping, best iteration is:\n",
      "[128]\tvalidation's auc: 0.999999\n",
      "Training until validation scores don't improve for 25 rounds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\aroma\\AppData\\Local\\Temp\\ipykernel_32672\\58112322.py:30: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  'learning_rate': trial.suggest_loguniform('learning_rate', 0.01, 0.2),\n",
      "C:\\Users\\aroma\\AppData\\Local\\Temp\\ipykernel_32672\\58112322.py:34: FutureWarning: suggest_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float instead.\n",
      "  'subsample': trial.suggest_uniform('subsample', 0.6, 1.0),\n",
      "C:\\Users\\aroma\\AppData\\Local\\Temp\\ipykernel_32672\\58112322.py:35: FutureWarning: suggest_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float instead.\n",
      "  'colsample_bytree': trial.suggest_uniform('colsample_bytree', 0.6, 1.0),\n",
      "C:\\Users\\aroma\\AppData\\Local\\Temp\\ipykernel_32672\\58112322.py:36: FutureWarning: suggest_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float instead.\n",
      "  'scale_pos_weight': trial.suggest_uniform('scale_pos_weight', 4.0, 8.0),  # Use around your computed value\n",
      "[I 2025-06-05 19:14:08,054] Trial 31 finished with value: 0.9996324389347412 and parameters: {'learning_rate': 0.16721045404004417, 'num_leaves': 143, 'max_depth': 19, 'min_child_samples': 86, 'subsample': 0.7519852717606653, 'colsample_bytree': 0.8498021371013151, 'scale_pos_weight': 6.138654152669172}. Best is trial 7 with value: 0.9999665764230088.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping, best iteration is:\n",
      "[2]\tvalidation's auc: 0.993504\n",
      "Training until validation scores don't improve for 25 rounds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\aroma\\AppData\\Local\\Temp\\ipykernel_32672\\58112322.py:30: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  'learning_rate': trial.suggest_loguniform('learning_rate', 0.01, 0.2),\n",
      "C:\\Users\\aroma\\AppData\\Local\\Temp\\ipykernel_32672\\58112322.py:34: FutureWarning: suggest_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float instead.\n",
      "  'subsample': trial.suggest_uniform('subsample', 0.6, 1.0),\n",
      "C:\\Users\\aroma\\AppData\\Local\\Temp\\ipykernel_32672\\58112322.py:35: FutureWarning: suggest_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float instead.\n",
      "  'colsample_bytree': trial.suggest_uniform('colsample_bytree', 0.6, 1.0),\n",
      "C:\\Users\\aroma\\AppData\\Local\\Temp\\ipykernel_32672\\58112322.py:36: FutureWarning: suggest_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float instead.\n",
      "  'scale_pos_weight': trial.suggest_uniform('scale_pos_weight', 4.0, 8.0),  # Use around your computed value\n",
      "[I 2025-06-05 19:14:08,307] Trial 32 finished with value: 0.9998161733986196 and parameters: {'learning_rate': 0.16106549029124173, 'num_leaves': 144, 'max_depth': 19, 'min_child_samples': 94, 'subsample': 0.7467603049142348, 'colsample_bytree': 0.8856975275700878, 'scale_pos_weight': 6.74142976111939}. Best is trial 7 with value: 0.9999665764230088.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping, best iteration is:\n",
      "[6]\tvalidation's auc: 0.99989\n",
      "Training until validation scores don't improve for 25 rounds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\aroma\\AppData\\Local\\Temp\\ipykernel_32672\\58112322.py:30: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  'learning_rate': trial.suggest_loguniform('learning_rate', 0.01, 0.2),\n",
      "C:\\Users\\aroma\\AppData\\Local\\Temp\\ipykernel_32672\\58112322.py:34: FutureWarning: suggest_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float instead.\n",
      "  'subsample': trial.suggest_uniform('subsample', 0.6, 1.0),\n",
      "C:\\Users\\aroma\\AppData\\Local\\Temp\\ipykernel_32672\\58112322.py:35: FutureWarning: suggest_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float instead.\n",
      "  'colsample_bytree': trial.suggest_uniform('colsample_bytree', 0.6, 1.0),\n",
      "C:\\Users\\aroma\\AppData\\Local\\Temp\\ipykernel_32672\\58112322.py:36: FutureWarning: suggest_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float instead.\n",
      "  'scale_pos_weight': trial.suggest_uniform('scale_pos_weight', 4.0, 8.0),  # Use around your computed value\n",
      "[I 2025-06-05 19:14:08,519] Trial 33 finished with value: 0.9996491521176176 and parameters: {'learning_rate': 0.12178501231361882, 'num_leaves': 131, 'max_depth': 18, 'min_child_samples': 89, 'subsample': 0.7050642851596128, 'colsample_bytree': 0.8654243190181943, 'scale_pos_weight': 6.155585115704424}. Best is trial 7 with value: 0.9999665764230088.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping, best iteration is:\n",
      "[2]\tvalidation's auc: 0.99339\n",
      "Training until validation scores don't improve for 25 rounds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\aroma\\AppData\\Local\\Temp\\ipykernel_32672\\58112322.py:30: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  'learning_rate': trial.suggest_loguniform('learning_rate', 0.01, 0.2),\n",
      "C:\\Users\\aroma\\AppData\\Local\\Temp\\ipykernel_32672\\58112322.py:34: FutureWarning: suggest_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float instead.\n",
      "  'subsample': trial.suggest_uniform('subsample', 0.6, 1.0),\n",
      "C:\\Users\\aroma\\AppData\\Local\\Temp\\ipykernel_32672\\58112322.py:35: FutureWarning: suggest_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float instead.\n",
      "  'colsample_bytree': trial.suggest_uniform('colsample_bytree', 0.6, 1.0),\n",
      "C:\\Users\\aroma\\AppData\\Local\\Temp\\ipykernel_32672\\58112322.py:36: FutureWarning: suggest_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float instead.\n",
      "  'scale_pos_weight': trial.suggest_uniform('scale_pos_weight', 4.0, 8.0),  # Use around your computed value\n",
      "[I 2025-06-05 19:14:08,905] Trial 34 finished with value: 0.9999665786571305 and parameters: {'learning_rate': 0.1614458587407259, 'num_leaves': 150, 'max_depth': 19, 'min_child_samples': 74, 'subsample': 0.6628764303318666, 'colsample_bytree': 0.9327214397233908, 'scale_pos_weight': 5.72295277674116}. Best is trial 34 with value: 0.9999665786571305.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping, best iteration is:\n",
      "[38]\tvalidation's auc: 1\n",
      "Training until validation scores don't improve for 25 rounds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\aroma\\AppData\\Local\\Temp\\ipykernel_32672\\58112322.py:30: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  'learning_rate': trial.suggest_loguniform('learning_rate', 0.01, 0.2),\n",
      "C:\\Users\\aroma\\AppData\\Local\\Temp\\ipykernel_32672\\58112322.py:34: FutureWarning: suggest_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float instead.\n",
      "  'subsample': trial.suggest_uniform('subsample', 0.6, 1.0),\n",
      "C:\\Users\\aroma\\AppData\\Local\\Temp\\ipykernel_32672\\58112322.py:35: FutureWarning: suggest_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float instead.\n",
      "  'colsample_bytree': trial.suggest_uniform('colsample_bytree', 0.6, 1.0),\n",
      "C:\\Users\\aroma\\AppData\\Local\\Temp\\ipykernel_32672\\58112322.py:36: FutureWarning: suggest_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float instead.\n",
      "  'scale_pos_weight': trial.suggest_uniform('scale_pos_weight', 4.0, 8.0),  # Use around your computed value\n",
      "[I 2025-06-05 19:14:09,144] Trial 35 finished with value: 0.9996992682193337 and parameters: {'learning_rate': 0.09088644044946317, 'num_leaves': 150, 'max_depth': 15, 'min_child_samples': 76, 'subsample': 0.6655900596422136, 'colsample_bytree': 0.9344274335033163, 'scale_pos_weight': 5.621936817148931}. Best is trial 34 with value: 0.9999665786571305.\n",
      "C:\\Users\\aroma\\AppData\\Local\\Temp\\ipykernel_32672\\58112322.py:30: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  'learning_rate': trial.suggest_loguniform('learning_rate', 0.01, 0.2),\n",
      "C:\\Users\\aroma\\AppData\\Local\\Temp\\ipykernel_32672\\58112322.py:34: FutureWarning: suggest_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float instead.\n",
      "  'subsample': trial.suggest_uniform('subsample', 0.6, 1.0),\n",
      "C:\\Users\\aroma\\AppData\\Local\\Temp\\ipykernel_32672\\58112322.py:35: FutureWarning: suggest_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float instead.\n",
      "  'colsample_bytree': trial.suggest_uniform('colsample_bytree', 0.6, 1.0),\n",
      "C:\\Users\\aroma\\AppData\\Local\\Temp\\ipykernel_32672\\58112322.py:36: FutureWarning: suggest_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float instead.\n",
      "  'scale_pos_weight': trial.suggest_uniform('scale_pos_weight', 4.0, 8.0),  # Use around your computed value\n",
      "[I 2025-06-05 19:14:09,327] Trial 36 finished with value: 0.9997159897756319 and parameters: {'learning_rate': 0.19708643808312482, 'num_leaves': 117, 'max_depth': 11, 'min_child_samples': 69, 'subsample': 0.6254822403105138, 'colsample_bytree': 0.9110611786213684, 'scale_pos_weight': 5.311271514064883}. Best is trial 34 with value: 0.9999665786571305.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping, best iteration is:\n",
      "[3]\tvalidation's auc: 0.999865\n",
      "Training until validation scores don't improve for 25 rounds\n",
      "Early stopping, best iteration is:\n",
      "[2]\tvalidation's auc: 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\aroma\\AppData\\Local\\Temp\\ipykernel_32672\\58112322.py:30: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  'learning_rate': trial.suggest_loguniform('learning_rate', 0.01, 0.2),\n",
      "C:\\Users\\aroma\\AppData\\Local\\Temp\\ipykernel_32672\\58112322.py:34: FutureWarning: suggest_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float instead.\n",
      "  'subsample': trial.suggest_uniform('subsample', 0.6, 1.0),\n",
      "C:\\Users\\aroma\\AppData\\Local\\Temp\\ipykernel_32672\\58112322.py:35: FutureWarning: suggest_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float instead.\n",
      "  'colsample_bytree': trial.suggest_uniform('colsample_bytree', 0.6, 1.0),\n",
      "C:\\Users\\aroma\\AppData\\Local\\Temp\\ipykernel_32672\\58112322.py:36: FutureWarning: suggest_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float instead.\n",
      "  'scale_pos_weight': trial.suggest_uniform('scale_pos_weight', 4.0, 8.0),  # Use around your computed value\n",
      "[I 2025-06-05 19:14:09,589] Trial 37 finished with value: 0.9997159897756319 and parameters: {'learning_rate': 0.02351311966529708, 'num_leaves': 94, 'max_depth': 20, 'min_child_samples': 66, 'subsample': 0.6072364686078291, 'colsample_bytree': 0.9548067335095933, 'scale_pos_weight': 5.932540106093068}. Best is trial 34 with value: 0.9999665786571305.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training until validation scores don't improve for 25 rounds\n",
      "Early stopping, best iteration is:\n",
      "[14]\tvalidation's auc: 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\aroma\\AppData\\Local\\Temp\\ipykernel_32672\\58112322.py:30: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  'learning_rate': trial.suggest_loguniform('learning_rate', 0.01, 0.2),\n",
      "C:\\Users\\aroma\\AppData\\Local\\Temp\\ipykernel_32672\\58112322.py:34: FutureWarning: suggest_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float instead.\n",
      "  'subsample': trial.suggest_uniform('subsample', 0.6, 1.0),\n",
      "C:\\Users\\aroma\\AppData\\Local\\Temp\\ipykernel_32672\\58112322.py:35: FutureWarning: suggest_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float instead.\n",
      "  'colsample_bytree': trial.suggest_uniform('colsample_bytree', 0.6, 1.0),\n",
      "C:\\Users\\aroma\\AppData\\Local\\Temp\\ipykernel_32672\\58112322.py:36: FutureWarning: suggest_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float instead.\n",
      "  'scale_pos_weight': trial.suggest_uniform('scale_pos_weight', 4.0, 8.0),  # Use around your computed value\n",
      "[I 2025-06-05 19:14:09,857] Trial 38 finished with value: 0.9998663190963171 and parameters: {'learning_rate': 0.14600207367005733, 'num_leaves': 125, 'max_depth': 17, 'min_child_samples': 72, 'subsample': 0.6356057960239543, 'colsample_bytree': 0.9688768188677415, 'scale_pos_weight': 6.526968850189363}. Best is trial 34 with value: 0.9999665786571305.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training until validation scores don't improve for 25 rounds\n",
      "Early stopping, best iteration is:\n",
      "[11]\tvalidation's auc: 0.999951\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\aroma\\AppData\\Local\\Temp\\ipykernel_32672\\58112322.py:30: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  'learning_rate': trial.suggest_loguniform('learning_rate', 0.01, 0.2),\n",
      "C:\\Users\\aroma\\AppData\\Local\\Temp\\ipykernel_32672\\58112322.py:34: FutureWarning: suggest_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float instead.\n",
      "  'subsample': trial.suggest_uniform('subsample', 0.6, 1.0),\n",
      "C:\\Users\\aroma\\AppData\\Local\\Temp\\ipykernel_32672\\58112322.py:35: FutureWarning: suggest_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float instead.\n",
      "  'colsample_bytree': trial.suggest_uniform('colsample_bytree', 0.6, 1.0),\n",
      "C:\\Users\\aroma\\AppData\\Local\\Temp\\ipykernel_32672\\58112322.py:36: FutureWarning: suggest_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float instead.\n",
      "  'scale_pos_weight': trial.suggest_uniform('scale_pos_weight', 4.0, 8.0),  # Use around your computed value\n",
      "[I 2025-06-05 19:14:10,128] Trial 39 finished with value: 0.9994986798569566 and parameters: {'learning_rate': 0.12061736687467982, 'num_leaves': 49, 'max_depth': 19, 'min_child_samples': 84, 'subsample': 0.7826367590538567, 'colsample_bytree': 0.6302860479454693, 'scale_pos_weight': 5.7293922298089885}. Best is trial 34 with value: 0.9999665786571305.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training until validation scores don't improve for 25 rounds\n",
      "Early stopping, best iteration is:\n",
      "[4]\tvalidation's auc: 0.999578\n",
      "Best hyperparameters: {'learning_rate': 0.1614458587407259, 'num_leaves': 150, 'max_depth': 19, 'min_child_samples': 74, 'subsample': 0.6628764303318666, 'colsample_bytree': 0.9327214397233908, 'scale_pos_weight': 5.72295277674116}\n"
     ]
    }
   ],
   "source": [
    "import optuna\n",
    "import pyarrow.parquet as pq\n",
    "import lightgbm as lgb\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import f1_score\n",
    "\n",
    "# Path to one medium/large cleaned file\n",
    "tune_file = r\"D:\\Canada\\Subjects\\Semester -2\\AIDI-2005-02 CAPSTONE TERM ll\\Bot_detector\\data\\cleaned\\Friday-16-02-2018_TrafficForML_CICFlowMeter.parquet\"\n",
    "BATCH_SIZE = 100_000\n",
    "\n",
    "# Load a batch\n",
    "parquet = pq.ParquetFile(tune_file)\n",
    "for batch in parquet.iter_batches(batch_size=BATCH_SIZE):\n",
    "    df = batch.to_pandas()\n",
    "    break  # Take first batch\n",
    "\n",
    "feature_cols = [c for c in df.columns if c != \"Label\"]\n",
    "X = df[feature_cols].astype(np.float32)\n",
    "y = df[\"Label\"].astype(np.int32)\n",
    "\n",
    "# Split for tuning\n",
    "X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.3, stratify=y, random_state=42)\n",
    "\n",
    "def objective(trial):\n",
    "    params = {\n",
    "        'objective': 'binary',\n",
    "        'boosting_type': 'gbdt',\n",
    "        'metric': 'auc',\n",
    "        'learning_rate': trial.suggest_loguniform('learning_rate', 0.01, 0.2),\n",
    "        'num_leaves': trial.suggest_int('num_leaves', 15, 150),\n",
    "        'max_depth': trial.suggest_int('max_depth', 3, 20),\n",
    "        'min_child_samples': trial.suggest_int('min_child_samples', 20, 100),\n",
    "        'subsample': trial.suggest_uniform('subsample', 0.6, 1.0),\n",
    "        'colsample_bytree': trial.suggest_uniform('colsample_bytree', 0.6, 1.0),\n",
    "        'scale_pos_weight': trial.suggest_uniform('scale_pos_weight', 4.0, 8.0),  # Use around your computed value\n",
    "        'verbose': -1,\n",
    "    }\n",
    "    lgb_train = lgb.Dataset(X_train, y_train)\n",
    "    lgb_val = lgb.Dataset(X_val, y_val, reference=lgb_train)\n",
    "    booster = lgb.train(\n",
    "        params, lgb_train,\n",
    "        valid_sets=[lgb_val],\n",
    "        valid_names=['validation'],\n",
    "        num_boost_round=300,\n",
    "        callbacks=[lgb.early_stopping(25), lgb.log_evaluation(0)]\n",
    "    )\n",
    "    # Use lower threshold to favor recall\n",
    "    y_pred = (booster.predict(X_val) > 0.2).astype(int)\n",
    "    return f1_score(y_val, y_pred, average='binary')\n",
    "\n",
    "study = optuna.create_study(direction='maximize')\n",
    "study.optimize(objective, n_trials=40)  # 40-50 for reasonable coverage\n",
    "\n",
    "best_params = study.best_params\n",
    "print(\"Best hyperparameters:\", best_params)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "dcd34829",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train files: ['D:\\\\Canada\\\\Subjects\\\\Semester -2\\\\AIDI-2005-02 CAPSTONE TERM ll\\\\Bot_detector\\\\data\\\\cleaned\\\\Friday-02-03-2018_TrafficForML_CICFlowMeter.parquet', 'D:\\\\Canada\\\\Subjects\\\\Semester -2\\\\AIDI-2005-02 CAPSTONE TERM ll\\\\Bot_detector\\\\data\\\\cleaned\\\\Friday-16-02-2018_TrafficForML_CICFlowMeter.parquet', 'D:\\\\Canada\\\\Subjects\\\\Semester -2\\\\AIDI-2005-02 CAPSTONE TERM ll\\\\Bot_detector\\\\data\\\\cleaned\\\\Friday-23-02-2018_TrafficForML_CICFlowMeter.parquet', 'D:\\\\Canada\\\\Subjects\\\\Semester -2\\\\AIDI-2005-02 CAPSTONE TERM ll\\\\Bot_detector\\\\data\\\\cleaned\\\\Thuesday-20-02-2018_TrafficForML_CICFlowMeter.parquet', 'D:\\\\Canada\\\\Subjects\\\\Semester -2\\\\AIDI-2005-02 CAPSTONE TERM ll\\\\Bot_detector\\\\data\\\\cleaned\\\\Thursday-01-03-2018_TrafficForML_CICFlowMeter.parquet', 'D:\\\\Canada\\\\Subjects\\\\Semester -2\\\\AIDI-2005-02 CAPSTONE TERM ll\\\\Bot_detector\\\\data\\\\cleaned\\\\Thursday-15-02-2018_TrafficForML_CICFlowMeter.parquet', 'D:\\\\Canada\\\\Subjects\\\\Semester -2\\\\AIDI-2005-02 CAPSTONE TERM ll\\\\Bot_detector\\\\data\\\\cleaned\\\\Thursday-22-02-2018_TrafficForML_CICFlowMeter.parquet', 'D:\\\\Canada\\\\Subjects\\\\Semester -2\\\\AIDI-2005-02 CAPSTONE TERM ll\\\\Bot_detector\\\\data\\\\cleaned\\\\Wednesday-14-02-2018_TrafficForML_CICFlowMeter.parquet', 'D:\\\\Canada\\\\Subjects\\\\Semester -2\\\\AIDI-2005-02 CAPSTONE TERM ll\\\\Bot_detector\\\\data\\\\cleaned\\\\Wednesday-21-02-2018_TrafficForML_CICFlowMeter.parquet']\n",
      "Test file: D:\\Canada\\Subjects\\Semester -2\\AIDI-2005-02 CAPSTONE TERM ll\\Bot_detector\\data\\cleaned\\Wednesday-28-02-2018_TrafficForML_CICFlowMeter.parquet\n",
      "scale_pos_weight: 4.83\n",
      "\n",
      "Processing: D:\\Canada\\Subjects\\Semester -2\\AIDI-2005-02 CAPSTONE TERM ll\\Bot_detector\\data\\cleaned\\Friday-02-03-2018_TrafficForML_CICFlowMeter.parquet\n",
      "  Training new model …\n",
      "[25]\ttraining's binary_logloss: 0.17198\n",
      "[50]\ttraining's binary_logloss: 0.0754174\n",
      "[75]\ttraining's binary_logloss: 0.034709\n",
      "[100]\ttraining's binary_logloss: 0.0162485\n",
      "[125]\ttraining's binary_logloss: 0.00768447\n",
      "[150]\ttraining's binary_logloss: 0.00365684\n",
      "[175]\ttraining's binary_logloss: 0.00175034\n",
      "[200]\ttraining's binary_logloss: 0.000837697\n",
      "  Continuing training … (batch 2)\n",
      "[225]\ttraining's binary_logloss: 0.000410345\n",
      "[250]\ttraining's binary_logloss: 0.000194189\n",
      "[275]\ttraining's binary_logloss: 9.23687e-05\n",
      "[300]\ttraining's binary_logloss: 4.42256e-05\n",
      "[325]\ttraining's binary_logloss: 2.13126e-05\n",
      "[350]\ttraining's binary_logloss: 1.04149e-05\n",
      "[375]\ttraining's binary_logloss: 5.03705e-06\n",
      "[400]\ttraining's binary_logloss: 2.47865e-06\n",
      "  Continuing training … (batch 3)\n",
      "[425]\ttraining's binary_logloss: 2.56693e-05\n",
      "[450]\ttraining's binary_logloss: 1.21164e-05\n",
      "[475]\ttraining's binary_logloss: 6.02507e-06\n",
      "[500]\ttraining's binary_logloss: 3.2584e-06\n",
      "[525]\ttraining's binary_logloss: 1.90547e-06\n",
      "[550]\ttraining's binary_logloss: 1.11504e-06\n",
      "[575]\ttraining's binary_logloss: 6.32866e-07\n",
      "[600]\ttraining's binary_logloss: 3.69544e-07\n",
      "  Continuing training … (batch 4)\n",
      "[625]\ttraining's binary_logloss: 2.68683\n",
      "[650]\ttraining's binary_logloss: 1.81341\n",
      "[675]\ttraining's binary_logloss: 0.518071\n",
      "[700]\ttraining's binary_logloss: 0.252425\n",
      "[725]\ttraining's binary_logloss: 0.180636\n",
      "[750]\ttraining's binary_logloss: 0.193696\n",
      "[775]\ttraining's binary_logloss: 0.285733\n",
      "[800]\ttraining's binary_logloss: 0.231174\n",
      "  Continuing training … (batch 5)\n",
      "[825]\ttraining's binary_logloss: 0.621466\n",
      "[850]\ttraining's binary_logloss: 0.621466\n",
      "[875]\ttraining's binary_logloss: 0.621466\n",
      "[900]\ttraining's binary_logloss: 0.621466\n",
      "[925]\ttraining's binary_logloss: 0.621466\n",
      "[950]\ttraining's binary_logloss: 0.621466\n",
      "[975]\ttraining's binary_logloss: 0.621466\n",
      "[1000]\ttraining's binary_logloss: 0.621466\n",
      "  Continuing training … (batch 6)\n",
      "[825]\ttraining's binary_logloss: 0.530637\n",
      "[850]\ttraining's binary_logloss: 0.530637\n",
      "[875]\ttraining's binary_logloss: 0.530637\n",
      "[900]\ttraining's binary_logloss: 0.530637\n",
      "[925]\ttraining's binary_logloss: 0.530637\n",
      "[950]\ttraining's binary_logloss: 0.530637\n",
      "[975]\ttraining's binary_logloss: 0.530637\n",
      "[1000]\ttraining's binary_logloss: 0.530637\n",
      "  Continuing training … (batch 7)\n",
      "[825]\ttraining's binary_logloss: 0.647202\n",
      "[850]\ttraining's binary_logloss: 0.647202\n",
      "[875]\ttraining's binary_logloss: 0.647202\n",
      "[900]\ttraining's binary_logloss: 0.647202\n",
      "[925]\ttraining's binary_logloss: 0.647202\n",
      "[950]\ttraining's binary_logloss: 0.647202\n",
      "[975]\ttraining's binary_logloss: 0.647202\n",
      "[1000]\ttraining's binary_logloss: 0.647202\n",
      "  Continuing training … (batch 8)\n",
      "[825]\ttraining's binary_logloss: 0.763926\n",
      "[850]\ttraining's binary_logloss: 0.763926\n",
      "[875]\ttraining's binary_logloss: 0.763926\n",
      "[900]\ttraining's binary_logloss: 0.763926\n",
      "[925]\ttraining's binary_logloss: 0.763926\n",
      "[950]\ttraining's binary_logloss: 0.763926\n",
      "[975]\ttraining's binary_logloss: 0.763926\n",
      "[1000]\ttraining's binary_logloss: 0.763926\n",
      "  Continuing training … (batch 9)\n",
      "[825]\ttraining's binary_logloss: 0.721785\n",
      "[850]\ttraining's binary_logloss: 0.721785\n",
      "[875]\ttraining's binary_logloss: 0.721785\n",
      "[900]\ttraining's binary_logloss: 0.721785\n",
      "[925]\ttraining's binary_logloss: 0.721785\n",
      "[950]\ttraining's binary_logloss: 0.721785\n",
      "[975]\ttraining's binary_logloss: 0.721785\n",
      "[1000]\ttraining's binary_logloss: 0.721785\n",
      "  Continuing training … (batch 10)\n",
      "[825]\ttraining's binary_logloss: 0.529869\n",
      "[850]\ttraining's binary_logloss: 0.529869\n",
      "[875]\ttraining's binary_logloss: 0.529869\n",
      "[900]\ttraining's binary_logloss: 0.529869\n",
      "[925]\ttraining's binary_logloss: 0.529869\n",
      "[950]\ttraining's binary_logloss: 0.529869\n",
      "[975]\ttraining's binary_logloss: 0.529869\n",
      "[1000]\ttraining's binary_logloss: 0.529869\n",
      "  Continuing training … (batch 11)\n",
      "[825]\ttraining's binary_logloss: 0.491468\n",
      "[850]\ttraining's binary_logloss: 0.491468\n",
      "[875]\ttraining's binary_logloss: 0.491468\n",
      "[900]\ttraining's binary_logloss: 0.491468\n",
      "[925]\ttraining's binary_logloss: 0.491468\n",
      "[950]\ttraining's binary_logloss: 0.491468\n",
      "[975]\ttraining's binary_logloss: 0.491468\n",
      "[1000]\ttraining's binary_logloss: 0.491468\n",
      "\n",
      "Processing: D:\\Canada\\Subjects\\Semester -2\\AIDI-2005-02 CAPSTONE TERM ll\\Bot_detector\\data\\cleaned\\Friday-16-02-2018_TrafficForML_CICFlowMeter.parquet\n",
      "  Continuing training … (batch 1)\n",
      "[825]\ttraining's binary_logloss: 33.9195\n",
      "[850]\ttraining's binary_logloss: 33.9195\n",
      "[875]\ttraining's binary_logloss: 33.9195\n",
      "[900]\ttraining's binary_logloss: 33.9195\n",
      "[925]\ttraining's binary_logloss: 33.9195\n",
      "[950]\ttraining's binary_logloss: 33.9195\n",
      "[975]\ttraining's binary_logloss: 33.9195\n",
      "[1000]\ttraining's binary_logloss: 33.9195\n",
      "  Continuing training … (batch 2)\n",
      "[825]\ttraining's binary_logloss: 18.451\n",
      "[850]\ttraining's binary_logloss: 18.451\n",
      "[875]\ttraining's binary_logloss: 18.451\n",
      "[900]\ttraining's binary_logloss: 18.451\n",
      "[925]\ttraining's binary_logloss: 18.451\n",
      "[950]\ttraining's binary_logloss: 18.451\n",
      "[975]\ttraining's binary_logloss: 18.451\n",
      "[1000]\ttraining's binary_logloss: 18.451\n",
      "  Continuing training … (batch 3)\n",
      "[825]\ttraining's binary_logloss: 15.7141\n",
      "[850]\ttraining's binary_logloss: 15.7141\n",
      "[875]\ttraining's binary_logloss: 15.7141\n",
      "[900]\ttraining's binary_logloss: 15.7141\n",
      "[925]\ttraining's binary_logloss: 15.7141\n",
      "[950]\ttraining's binary_logloss: 15.7141\n",
      "[975]\ttraining's binary_logloss: 15.7141\n",
      "[1000]\ttraining's binary_logloss: 15.7141\n",
      "  Continuing training … (batch 4)\n",
      "[825]\ttraining's binary_logloss: 15.6776\n",
      "[850]\ttraining's binary_logloss: 15.6776\n",
      "[875]\ttraining's binary_logloss: 15.6776\n",
      "[900]\ttraining's binary_logloss: 15.6776\n",
      "[925]\ttraining's binary_logloss: 15.6776\n",
      "[950]\ttraining's binary_logloss: 15.6776\n",
      "[975]\ttraining's binary_logloss: 15.6776\n",
      "[1000]\ttraining's binary_logloss: 15.6776\n",
      "  Continuing training … (batch 5)\n",
      "[825]\ttraining's binary_logloss: 15.9372\n",
      "[850]\ttraining's binary_logloss: 15.9372\n",
      "[875]\ttraining's binary_logloss: 15.9372\n",
      "[900]\ttraining's binary_logloss: 15.9372\n",
      "[925]\ttraining's binary_logloss: 15.9372\n",
      "[950]\ttraining's binary_logloss: 15.9372\n",
      "[975]\ttraining's binary_logloss: 15.9372\n",
      "[1000]\ttraining's binary_logloss: 15.9372\n",
      "  Continuing training … (batch 6)\n",
      "[825]\ttraining's binary_logloss: 15.6167\n",
      "[850]\ttraining's binary_logloss: 15.6167\n",
      "[875]\ttraining's binary_logloss: 15.6167\n",
      "[900]\ttraining's binary_logloss: 15.6167\n",
      "[925]\ttraining's binary_logloss: 15.6167\n",
      "[950]\ttraining's binary_logloss: 15.6167\n",
      "[975]\ttraining's binary_logloss: 15.6167\n",
      "[1000]\ttraining's binary_logloss: 15.6167\n",
      "  Continuing training … (batch 7)\n",
      "[825]\ttraining's binary_logloss: 6.81452\n",
      "[850]\ttraining's binary_logloss: 6.81452\n",
      "[875]\ttraining's binary_logloss: 6.81452\n",
      "[900]\ttraining's binary_logloss: 6.81452\n",
      "[925]\ttraining's binary_logloss: 6.81452\n",
      "[950]\ttraining's binary_logloss: 6.81452\n",
      "[975]\ttraining's binary_logloss: 6.81452\n",
      "[1000]\ttraining's binary_logloss: 6.81452\n",
      "  Continuing training … (batch 8)\n",
      "[825]\ttraining's binary_logloss: 6.33579\n",
      "[850]\ttraining's binary_logloss: 6.33579\n",
      "[875]\ttraining's binary_logloss: 6.33579\n",
      "[900]\ttraining's binary_logloss: 6.33579\n",
      "[925]\ttraining's binary_logloss: 6.33579\n",
      "[950]\ttraining's binary_logloss: 6.33579\n",
      "[975]\ttraining's binary_logloss: 6.33579\n",
      "[1000]\ttraining's binary_logloss: 6.33579\n",
      "  Continuing training … (batch 9)\n",
      "[825]\ttraining's binary_logloss: 7.03002\n",
      "[850]\ttraining's binary_logloss: 7.03002\n",
      "[875]\ttraining's binary_logloss: 7.03002\n",
      "[900]\ttraining's binary_logloss: 7.03002\n",
      "[925]\ttraining's binary_logloss: 7.03002\n",
      "[950]\ttraining's binary_logloss: 7.03002\n",
      "[975]\ttraining's binary_logloss: 7.03002\n",
      "[1000]\ttraining's binary_logloss: 7.03002\n",
      "  Continuing training … (batch 10)\n",
      "[825]\ttraining's binary_logloss: 7.22413\n",
      "[850]\ttraining's binary_logloss: 7.22413\n",
      "[875]\ttraining's binary_logloss: 7.22413\n",
      "[900]\ttraining's binary_logloss: 7.22413\n",
      "[925]\ttraining's binary_logloss: 7.22413\n",
      "[950]\ttraining's binary_logloss: 7.22413\n",
      "[975]\ttraining's binary_logloss: 7.22413\n",
      "[1000]\ttraining's binary_logloss: 7.22413\n",
      "  Continuing training … (batch 11)\n",
      "[825]\ttraining's binary_logloss: 34.4982\n",
      "[850]\ttraining's binary_logloss: 34.4982\n",
      "[875]\ttraining's binary_logloss: 34.4982\n",
      "[900]\ttraining's binary_logloss: 34.4982\n",
      "[925]\ttraining's binary_logloss: 34.4982\n",
      "[950]\ttraining's binary_logloss: 34.4982\n",
      "[975]\ttraining's binary_logloss: 34.4982\n",
      "[1000]\ttraining's binary_logloss: 34.4982\n",
      "\n",
      "Processing: D:\\Canada\\Subjects\\Semester -2\\AIDI-2005-02 CAPSTONE TERM ll\\Bot_detector\\data\\cleaned\\Friday-23-02-2018_TrafficForML_CICFlowMeter.parquet\n",
      "  Continuing training … (batch 1)\n",
      "[825]\ttraining's binary_logloss: 0.685249\n",
      "[850]\ttraining's binary_logloss: 0.685249\n",
      "[875]\ttraining's binary_logloss: 0.685249\n",
      "[900]\ttraining's binary_logloss: 0.685249\n",
      "[925]\ttraining's binary_logloss: 0.685249\n",
      "[950]\ttraining's binary_logloss: 0.685249\n",
      "[975]\ttraining's binary_logloss: 0.685249\n",
      "[1000]\ttraining's binary_logloss: 0.685249\n",
      "  Continuing training … (batch 2)\n",
      "[825]\ttraining's binary_logloss: 0.194799\n",
      "[850]\ttraining's binary_logloss: 0.194799\n",
      "[875]\ttraining's binary_logloss: 0.194799\n",
      "[900]\ttraining's binary_logloss: 0.194799\n",
      "[925]\ttraining's binary_logloss: 0.194799\n",
      "[950]\ttraining's binary_logloss: 0.194799\n",
      "[975]\ttraining's binary_logloss: 0.194799\n",
      "[1000]\ttraining's binary_logloss: 0.194799\n",
      "  Continuing training … (batch 3)\n",
      "[825]\ttraining's binary_logloss: 0.180292\n",
      "[850]\ttraining's binary_logloss: 0.180292\n",
      "[875]\ttraining's binary_logloss: 0.180292\n",
      "[900]\ttraining's binary_logloss: 0.180292\n",
      "[925]\ttraining's binary_logloss: 0.180292\n",
      "[950]\ttraining's binary_logloss: 0.180292\n",
      "[975]\ttraining's binary_logloss: 0.180292\n",
      "[1000]\ttraining's binary_logloss: 0.180292\n",
      "  Continuing training … (batch 4)\n",
      "[825]\ttraining's binary_logloss: 0.349644\n",
      "[850]\ttraining's binary_logloss: 0.349644\n",
      "[875]\ttraining's binary_logloss: 0.349644\n",
      "[900]\ttraining's binary_logloss: 0.349644\n",
      "[925]\ttraining's binary_logloss: 0.349644\n",
      "[950]\ttraining's binary_logloss: 0.349644\n",
      "[975]\ttraining's binary_logloss: 0.349644\n",
      "[1000]\ttraining's binary_logloss: 0.349644\n",
      "  Continuing training … (batch 5)\n",
      "[825]\ttraining's binary_logloss: 0.423791\n",
      "[850]\ttraining's binary_logloss: 0.423791\n",
      "[875]\ttraining's binary_logloss: 0.423791\n",
      "[900]\ttraining's binary_logloss: 0.423791\n",
      "[925]\ttraining's binary_logloss: 0.423791\n",
      "[950]\ttraining's binary_logloss: 0.423791\n",
      "[975]\ttraining's binary_logloss: 0.423791\n",
      "[1000]\ttraining's binary_logloss: 0.423791\n",
      "  Continuing training … (batch 6)\n",
      "[825]\ttraining's binary_logloss: 0.503921\n",
      "[850]\ttraining's binary_logloss: 0.503921\n",
      "[875]\ttraining's binary_logloss: 0.503921\n",
      "[900]\ttraining's binary_logloss: 0.503921\n",
      "[925]\ttraining's binary_logloss: 0.503921\n",
      "[950]\ttraining's binary_logloss: 0.503921\n",
      "[975]\ttraining's binary_logloss: 0.503921\n",
      "[1000]\ttraining's binary_logloss: 0.503921\n",
      "  Continuing training … (batch 7)\n",
      "[825]\ttraining's binary_logloss: 0.359549\n",
      "[850]\ttraining's binary_logloss: 0.359549\n",
      "[875]\ttraining's binary_logloss: 0.359549\n",
      "[900]\ttraining's binary_logloss: 0.359549\n",
      "[925]\ttraining's binary_logloss: 0.359549\n",
      "[950]\ttraining's binary_logloss: 0.359549\n",
      "[975]\ttraining's binary_logloss: 0.359549\n",
      "[1000]\ttraining's binary_logloss: 0.359549\n",
      "  Continuing training … (batch 8)\n",
      "[825]\ttraining's binary_logloss: 0.316721\n",
      "[850]\ttraining's binary_logloss: 0.316721\n",
      "[875]\ttraining's binary_logloss: 0.316721\n",
      "[900]\ttraining's binary_logloss: 0.316721\n",
      "[925]\ttraining's binary_logloss: 0.316721\n",
      "[950]\ttraining's binary_logloss: 0.316721\n",
      "[975]\ttraining's binary_logloss: 0.316721\n",
      "[1000]\ttraining's binary_logloss: 0.316721\n",
      "  Continuing training … (batch 9)\n",
      "[825]\ttraining's binary_logloss: 0.371983\n",
      "[850]\ttraining's binary_logloss: 0.371983\n",
      "[875]\ttraining's binary_logloss: 0.371983\n",
      "[900]\ttraining's binary_logloss: 0.371983\n",
      "[925]\ttraining's binary_logloss: 0.371983\n",
      "[950]\ttraining's binary_logloss: 0.371983\n",
      "[975]\ttraining's binary_logloss: 0.371983\n",
      "[1000]\ttraining's binary_logloss: 0.371983\n",
      "  Continuing training … (batch 10)\n",
      "[825]\ttraining's binary_logloss: 0.282527\n",
      "[850]\ttraining's binary_logloss: 0.282527\n",
      "[875]\ttraining's binary_logloss: 0.282527\n",
      "[900]\ttraining's binary_logloss: 0.282527\n",
      "[925]\ttraining's binary_logloss: 0.282527\n",
      "[950]\ttraining's binary_logloss: 0.282527\n",
      "[975]\ttraining's binary_logloss: 0.282527\n",
      "[1000]\ttraining's binary_logloss: 0.282527\n",
      "  Continuing training … (batch 11)\n",
      "[825]\ttraining's binary_logloss: 0.270195\n",
      "[850]\ttraining's binary_logloss: 0.270195\n",
      "[875]\ttraining's binary_logloss: 0.270195\n",
      "[900]\ttraining's binary_logloss: 0.270195\n",
      "[925]\ttraining's binary_logloss: 0.270195\n",
      "[950]\ttraining's binary_logloss: 0.270195\n",
      "[975]\ttraining's binary_logloss: 0.270195\n",
      "[1000]\ttraining's binary_logloss: 0.270195\n",
      "\n",
      "Processing: D:\\Canada\\Subjects\\Semester -2\\AIDI-2005-02 CAPSTONE TERM ll\\Bot_detector\\data\\cleaned\\Thuesday-20-02-2018_TrafficForML_CICFlowMeter.parquet\n",
      "  Continuing training … (batch 1)\n",
      "[825]\ttraining's binary_logloss: 34.5105\n",
      "[850]\ttraining's binary_logloss: 34.5105\n",
      "[875]\ttraining's binary_logloss: 34.5105\n",
      "[900]\ttraining's binary_logloss: 34.5105\n",
      "[925]\ttraining's binary_logloss: 34.5105\n",
      "[950]\ttraining's binary_logloss: 34.5105\n",
      "[975]\ttraining's binary_logloss: 34.5105\n",
      "[1000]\ttraining's binary_logloss: 34.5105\n",
      "  Continuing training … (batch 2)\n",
      "[825]\ttraining's binary_logloss: 34.0725\n",
      "[850]\ttraining's binary_logloss: 34.0725\n",
      "[875]\ttraining's binary_logloss: 34.0725\n",
      "[900]\ttraining's binary_logloss: 34.0725\n",
      "[925]\ttraining's binary_logloss: 34.0725\n",
      "[950]\ttraining's binary_logloss: 34.0725\n",
      "[975]\ttraining's binary_logloss: 34.0725\n",
      "[1000]\ttraining's binary_logloss: 34.0725\n",
      "  Continuing training … (batch 3)\n",
      "[825]\ttraining's binary_logloss: 34.4832\n",
      "[850]\ttraining's binary_logloss: 34.4832\n",
      "[875]\ttraining's binary_logloss: 34.4832\n",
      "[900]\ttraining's binary_logloss: 34.4832\n",
      "[925]\ttraining's binary_logloss: 34.4832\n",
      "[950]\ttraining's binary_logloss: 34.4832\n",
      "[975]\ttraining's binary_logloss: 34.4832\n",
      "[1000]\ttraining's binary_logloss: 34.4832\n",
      "  Continuing training … (batch 4)\n",
      "[825]\ttraining's binary_logloss: 34.4624\n",
      "[850]\ttraining's binary_logloss: 34.4624\n",
      "[875]\ttraining's binary_logloss: 34.4624\n",
      "[900]\ttraining's binary_logloss: 34.4624\n",
      "[925]\ttraining's binary_logloss: 34.4624\n",
      "[950]\ttraining's binary_logloss: 34.4624\n",
      "[975]\ttraining's binary_logloss: 34.4624\n",
      "[1000]\ttraining's binary_logloss: 34.4624\n",
      "  Continuing training … (batch 5)\n",
      "[825]\ttraining's binary_logloss: 34.0407\n",
      "[850]\ttraining's binary_logloss: 34.0407\n",
      "[875]\ttraining's binary_logloss: 34.0407\n",
      "[900]\ttraining's binary_logloss: 34.0407\n",
      "[925]\ttraining's binary_logloss: 34.0407\n",
      "[950]\ttraining's binary_logloss: 34.0407\n",
      "[975]\ttraining's binary_logloss: 34.0407\n",
      "[1000]\ttraining's binary_logloss: 34.0407\n",
      "  Continuing training … (batch 6)\n",
      "[825]\ttraining's binary_logloss: 26.6757\n",
      "[850]\ttraining's binary_logloss: 26.6757\n",
      "[875]\ttraining's binary_logloss: 26.6757\n",
      "[900]\ttraining's binary_logloss: 26.6757\n",
      "[925]\ttraining's binary_logloss: 26.6757\n",
      "[950]\ttraining's binary_logloss: 26.6757\n",
      "[975]\ttraining's binary_logloss: 26.6757\n",
      "[1000]\ttraining's binary_logloss: 26.6757\n",
      "  Continuing training … (batch 7)\n",
      "[825]\ttraining's binary_logloss: 0.217594\n",
      "[850]\ttraining's binary_logloss: 0.217594\n",
      "[875]\ttraining's binary_logloss: 0.217594\n",
      "[900]\ttraining's binary_logloss: 0.217594\n",
      "[925]\ttraining's binary_logloss: 0.217594\n",
      "[950]\ttraining's binary_logloss: 0.217594\n",
      "[975]\ttraining's binary_logloss: 0.217594\n",
      "[1000]\ttraining's binary_logloss: 0.217594\n",
      "  Continuing training … (batch 8)\n",
      "[825]\ttraining's binary_logloss: 0.186164\n",
      "[850]\ttraining's binary_logloss: 0.186164\n",
      "[875]\ttraining's binary_logloss: 0.186164\n",
      "[900]\ttraining's binary_logloss: 0.186164\n",
      "[925]\ttraining's binary_logloss: 0.186164\n",
      "[950]\ttraining's binary_logloss: 0.186164\n",
      "[975]\ttraining's binary_logloss: 0.186164\n",
      "[1000]\ttraining's binary_logloss: 0.186164\n",
      "  Continuing training … (batch 9)\n",
      "[825]\ttraining's binary_logloss: 0.257314\n",
      "[850]\ttraining's binary_logloss: 0.257314\n",
      "[875]\ttraining's binary_logloss: 0.257314\n",
      "[900]\ttraining's binary_logloss: 0.257314\n",
      "[925]\ttraining's binary_logloss: 0.257314\n",
      "[950]\ttraining's binary_logloss: 0.257314\n",
      "[975]\ttraining's binary_logloss: 0.257314\n",
      "[1000]\ttraining's binary_logloss: 0.257314\n",
      "  Continuing training … (batch 10)\n",
      "[825]\ttraining's binary_logloss: 0.3295\n",
      "[850]\ttraining's binary_logloss: 0.3295\n",
      "[875]\ttraining's binary_logloss: 0.3295\n",
      "[900]\ttraining's binary_logloss: 0.3295\n",
      "[925]\ttraining's binary_logloss: 0.3295\n",
      "[950]\ttraining's binary_logloss: 0.3295\n",
      "[975]\ttraining's binary_logloss: 0.3295\n",
      "[1000]\ttraining's binary_logloss: 0.3295\n",
      "  Continuing training … (batch 11)\n",
      "[825]\ttraining's binary_logloss: 0.215522\n",
      "[850]\ttraining's binary_logloss: 0.215522\n",
      "[875]\ttraining's binary_logloss: 0.215522\n",
      "[900]\ttraining's binary_logloss: 0.215522\n",
      "[925]\ttraining's binary_logloss: 0.215522\n",
      "[950]\ttraining's binary_logloss: 0.215522\n",
      "[975]\ttraining's binary_logloss: 0.215522\n",
      "[1000]\ttraining's binary_logloss: 0.215522\n",
      "  Continuing training … (batch 12)\n",
      "[825]\ttraining's binary_logloss: 0.294616\n",
      "[850]\ttraining's binary_logloss: 0.294616\n",
      "[875]\ttraining's binary_logloss: 0.294616\n",
      "[900]\ttraining's binary_logloss: 0.294616\n",
      "[925]\ttraining's binary_logloss: 0.294616\n",
      "[950]\ttraining's binary_logloss: 0.294616\n",
      "[975]\ttraining's binary_logloss: 0.294616\n",
      "[1000]\ttraining's binary_logloss: 0.294616\n",
      "  Continuing training … (batch 13)\n",
      "[825]\ttraining's binary_logloss: 0.227956\n",
      "[850]\ttraining's binary_logloss: 0.227956\n",
      "[875]\ttraining's binary_logloss: 0.227956\n",
      "[900]\ttraining's binary_logloss: 0.227956\n",
      "[925]\ttraining's binary_logloss: 0.227956\n",
      "[950]\ttraining's binary_logloss: 0.227956\n",
      "[975]\ttraining's binary_logloss: 0.227956\n",
      "[1000]\ttraining's binary_logloss: 0.227956\n",
      "  Continuing training … (batch 14)\n",
      "[825]\ttraining's binary_logloss: 0.267551\n",
      "[850]\ttraining's binary_logloss: 0.267551\n",
      "[875]\ttraining's binary_logloss: 0.267551\n",
      "[900]\ttraining's binary_logloss: 0.267551\n",
      "[925]\ttraining's binary_logloss: 0.267551\n",
      "[950]\ttraining's binary_logloss: 0.267551\n",
      "[975]\ttraining's binary_logloss: 0.267551\n",
      "[1000]\ttraining's binary_logloss: 0.267551\n",
      "  Continuing training … (batch 15)\n",
      "[825]\ttraining's binary_logloss: 0.313957\n",
      "[850]\ttraining's binary_logloss: 0.313957\n",
      "[875]\ttraining's binary_logloss: 0.313957\n",
      "[900]\ttraining's binary_logloss: 0.313957\n",
      "[925]\ttraining's binary_logloss: 0.313957\n",
      "[950]\ttraining's binary_logloss: 0.313957\n",
      "[975]\ttraining's binary_logloss: 0.313957\n",
      "[1000]\ttraining's binary_logloss: 0.313957\n",
      "  Continuing training … (batch 16)\n",
      "[825]\ttraining's binary_logloss: 0.262495\n",
      "[850]\ttraining's binary_logloss: 0.262495\n",
      "[875]\ttraining's binary_logloss: 0.262495\n",
      "[900]\ttraining's binary_logloss: 0.262495\n",
      "[925]\ttraining's binary_logloss: 0.262495\n",
      "[950]\ttraining's binary_logloss: 0.262495\n",
      "[975]\ttraining's binary_logloss: 0.262495\n",
      "[1000]\ttraining's binary_logloss: 0.262495\n",
      "  Continuing training … (batch 17)\n",
      "[825]\ttraining's binary_logloss: 0.273547\n",
      "[850]\ttraining's binary_logloss: 0.273547\n",
      "[875]\ttraining's binary_logloss: 0.273547\n",
      "[900]\ttraining's binary_logloss: 0.273547\n",
      "[925]\ttraining's binary_logloss: 0.273547\n",
      "[950]\ttraining's binary_logloss: 0.273547\n",
      "[975]\ttraining's binary_logloss: 0.273547\n",
      "[1000]\ttraining's binary_logloss: 0.273547\n",
      "  Continuing training … (batch 18)\n",
      "[825]\ttraining's binary_logloss: 0.205851\n",
      "[850]\ttraining's binary_logloss: 0.205851\n",
      "[875]\ttraining's binary_logloss: 0.205851\n",
      "[900]\ttraining's binary_logloss: 0.205851\n",
      "[925]\ttraining's binary_logloss: 0.205851\n",
      "[950]\ttraining's binary_logloss: 0.205851\n",
      "[975]\ttraining's binary_logloss: 0.205851\n",
      "[1000]\ttraining's binary_logloss: 0.205851\n",
      "  Continuing training … (batch 19)\n",
      "[825]\ttraining's binary_logloss: 0.231248\n",
      "[850]\ttraining's binary_logloss: 0.231248\n",
      "[875]\ttraining's binary_logloss: 0.231248\n",
      "[900]\ttraining's binary_logloss: 0.231248\n",
      "[925]\ttraining's binary_logloss: 0.231248\n",
      "[950]\ttraining's binary_logloss: 0.231248\n",
      "[975]\ttraining's binary_logloss: 0.231248\n",
      "[1000]\ttraining's binary_logloss: 0.231248\n",
      "  Continuing training … (batch 20)\n",
      "[825]\ttraining's binary_logloss: 0.158533\n",
      "[850]\ttraining's binary_logloss: 0.158533\n",
      "[875]\ttraining's binary_logloss: 0.158533\n",
      "[900]\ttraining's binary_logloss: 0.158533\n",
      "[925]\ttraining's binary_logloss: 0.158533\n",
      "[950]\ttraining's binary_logloss: 0.158533\n",
      "[975]\ttraining's binary_logloss: 0.158533\n",
      "[1000]\ttraining's binary_logloss: 0.158533\n",
      "  Continuing training … (batch 21)\n",
      "[825]\ttraining's binary_logloss: 0.190999\n",
      "[850]\ttraining's binary_logloss: 0.190999\n",
      "[875]\ttraining's binary_logloss: 0.190999\n",
      "[900]\ttraining's binary_logloss: 0.190999\n",
      "[925]\ttraining's binary_logloss: 0.190999\n",
      "[950]\ttraining's binary_logloss: 0.190999\n",
      "[975]\ttraining's binary_logloss: 0.190999\n",
      "[1000]\ttraining's binary_logloss: 0.190999\n",
      "  Continuing training … (batch 22)\n",
      "[825]\ttraining's binary_logloss: 0.176493\n",
      "[850]\ttraining's binary_logloss: 0.176493\n",
      "[875]\ttraining's binary_logloss: 0.176493\n",
      "[900]\ttraining's binary_logloss: 0.176493\n",
      "[925]\ttraining's binary_logloss: 0.176493\n",
      "[950]\ttraining's binary_logloss: 0.176493\n",
      "[975]\ttraining's binary_logloss: 0.176493\n",
      "[1000]\ttraining's binary_logloss: 0.176493\n",
      "  Continuing training … (batch 23)\n",
      "[825]\ttraining's binary_logloss: 0.209996\n",
      "[850]\ttraining's binary_logloss: 0.209996\n",
      "[875]\ttraining's binary_logloss: 0.209996\n",
      "[900]\ttraining's binary_logloss: 0.209996\n",
      "[925]\ttraining's binary_logloss: 0.209996\n",
      "[950]\ttraining's binary_logloss: 0.209996\n",
      "[975]\ttraining's binary_logloss: 0.209996\n",
      "[1000]\ttraining's binary_logloss: 0.209996\n",
      "  Continuing training … (batch 24)\n",
      "[825]\ttraining's binary_logloss: 0.199289\n",
      "[850]\ttraining's binary_logloss: 0.199289\n",
      "[875]\ttraining's binary_logloss: 0.199289\n",
      "[900]\ttraining's binary_logloss: 0.199289\n",
      "[925]\ttraining's binary_logloss: 0.199289\n",
      "[950]\ttraining's binary_logloss: 0.199289\n",
      "[975]\ttraining's binary_logloss: 0.199289\n",
      "[1000]\ttraining's binary_logloss: 0.199289\n",
      "  Continuing training … (batch 25)\n",
      "[825]\ttraining's binary_logloss: 0.183056\n",
      "[850]\ttraining's binary_logloss: 0.183056\n",
      "[875]\ttraining's binary_logloss: 0.183056\n",
      "[900]\ttraining's binary_logloss: 0.183056\n",
      "[925]\ttraining's binary_logloss: 0.183056\n",
      "[950]\ttraining's binary_logloss: 0.183056\n",
      "[975]\ttraining's binary_logloss: 0.183056\n",
      "[1000]\ttraining's binary_logloss: 0.183056\n",
      "  Continuing training … (batch 26)\n",
      "[825]\ttraining's binary_logloss: 0.203779\n",
      "[850]\ttraining's binary_logloss: 0.203779\n",
      "[875]\ttraining's binary_logloss: 0.203779\n",
      "[900]\ttraining's binary_logloss: 0.203779\n",
      "[925]\ttraining's binary_logloss: 0.203779\n",
      "[950]\ttraining's binary_logloss: 0.203779\n",
      "[975]\ttraining's binary_logloss: 0.203779\n",
      "[1000]\ttraining's binary_logloss: 0.203779\n",
      "  Continuing training … (batch 27)\n",
      "[825]\ttraining's binary_logloss: 0.176839\n",
      "[850]\ttraining's binary_logloss: 0.176839\n",
      "[875]\ttraining's binary_logloss: 0.176839\n",
      "[900]\ttraining's binary_logloss: 0.176839\n",
      "[925]\ttraining's binary_logloss: 0.176839\n",
      "[950]\ttraining's binary_logloss: 0.176839\n",
      "[975]\ttraining's binary_logloss: 0.176839\n",
      "[1000]\ttraining's binary_logloss: 0.176839\n",
      "  Continuing training … (batch 28)\n",
      "[825]\ttraining's binary_logloss: 0.211723\n",
      "[850]\ttraining's binary_logloss: 0.211723\n",
      "[875]\ttraining's binary_logloss: 0.211723\n",
      "[900]\ttraining's binary_logloss: 0.211723\n",
      "[925]\ttraining's binary_logloss: 0.211723\n",
      "[950]\ttraining's binary_logloss: 0.211723\n",
      "[975]\ttraining's binary_logloss: 0.211723\n",
      "[1000]\ttraining's binary_logloss: 0.211723\n",
      "  Continuing training … (batch 29)\n",
      "[825]\ttraining's binary_logloss: 0.182019\n",
      "[850]\ttraining's binary_logloss: 0.182019\n",
      "[875]\ttraining's binary_logloss: 0.182019\n",
      "[900]\ttraining's binary_logloss: 0.182019\n",
      "[925]\ttraining's binary_logloss: 0.182019\n",
      "[950]\ttraining's binary_logloss: 0.182019\n",
      "[975]\ttraining's binary_logloss: 0.182019\n",
      "[1000]\ttraining's binary_logloss: 0.182019\n",
      "  Continuing training … (batch 30)\n",
      "[825]\ttraining's binary_logloss: 0.173385\n",
      "[850]\ttraining's binary_logloss: 0.173385\n",
      "[875]\ttraining's binary_logloss: 0.173385\n",
      "[900]\ttraining's binary_logloss: 0.173385\n",
      "[925]\ttraining's binary_logloss: 0.173385\n",
      "[950]\ttraining's binary_logloss: 0.173385\n",
      "[975]\ttraining's binary_logloss: 0.173385\n",
      "[1000]\ttraining's binary_logloss: 0.173385\n",
      "  Continuing training … (batch 31)\n",
      "[825]\ttraining's binary_logloss: 0.20965\n",
      "[850]\ttraining's binary_logloss: 0.20965\n",
      "[875]\ttraining's binary_logloss: 0.20965\n",
      "[900]\ttraining's binary_logloss: 0.20965\n",
      "[925]\ttraining's binary_logloss: 0.20965\n",
      "[950]\ttraining's binary_logloss: 0.20965\n",
      "[975]\ttraining's binary_logloss: 0.20965\n",
      "[1000]\ttraining's binary_logloss: 0.20965\n",
      "  Continuing training … (batch 32)\n",
      "[825]\ttraining's binary_logloss: 0.201706\n",
      "[850]\ttraining's binary_logloss: 0.201706\n",
      "[875]\ttraining's binary_logloss: 0.201706\n",
      "[900]\ttraining's binary_logloss: 0.201706\n",
      "[925]\ttraining's binary_logloss: 0.201706\n",
      "[950]\ttraining's binary_logloss: 0.201706\n",
      "[975]\ttraining's binary_logloss: 0.201706\n",
      "[1000]\ttraining's binary_logloss: 0.201706\n",
      "  Continuing training … (batch 33)\n",
      "[825]\ttraining's binary_logloss: 0.220357\n",
      "[850]\ttraining's binary_logloss: 0.220357\n",
      "[875]\ttraining's binary_logloss: 0.220357\n",
      "[900]\ttraining's binary_logloss: 0.220357\n",
      "[925]\ttraining's binary_logloss: 0.220357\n",
      "[950]\ttraining's binary_logloss: 0.220357\n",
      "[975]\ttraining's binary_logloss: 0.220357\n",
      "[1000]\ttraining's binary_logloss: 0.220357\n",
      "  Continuing training … (batch 34)\n",
      "[825]\ttraining's binary_logloss: 0.244189\n",
      "[850]\ttraining's binary_logloss: 0.244189\n",
      "[875]\ttraining's binary_logloss: 0.244189\n",
      "[900]\ttraining's binary_logloss: 0.244189\n",
      "[925]\ttraining's binary_logloss: 0.244189\n",
      "[950]\ttraining's binary_logloss: 0.244189\n",
      "[975]\ttraining's binary_logloss: 0.244189\n",
      "[1000]\ttraining's binary_logloss: 0.244189\n",
      "  Continuing training … (batch 35)\n",
      "[825]\ttraining's binary_logloss: 0.203779\n",
      "[850]\ttraining's binary_logloss: 0.203779\n",
      "[875]\ttraining's binary_logloss: 0.203779\n",
      "[900]\ttraining's binary_logloss: 0.203779\n",
      "[925]\ttraining's binary_logloss: 0.203779\n",
      "[950]\ttraining's binary_logloss: 0.203779\n",
      "[975]\ttraining's binary_logloss: 0.203779\n",
      "[1000]\ttraining's binary_logloss: 0.203779\n",
      "  Continuing training … (batch 36)\n",
      "[825]\ttraining's binary_logloss: 0.190309\n",
      "[850]\ttraining's binary_logloss: 0.190309\n",
      "[875]\ttraining's binary_logloss: 0.190309\n",
      "[900]\ttraining's binary_logloss: 0.190309\n",
      "[925]\ttraining's binary_logloss: 0.190309\n",
      "[950]\ttraining's binary_logloss: 0.190309\n",
      "[975]\ttraining's binary_logloss: 0.190309\n",
      "[1000]\ttraining's binary_logloss: 0.190309\n",
      "  Continuing training … (batch 37)\n",
      "[825]\ttraining's binary_logloss: 0.193417\n",
      "[850]\ttraining's binary_logloss: 0.193417\n",
      "[875]\ttraining's binary_logloss: 0.193417\n",
      "[900]\ttraining's binary_logloss: 0.193417\n",
      "[925]\ttraining's binary_logloss: 0.193417\n",
      "[950]\ttraining's binary_logloss: 0.193417\n",
      "[975]\ttraining's binary_logloss: 0.193417\n",
      "[1000]\ttraining's binary_logloss: 0.193417\n",
      "  Continuing training … (batch 38)\n",
      "[825]\ttraining's binary_logloss: 0.203779\n",
      "[850]\ttraining's binary_logloss: 0.203779\n",
      "[875]\ttraining's binary_logloss: 0.203779\n",
      "[900]\ttraining's binary_logloss: 0.203779\n",
      "[925]\ttraining's binary_logloss: 0.203779\n",
      "[950]\ttraining's binary_logloss: 0.203779\n",
      "[975]\ttraining's binary_logloss: 0.203779\n",
      "[1000]\ttraining's binary_logloss: 0.203779\n",
      "  Continuing training … (batch 39)\n",
      "[825]\ttraining's binary_logloss: 0.207233\n",
      "[850]\ttraining's binary_logloss: 0.207233\n",
      "[875]\ttraining's binary_logloss: 0.207233\n",
      "[900]\ttraining's binary_logloss: 0.207233\n",
      "[925]\ttraining's binary_logloss: 0.207233\n",
      "[950]\ttraining's binary_logloss: 0.207233\n",
      "[975]\ttraining's binary_logloss: 0.207233\n",
      "[1000]\ttraining's binary_logloss: 0.207233\n",
      "  Continuing training … (batch 40)\n",
      "[825]\ttraining's binary_logloss: 0.177875\n",
      "[850]\ttraining's binary_logloss: 0.177875\n",
      "[875]\ttraining's binary_logloss: 0.177875\n",
      "[900]\ttraining's binary_logloss: 0.177875\n",
      "[925]\ttraining's binary_logloss: 0.177875\n",
      "[950]\ttraining's binary_logloss: 0.177875\n",
      "[975]\ttraining's binary_logloss: 0.177875\n",
      "[1000]\ttraining's binary_logloss: 0.177875\n",
      "  Continuing training … (batch 41)\n",
      "[825]\ttraining's binary_logloss: 0.194453\n",
      "[850]\ttraining's binary_logloss: 0.194453\n",
      "[875]\ttraining's binary_logloss: 0.194453\n",
      "[900]\ttraining's binary_logloss: 0.194453\n",
      "[925]\ttraining's binary_logloss: 0.194453\n",
      "[950]\ttraining's binary_logloss: 0.194453\n",
      "[975]\ttraining's binary_logloss: 0.194453\n",
      "[1000]\ttraining's binary_logloss: 0.194453\n",
      "  Continuing training … (batch 42)\n",
      "[825]\ttraining's binary_logloss: 0.19169\n",
      "[850]\ttraining's binary_logloss: 0.19169\n",
      "[875]\ttraining's binary_logloss: 0.19169\n",
      "[900]\ttraining's binary_logloss: 0.19169\n",
      "[925]\ttraining's binary_logloss: 0.19169\n",
      "[950]\ttraining's binary_logloss: 0.19169\n",
      "[975]\ttraining's binary_logloss: 0.19169\n",
      "[1000]\ttraining's binary_logloss: 0.19169\n",
      "  Continuing training … (batch 43)\n",
      "[825]\ttraining's binary_logloss: 0.206887\n",
      "[850]\ttraining's binary_logloss: 0.206887\n",
      "[875]\ttraining's binary_logloss: 0.206887\n",
      "[900]\ttraining's binary_logloss: 0.206887\n",
      "[925]\ttraining's binary_logloss: 0.206887\n",
      "[950]\ttraining's binary_logloss: 0.206887\n",
      "[975]\ttraining's binary_logloss: 0.206887\n",
      "[1000]\ttraining's binary_logloss: 0.206887\n",
      "  Continuing training … (batch 44)\n",
      "[825]\ttraining's binary_logloss: 0.203088\n",
      "[850]\ttraining's binary_logloss: 0.203088\n",
      "[875]\ttraining's binary_logloss: 0.203088\n",
      "[900]\ttraining's binary_logloss: 0.203088\n",
      "[925]\ttraining's binary_logloss: 0.203088\n",
      "[950]\ttraining's binary_logloss: 0.203088\n",
      "[975]\ttraining's binary_logloss: 0.203088\n",
      "[1000]\ttraining's binary_logloss: 0.203088\n",
      "  Continuing training … (batch 45)\n",
      "[825]\ttraining's binary_logloss: 0.170622\n",
      "[850]\ttraining's binary_logloss: 0.170622\n",
      "[875]\ttraining's binary_logloss: 0.170622\n",
      "[900]\ttraining's binary_logloss: 0.170622\n",
      "[925]\ttraining's binary_logloss: 0.170622\n",
      "[950]\ttraining's binary_logloss: 0.170622\n",
      "[975]\ttraining's binary_logloss: 0.170622\n",
      "[1000]\ttraining's binary_logloss: 0.170622\n",
      "  Continuing training … (batch 46)\n",
      "[825]\ttraining's binary_logloss: 0.236591\n",
      "[850]\ttraining's binary_logloss: 0.236591\n",
      "[875]\ttraining's binary_logloss: 0.236591\n",
      "[900]\ttraining's binary_logloss: 0.236591\n",
      "[925]\ttraining's binary_logloss: 0.236591\n",
      "[950]\ttraining's binary_logloss: 0.236591\n",
      "[975]\ttraining's binary_logloss: 0.236591\n",
      "[1000]\ttraining's binary_logloss: 0.236591\n",
      "  Continuing training … (batch 47)\n",
      "[825]\ttraining's binary_logloss: 0.20965\n",
      "[850]\ttraining's binary_logloss: 0.20965\n",
      "[875]\ttraining's binary_logloss: 0.20965\n",
      "[900]\ttraining's binary_logloss: 0.20965\n",
      "[925]\ttraining's binary_logloss: 0.20965\n",
      "[950]\ttraining's binary_logloss: 0.20965\n",
      "[975]\ttraining's binary_logloss: 0.20965\n",
      "[1000]\ttraining's binary_logloss: 0.20965\n",
      "  Continuing training … (batch 48)\n",
      "[825]\ttraining's binary_logloss: 0.181329\n",
      "[850]\ttraining's binary_logloss: 0.181329\n",
      "[875]\ttraining's binary_logloss: 0.181329\n",
      "[900]\ttraining's binary_logloss: 0.181329\n",
      "[925]\ttraining's binary_logloss: 0.181329\n",
      "[950]\ttraining's binary_logloss: 0.181329\n",
      "[975]\ttraining's binary_logloss: 0.181329\n",
      "[1000]\ttraining's binary_logloss: 0.181329\n",
      "  Continuing training … (batch 49)\n",
      "[825]\ttraining's binary_logloss: 0.203779\n",
      "[850]\ttraining's binary_logloss: 0.203779\n",
      "[875]\ttraining's binary_logloss: 0.203779\n",
      "[900]\ttraining's binary_logloss: 0.203779\n",
      "[925]\ttraining's binary_logloss: 0.203779\n",
      "[950]\ttraining's binary_logloss: 0.203779\n",
      "[975]\ttraining's binary_logloss: 0.203779\n",
      "[1000]\ttraining's binary_logloss: 0.203779\n",
      "  Continuing training … (batch 50)\n",
      "[825]\ttraining's binary_logloss: 0.173385\n",
      "[850]\ttraining's binary_logloss: 0.173385\n",
      "[875]\ttraining's binary_logloss: 0.173385\n",
      "[900]\ttraining's binary_logloss: 0.173385\n",
      "[925]\ttraining's binary_logloss: 0.173385\n",
      "[950]\ttraining's binary_logloss: 0.173385\n",
      "[975]\ttraining's binary_logloss: 0.173385\n",
      "[1000]\ttraining's binary_logloss: 0.173385\n",
      "  Continuing training … (batch 51)\n",
      "[825]\ttraining's binary_logloss: 0.180292\n",
      "[850]\ttraining's binary_logloss: 0.180292\n",
      "[875]\ttraining's binary_logloss: 0.180292\n",
      "[900]\ttraining's binary_logloss: 0.180292\n",
      "[925]\ttraining's binary_logloss: 0.180292\n",
      "[950]\ttraining's binary_logloss: 0.180292\n",
      "[975]\ttraining's binary_logloss: 0.180292\n",
      "[1000]\ttraining's binary_logloss: 0.180292\n",
      "  Continuing training … (batch 52)\n",
      "[825]\ttraining's binary_logloss: 0.218285\n",
      "[850]\ttraining's binary_logloss: 0.218285\n",
      "[875]\ttraining's binary_logloss: 0.218285\n",
      "[900]\ttraining's binary_logloss: 0.218285\n",
      "[925]\ttraining's binary_logloss: 0.218285\n",
      "[950]\ttraining's binary_logloss: 0.218285\n",
      "[975]\ttraining's binary_logloss: 0.218285\n",
      "[1000]\ttraining's binary_logloss: 0.218285\n",
      "  Continuing training … (batch 53)\n",
      "[825]\ttraining's binary_logloss: 0.183746\n",
      "[850]\ttraining's binary_logloss: 0.183746\n",
      "[875]\ttraining's binary_logloss: 0.183746\n",
      "[900]\ttraining's binary_logloss: 0.183746\n",
      "[925]\ttraining's binary_logloss: 0.183746\n",
      "[950]\ttraining's binary_logloss: 0.183746\n",
      "[975]\ttraining's binary_logloss: 0.183746\n",
      "[1000]\ttraining's binary_logloss: 0.183746\n",
      "  Continuing training … (batch 54)\n",
      "[825]\ttraining's binary_logloss: 0.22312\n",
      "[850]\ttraining's binary_logloss: 0.22312\n",
      "[875]\ttraining's binary_logloss: 0.22312\n",
      "[900]\ttraining's binary_logloss: 0.22312\n",
      "[925]\ttraining's binary_logloss: 0.22312\n",
      "[950]\ttraining's binary_logloss: 0.22312\n",
      "[975]\ttraining's binary_logloss: 0.22312\n",
      "[1000]\ttraining's binary_logloss: 0.22312\n",
      "  Continuing training … (batch 55)\n",
      "[825]\ttraining's binary_logloss: 0.198253\n",
      "[850]\ttraining's binary_logloss: 0.198253\n",
      "[875]\ttraining's binary_logloss: 0.198253\n",
      "[900]\ttraining's binary_logloss: 0.198253\n",
      "[925]\ttraining's binary_logloss: 0.198253\n",
      "[950]\ttraining's binary_logloss: 0.198253\n",
      "[975]\ttraining's binary_logloss: 0.198253\n",
      "[1000]\ttraining's binary_logloss: 0.198253\n",
      "  Continuing training … (batch 56)\n",
      "[825]\ttraining's binary_logloss: 0.210341\n",
      "[850]\ttraining's binary_logloss: 0.210341\n",
      "[875]\ttraining's binary_logloss: 0.210341\n",
      "[900]\ttraining's binary_logloss: 0.210341\n",
      "[925]\ttraining's binary_logloss: 0.210341\n",
      "[950]\ttraining's binary_logloss: 0.210341\n",
      "[975]\ttraining's binary_logloss: 0.210341\n",
      "[1000]\ttraining's binary_logloss: 0.210341\n",
      "  Continuing training … (batch 57)\n",
      "[825]\ttraining's binary_logloss: 0.17373\n",
      "[850]\ttraining's binary_logloss: 0.17373\n",
      "[875]\ttraining's binary_logloss: 0.17373\n",
      "[900]\ttraining's binary_logloss: 0.17373\n",
      "[925]\ttraining's binary_logloss: 0.17373\n",
      "[950]\ttraining's binary_logloss: 0.17373\n",
      "[975]\ttraining's binary_logloss: 0.17373\n",
      "[1000]\ttraining's binary_logloss: 0.17373\n",
      "  Continuing training … (batch 58)\n",
      "[825]\ttraining's binary_logloss: 0.239008\n",
      "[850]\ttraining's binary_logloss: 0.239008\n",
      "[875]\ttraining's binary_logloss: 0.239008\n",
      "[900]\ttraining's binary_logloss: 0.239008\n",
      "[925]\ttraining's binary_logloss: 0.239008\n",
      "[950]\ttraining's binary_logloss: 0.239008\n",
      "[975]\ttraining's binary_logloss: 0.239008\n",
      "[1000]\ttraining's binary_logloss: 0.239008\n",
      "  Continuing training … (batch 59)\n",
      "[825]\ttraining's binary_logloss: 0.165786\n",
      "[850]\ttraining's binary_logloss: 0.165786\n",
      "[875]\ttraining's binary_logloss: 0.165786\n",
      "[900]\ttraining's binary_logloss: 0.165786\n",
      "[925]\ttraining's binary_logloss: 0.165786\n",
      "[950]\ttraining's binary_logloss: 0.165786\n",
      "[975]\ttraining's binary_logloss: 0.165786\n",
      "[1000]\ttraining's binary_logloss: 0.165786\n",
      "  Continuing training … (batch 60)\n",
      "[825]\ttraining's binary_logloss: 0.229683\n",
      "[850]\ttraining's binary_logloss: 0.229683\n",
      "[875]\ttraining's binary_logloss: 0.229683\n",
      "[900]\ttraining's binary_logloss: 0.229683\n",
      "[925]\ttraining's binary_logloss: 0.229683\n",
      "[950]\ttraining's binary_logloss: 0.229683\n",
      "[975]\ttraining's binary_logloss: 0.229683\n",
      "[1000]\ttraining's binary_logloss: 0.229683\n",
      "  Continuing training … (batch 61)\n",
      "[825]\ttraining's binary_logloss: 0.292543\n",
      "[850]\ttraining's binary_logloss: 0.292543\n",
      "[875]\ttraining's binary_logloss: 0.292543\n",
      "[900]\ttraining's binary_logloss: 0.292543\n",
      "[925]\ttraining's binary_logloss: 0.292543\n",
      "[950]\ttraining's binary_logloss: 0.292543\n",
      "[975]\ttraining's binary_logloss: 0.292543\n",
      "[1000]\ttraining's binary_logloss: 0.292543\n",
      "  Continuing training … (batch 62)\n",
      "[825]\ttraining's binary_logloss: 0.255587\n",
      "[850]\ttraining's binary_logloss: 0.255587\n",
      "[875]\ttraining's binary_logloss: 0.255587\n",
      "[900]\ttraining's binary_logloss: 0.255587\n",
      "[925]\ttraining's binary_logloss: 0.255587\n",
      "[950]\ttraining's binary_logloss: 0.255587\n",
      "[975]\ttraining's binary_logloss: 0.255587\n",
      "[1000]\ttraining's binary_logloss: 0.255587\n",
      "  Continuing training … (batch 63)\n",
      "[825]\ttraining's binary_logloss: 0.336408\n",
      "[850]\ttraining's binary_logloss: 0.336408\n",
      "[875]\ttraining's binary_logloss: 0.336408\n",
      "[900]\ttraining's binary_logloss: 0.336408\n",
      "[925]\ttraining's binary_logloss: 0.336408\n",
      "[950]\ttraining's binary_logloss: 0.336408\n",
      "[975]\ttraining's binary_logloss: 0.336408\n",
      "[1000]\ttraining's binary_logloss: 0.336408\n",
      "  Continuing training … (batch 64)\n",
      "[825]\ttraining's binary_logloss: 0.302905\n",
      "[850]\ttraining's binary_logloss: 0.302905\n",
      "[875]\ttraining's binary_logloss: 0.302905\n",
      "[900]\ttraining's binary_logloss: 0.302905\n",
      "[925]\ttraining's binary_logloss: 0.302905\n",
      "[950]\ttraining's binary_logloss: 0.302905\n",
      "[975]\ttraining's binary_logloss: 0.302905\n",
      "[1000]\ttraining's binary_logloss: 0.302905\n",
      "  Continuing training … (batch 65)\n",
      "[825]\ttraining's binary_logloss: 0.304287\n",
      "[850]\ttraining's binary_logloss: 0.304287\n",
      "[875]\ttraining's binary_logloss: 0.304287\n",
      "[900]\ttraining's binary_logloss: 0.304287\n",
      "[925]\ttraining's binary_logloss: 0.304287\n",
      "[950]\ttraining's binary_logloss: 0.304287\n",
      "[975]\ttraining's binary_logloss: 0.304287\n",
      "[1000]\ttraining's binary_logloss: 0.304287\n",
      "  Continuing training … (batch 66)\n",
      "[825]\ttraining's binary_logloss: 0.276656\n",
      "[850]\ttraining's binary_logloss: 0.276656\n",
      "[875]\ttraining's binary_logloss: 0.276656\n",
      "[900]\ttraining's binary_logloss: 0.276656\n",
      "[925]\ttraining's binary_logloss: 0.276656\n",
      "[950]\ttraining's binary_logloss: 0.276656\n",
      "[975]\ttraining's binary_logloss: 0.276656\n",
      "[1000]\ttraining's binary_logloss: 0.276656\n",
      "  Continuing training … (batch 67)\n",
      "[825]\ttraining's binary_logloss: 0.336753\n",
      "[850]\ttraining's binary_logloss: 0.336753\n",
      "[875]\ttraining's binary_logloss: 0.336753\n",
      "[900]\ttraining's binary_logloss: 0.336753\n",
      "[925]\ttraining's binary_logloss: 0.336753\n",
      "[950]\ttraining's binary_logloss: 0.336753\n",
      "[975]\ttraining's binary_logloss: 0.336753\n",
      "[1000]\ttraining's binary_logloss: 0.336753\n",
      "  Continuing training … (batch 68)\n",
      "[825]\ttraining's binary_logloss: 0.30256\n",
      "[850]\ttraining's binary_logloss: 0.30256\n",
      "[875]\ttraining's binary_logloss: 0.30256\n",
      "[900]\ttraining's binary_logloss: 0.30256\n",
      "[925]\ttraining's binary_logloss: 0.30256\n",
      "[950]\ttraining's binary_logloss: 0.30256\n",
      "[975]\ttraining's binary_logloss: 0.30256\n",
      "[1000]\ttraining's binary_logloss: 0.30256\n",
      "  Continuing training … (batch 69)\n",
      "[825]\ttraining's binary_logloss: 0.24488\n",
      "[850]\ttraining's binary_logloss: 0.24488\n",
      "[875]\ttraining's binary_logloss: 0.24488\n",
      "[900]\ttraining's binary_logloss: 0.24488\n",
      "[925]\ttraining's binary_logloss: 0.24488\n",
      "[950]\ttraining's binary_logloss: 0.24488\n",
      "[975]\ttraining's binary_logloss: 0.24488\n",
      "[1000]\ttraining's binary_logloss: 0.24488\n",
      "  Continuing training … (batch 70)\n",
      "[825]\ttraining's binary_logloss: 0.354713\n",
      "[850]\ttraining's binary_logloss: 0.354713\n",
      "[875]\ttraining's binary_logloss: 0.354713\n",
      "[900]\ttraining's binary_logloss: 0.354713\n",
      "[925]\ttraining's binary_logloss: 0.354713\n",
      "[950]\ttraining's binary_logloss: 0.354713\n",
      "[975]\ttraining's binary_logloss: 0.354713\n",
      "[1000]\ttraining's binary_logloss: 0.354713\n",
      "  Continuing training … (batch 71)\n",
      "[825]\ttraining's binary_logloss: 0.227611\n",
      "[850]\ttraining's binary_logloss: 0.227611\n",
      "[875]\ttraining's binary_logloss: 0.227611\n",
      "[900]\ttraining's binary_logloss: 0.227611\n",
      "[925]\ttraining's binary_logloss: 0.227611\n",
      "[950]\ttraining's binary_logloss: 0.227611\n",
      "[975]\ttraining's binary_logloss: 0.227611\n",
      "[1000]\ttraining's binary_logloss: 0.227611\n",
      "  Continuing training … (batch 72)\n",
      "[825]\ttraining's binary_logloss: 0.256968\n",
      "[850]\ttraining's binary_logloss: 0.256968\n",
      "[875]\ttraining's binary_logloss: 0.256968\n",
      "[900]\ttraining's binary_logloss: 0.256968\n",
      "[925]\ttraining's binary_logloss: 0.256968\n",
      "[950]\ttraining's binary_logloss: 0.256968\n",
      "[975]\ttraining's binary_logloss: 0.256968\n",
      "[1000]\ttraining's binary_logloss: 0.256968\n",
      "  Continuing training … (batch 73)\n",
      "[825]\ttraining's binary_logloss: 0.714953\n",
      "[850]\ttraining's binary_logloss: 0.714953\n",
      "[875]\ttraining's binary_logloss: 0.714953\n",
      "[900]\ttraining's binary_logloss: 0.714953\n",
      "[925]\ttraining's binary_logloss: 0.714953\n",
      "[950]\ttraining's binary_logloss: 0.714953\n",
      "[975]\ttraining's binary_logloss: 0.714953\n",
      "[1000]\ttraining's binary_logloss: 0.714953\n",
      "  Continuing training … (batch 74)\n",
      "[825]\ttraining's binary_logloss: 0.337789\n",
      "[850]\ttraining's binary_logloss: 0.337789\n",
      "[875]\ttraining's binary_logloss: 0.337789\n",
      "[900]\ttraining's binary_logloss: 0.337789\n",
      "[925]\ttraining's binary_logloss: 0.337789\n",
      "[950]\ttraining's binary_logloss: 0.337789\n",
      "[975]\ttraining's binary_logloss: 0.337789\n",
      "[1000]\ttraining's binary_logloss: 0.337789\n",
      "  Continuing training … (batch 75)\n",
      "[825]\ttraining's binary_logloss: 0.419301\n",
      "[850]\ttraining's binary_logloss: 0.419301\n",
      "[875]\ttraining's binary_logloss: 0.419301\n",
      "[900]\ttraining's binary_logloss: 0.419301\n",
      "[925]\ttraining's binary_logloss: 0.419301\n",
      "[950]\ttraining's binary_logloss: 0.419301\n",
      "[975]\ttraining's binary_logloss: 0.419301\n",
      "[1000]\ttraining's binary_logloss: 0.419301\n",
      "  Continuing training … (batch 76)\n",
      "[825]\ttraining's binary_logloss: 0.466273\n",
      "[850]\ttraining's binary_logloss: 0.466273\n",
      "[875]\ttraining's binary_logloss: 0.466273\n",
      "[900]\ttraining's binary_logloss: 0.466273\n",
      "[925]\ttraining's binary_logloss: 0.466273\n",
      "[950]\ttraining's binary_logloss: 0.466273\n",
      "[975]\ttraining's binary_logloss: 0.466273\n",
      "[1000]\ttraining's binary_logloss: 0.466273\n",
      "  Continuing training … (batch 77)\n",
      "[825]\ttraining's binary_logloss: 0.466273\n",
      "[850]\ttraining's binary_logloss: 0.466273\n",
      "[875]\ttraining's binary_logloss: 0.466273\n",
      "[900]\ttraining's binary_logloss: 0.466273\n",
      "[925]\ttraining's binary_logloss: 0.466273\n",
      "[950]\ttraining's binary_logloss: 0.466273\n",
      "[975]\ttraining's binary_logloss: 0.466273\n",
      "[1000]\ttraining's binary_logloss: 0.466273\n",
      "  Continuing training … (batch 78)\n",
      "[825]\ttraining's binary_logloss: 0.361966\n",
      "[850]\ttraining's binary_logloss: 0.361966\n",
      "[875]\ttraining's binary_logloss: 0.361966\n",
      "[900]\ttraining's binary_logloss: 0.361966\n",
      "[925]\ttraining's binary_logloss: 0.361966\n",
      "[950]\ttraining's binary_logloss: 0.361966\n",
      "[975]\ttraining's binary_logloss: 0.361966\n",
      "[1000]\ttraining's binary_logloss: 0.361966\n",
      "  Continuing training … (batch 79)\n",
      "[825]\ttraining's binary_logloss: 0.432771\n",
      "[850]\ttraining's binary_logloss: 0.432771\n",
      "[875]\ttraining's binary_logloss: 0.432771\n",
      "[900]\ttraining's binary_logloss: 0.432771\n",
      "[925]\ttraining's binary_logloss: 0.432771\n",
      "[950]\ttraining's binary_logloss: 0.432771\n",
      "[975]\ttraining's binary_logloss: 0.432771\n",
      "[1000]\ttraining's binary_logloss: 0.432771\n",
      "  Continuing training … (batch 80)\n",
      "[825]\ttraining's binary_logloss: 1.15488\n",
      "[850]\ttraining's binary_logloss: 1.15488\n",
      "[875]\ttraining's binary_logloss: 1.15488\n",
      "[900]\ttraining's binary_logloss: 1.15488\n",
      "[925]\ttraining's binary_logloss: 1.15488\n",
      "[950]\ttraining's binary_logloss: 1.15488\n",
      "[975]\ttraining's binary_logloss: 1.15488\n",
      "[1000]\ttraining's binary_logloss: 1.15488\n",
      "\n",
      "Processing: D:\\Canada\\Subjects\\Semester -2\\AIDI-2005-02 CAPSTONE TERM ll\\Bot_detector\\data\\cleaned\\Thursday-01-03-2018_TrafficForML_CICFlowMeter.parquet\n",
      "  Continuing training … (batch 1)\n",
      "[825]\ttraining's binary_logloss: 0.23141\n",
      "[850]\ttraining's binary_logloss: 0.23141\n",
      "[875]\ttraining's binary_logloss: 0.23141\n",
      "[900]\ttraining's binary_logloss: 0.23141\n",
      "[925]\ttraining's binary_logloss: 0.23141\n",
      "[950]\ttraining's binary_logloss: 0.23141\n",
      "[975]\ttraining's binary_logloss: 0.23141\n",
      "[1000]\ttraining's binary_logloss: 0.23141\n",
      "  Continuing training … (batch 2)\n",
      "[825]\ttraining's binary_logloss: 18.7521\n",
      "[850]\ttraining's binary_logloss: 18.7521\n",
      "[875]\ttraining's binary_logloss: 18.7521\n",
      "[900]\ttraining's binary_logloss: 18.7521\n",
      "[925]\ttraining's binary_logloss: 18.7521\n",
      "[950]\ttraining's binary_logloss: 18.7521\n",
      "[975]\ttraining's binary_logloss: 18.7521\n",
      "[1000]\ttraining's binary_logloss: 18.7521\n",
      "  Continuing training … (batch 3)\n",
      "[825]\ttraining's binary_logloss: 13.4309\n",
      "[850]\ttraining's binary_logloss: 13.4309\n",
      "[875]\ttraining's binary_logloss: 13.4309\n",
      "[900]\ttraining's binary_logloss: 13.4309\n",
      "[925]\ttraining's binary_logloss: 13.4309\n",
      "[950]\ttraining's binary_logloss: 13.4309\n",
      "[975]\ttraining's binary_logloss: 13.4309\n",
      "[1000]\ttraining's binary_logloss: 13.4309\n",
      "  Continuing training … (batch 4)\n",
      "[825]\ttraining's binary_logloss: 0.227667\n",
      "[850]\ttraining's binary_logloss: 0.227667\n",
      "[875]\ttraining's binary_logloss: 0.227667\n",
      "[900]\ttraining's binary_logloss: 0.227667\n",
      "[925]\ttraining's binary_logloss: 0.227667\n",
      "[950]\ttraining's binary_logloss: 0.227667\n",
      "[975]\ttraining's binary_logloss: 0.227667\n",
      "[1000]\ttraining's binary_logloss: 0.227667\n",
      "\n",
      "Processing: D:\\Canada\\Subjects\\Semester -2\\AIDI-2005-02 CAPSTONE TERM ll\\Bot_detector\\data\\cleaned\\Thursday-15-02-2018_TrafficForML_CICFlowMeter.parquet\n",
      "  Continuing training … (batch 1)\n",
      "[825]\ttraining's binary_logloss: 18.2696\n",
      "[850]\ttraining's binary_logloss: 18.2696\n",
      "[875]\ttraining's binary_logloss: 18.2696\n",
      "[900]\ttraining's binary_logloss: 18.2696\n",
      "[925]\ttraining's binary_logloss: 18.2696\n",
      "[950]\ttraining's binary_logloss: 18.2696\n",
      "[975]\ttraining's binary_logloss: 18.2696\n",
      "[1000]\ttraining's binary_logloss: 18.2696\n",
      "  Continuing training … (batch 2)\n",
      "[825]\ttraining's binary_logloss: 0.227265\n",
      "[850]\ttraining's binary_logloss: 0.227265\n",
      "[875]\ttraining's binary_logloss: 0.227265\n",
      "[900]\ttraining's binary_logloss: 0.227265\n",
      "[925]\ttraining's binary_logloss: 0.227265\n",
      "[950]\ttraining's binary_logloss: 0.227265\n",
      "[975]\ttraining's binary_logloss: 0.227265\n",
      "[1000]\ttraining's binary_logloss: 0.227265\n",
      "  Continuing training … (batch 3)\n",
      "[825]\ttraining's binary_logloss: 0.328464\n",
      "[850]\ttraining's binary_logloss: 0.328464\n",
      "[875]\ttraining's binary_logloss: 0.328464\n",
      "[900]\ttraining's binary_logloss: 0.328464\n",
      "[925]\ttraining's binary_logloss: 0.328464\n",
      "[950]\ttraining's binary_logloss: 0.328464\n",
      "[975]\ttraining's binary_logloss: 0.328464\n",
      "[1000]\ttraining's binary_logloss: 0.328464\n",
      "  Continuing training … (batch 4)\n",
      "[825]\ttraining's binary_logloss: 0.228301\n",
      "[850]\ttraining's binary_logloss: 0.228301\n",
      "[875]\ttraining's binary_logloss: 0.228301\n",
      "[900]\ttraining's binary_logloss: 0.228301\n",
      "[925]\ttraining's binary_logloss: 0.228301\n",
      "[950]\ttraining's binary_logloss: 0.228301\n",
      "[975]\ttraining's binary_logloss: 0.228301\n",
      "[1000]\ttraining's binary_logloss: 0.228301\n",
      "  Continuing training … (batch 5)\n",
      "[825]\ttraining's binary_logloss: 0.329845\n",
      "[850]\ttraining's binary_logloss: 0.329845\n",
      "[875]\ttraining's binary_logloss: 0.329845\n",
      "[900]\ttraining's binary_logloss: 0.329845\n",
      "[925]\ttraining's binary_logloss: 0.329845\n",
      "[950]\ttraining's binary_logloss: 0.329845\n",
      "[975]\ttraining's binary_logloss: 0.329845\n",
      "[1000]\ttraining's binary_logloss: 0.329845\n",
      "  Continuing training … (batch 6)\n",
      "[825]\ttraining's binary_logloss: 0.263185\n",
      "[850]\ttraining's binary_logloss: 0.263185\n",
      "[875]\ttraining's binary_logloss: 0.263185\n",
      "[900]\ttraining's binary_logloss: 0.263185\n",
      "[925]\ttraining's binary_logloss: 0.263185\n",
      "[950]\ttraining's binary_logloss: 0.263185\n",
      "[975]\ttraining's binary_logloss: 0.263185\n",
      "[1000]\ttraining's binary_logloss: 0.263185\n",
      "  Continuing training … (batch 7)\n",
      "[825]\ttraining's binary_logloss: 0.314303\n",
      "[850]\ttraining's binary_logloss: 0.314303\n",
      "[875]\ttraining's binary_logloss: 0.314303\n",
      "[900]\ttraining's binary_logloss: 0.314303\n",
      "[925]\ttraining's binary_logloss: 0.314303\n",
      "[950]\ttraining's binary_logloss: 0.314303\n",
      "[975]\ttraining's binary_logloss: 0.314303\n",
      "[1000]\ttraining's binary_logloss: 0.314303\n",
      "  Continuing training … (batch 8)\n",
      "[825]\ttraining's binary_logloss: 0.233482\n",
      "[850]\ttraining's binary_logloss: 0.233482\n",
      "[875]\ttraining's binary_logloss: 0.233482\n",
      "[900]\ttraining's binary_logloss: 0.233482\n",
      "[925]\ttraining's binary_logloss: 0.233482\n",
      "[950]\ttraining's binary_logloss: 0.233482\n",
      "[975]\ttraining's binary_logloss: 0.233482\n",
      "[1000]\ttraining's binary_logloss: 0.233482\n",
      "  Continuing training … (batch 9)\n",
      "[825]\ttraining's binary_logloss: 0.232101\n",
      "[850]\ttraining's binary_logloss: 0.232101\n",
      "[875]\ttraining's binary_logloss: 0.232101\n",
      "[900]\ttraining's binary_logloss: 0.232101\n",
      "[925]\ttraining's binary_logloss: 0.232101\n",
      "[950]\ttraining's binary_logloss: 0.232101\n",
      "[975]\ttraining's binary_logloss: 0.232101\n",
      "[1000]\ttraining's binary_logloss: 0.232101\n",
      "  Continuing training … (batch 10)\n",
      "[825]\ttraining's binary_logloss: 0.209996\n",
      "[850]\ttraining's binary_logloss: 0.209996\n",
      "[875]\ttraining's binary_logloss: 0.209996\n",
      "[900]\ttraining's binary_logloss: 0.209996\n",
      "[925]\ttraining's binary_logloss: 0.209996\n",
      "[950]\ttraining's binary_logloss: 0.209996\n",
      "[975]\ttraining's binary_logloss: 0.209996\n",
      "[1000]\ttraining's binary_logloss: 0.209996\n",
      "  Continuing training … (batch 11)\n",
      "[825]\ttraining's binary_logloss: 0.243887\n",
      "[850]\ttraining's binary_logloss: 0.243887\n",
      "[875]\ttraining's binary_logloss: 0.243887\n",
      "[900]\ttraining's binary_logloss: 0.243887\n",
      "[925]\ttraining's binary_logloss: 0.243887\n",
      "[950]\ttraining's binary_logloss: 0.243887\n",
      "[975]\ttraining's binary_logloss: 0.243887\n",
      "[1000]\ttraining's binary_logloss: 0.243887\n",
      "\n",
      "Processing: D:\\Canada\\Subjects\\Semester -2\\AIDI-2005-02 CAPSTONE TERM ll\\Bot_detector\\data\\cleaned\\Thursday-22-02-2018_TrafficForML_CICFlowMeter.parquet\n",
      "  Continuing training … (batch 1)\n",
      "[825]\ttraining's binary_logloss: 0.447968\n",
      "[850]\ttraining's binary_logloss: 0.447968\n",
      "[875]\ttraining's binary_logloss: 0.447968\n",
      "[900]\ttraining's binary_logloss: 0.447968\n",
      "[925]\ttraining's binary_logloss: 0.447968\n",
      "[950]\ttraining's binary_logloss: 0.447968\n",
      "[975]\ttraining's binary_logloss: 0.447968\n",
      "[1000]\ttraining's binary_logloss: 0.447968\n",
      "  Continuing training … (batch 2)\n",
      "[825]\ttraining's binary_logloss: 0.323628\n",
      "[850]\ttraining's binary_logloss: 0.323628\n",
      "[875]\ttraining's binary_logloss: 0.323628\n",
      "[900]\ttraining's binary_logloss: 0.323628\n",
      "[925]\ttraining's binary_logloss: 0.323628\n",
      "[950]\ttraining's binary_logloss: 0.323628\n",
      "[975]\ttraining's binary_logloss: 0.323628\n",
      "[1000]\ttraining's binary_logloss: 0.323628\n",
      "  Continuing training … (batch 3)\n",
      "[825]\ttraining's binary_logloss: 0.508065\n",
      "[850]\ttraining's binary_logloss: 0.508065\n",
      "[875]\ttraining's binary_logloss: 0.508065\n",
      "[900]\ttraining's binary_logloss: 0.508065\n",
      "[925]\ttraining's binary_logloss: 0.508065\n",
      "[950]\ttraining's binary_logloss: 0.508065\n",
      "[975]\ttraining's binary_logloss: 0.508065\n",
      "[1000]\ttraining's binary_logloss: 0.508065\n",
      "  Continuing training … (batch 4)\n",
      "[825]\ttraining's binary_logloss: 0.275274\n",
      "[850]\ttraining's binary_logloss: 0.275274\n",
      "[875]\ttraining's binary_logloss: 0.275274\n",
      "[900]\ttraining's binary_logloss: 0.275274\n",
      "[925]\ttraining's binary_logloss: 0.275274\n",
      "[950]\ttraining's binary_logloss: 0.275274\n",
      "[975]\ttraining's binary_logloss: 0.275274\n",
      "[1000]\ttraining's binary_logloss: 0.275274\n",
      "  Continuing training … (batch 5)\n",
      "[825]\ttraining's binary_logloss: 0.685707\n",
      "[850]\ttraining's binary_logloss: 0.685707\n",
      "[875]\ttraining's binary_logloss: 0.685707\n",
      "[900]\ttraining's binary_logloss: 0.685707\n",
      "[925]\ttraining's binary_logloss: 0.685707\n",
      "[950]\ttraining's binary_logloss: 0.685707\n",
      "[975]\ttraining's binary_logloss: 0.685707\n",
      "[1000]\ttraining's binary_logloss: 0.685707\n",
      "  Continuing training … (batch 6)\n",
      "[825]\ttraining's binary_logloss: 0.417919\n",
      "[850]\ttraining's binary_logloss: 0.417919\n",
      "[875]\ttraining's binary_logloss: 0.417919\n",
      "[900]\ttraining's binary_logloss: 0.417919\n",
      "[925]\ttraining's binary_logloss: 0.417919\n",
      "[950]\ttraining's binary_logloss: 0.417919\n",
      "[975]\ttraining's binary_logloss: 0.417919\n",
      "[1000]\ttraining's binary_logloss: 0.417919\n",
      "  Continuing training … (batch 7)\n",
      "[825]\ttraining's binary_logloss: 0.397541\n",
      "[850]\ttraining's binary_logloss: 0.397541\n",
      "[875]\ttraining's binary_logloss: 0.397541\n",
      "[900]\ttraining's binary_logloss: 0.397541\n",
      "[925]\ttraining's binary_logloss: 0.397541\n",
      "[950]\ttraining's binary_logloss: 0.397541\n",
      "[975]\ttraining's binary_logloss: 0.397541\n",
      "[1000]\ttraining's binary_logloss: 0.397541\n",
      "  Continuing training … (batch 8)\n",
      "[825]\ttraining's binary_logloss: 0.437952\n",
      "[850]\ttraining's binary_logloss: 0.437952\n",
      "[875]\ttraining's binary_logloss: 0.437952\n",
      "[900]\ttraining's binary_logloss: 0.437952\n",
      "[925]\ttraining's binary_logloss: 0.437952\n",
      "[950]\ttraining's binary_logloss: 0.437952\n",
      "[975]\ttraining's binary_logloss: 0.437952\n",
      "[1000]\ttraining's binary_logloss: 0.437952\n",
      "  Continuing training … (batch 9)\n",
      "[825]\ttraining's binary_logloss: 0.229683\n",
      "[850]\ttraining's binary_logloss: 0.229683\n",
      "[875]\ttraining's binary_logloss: 0.229683\n",
      "[900]\ttraining's binary_logloss: 0.229683\n",
      "[925]\ttraining's binary_logloss: 0.229683\n",
      "[950]\ttraining's binary_logloss: 0.229683\n",
      "[975]\ttraining's binary_logloss: 0.229683\n",
      "[1000]\ttraining's binary_logloss: 0.229683\n",
      "  Continuing training … (batch 10)\n",
      "[825]\ttraining's binary_logloss: 0.306359\n",
      "[850]\ttraining's binary_logloss: 0.306359\n",
      "[875]\ttraining's binary_logloss: 0.306359\n",
      "[900]\ttraining's binary_logloss: 0.306359\n",
      "[925]\ttraining's binary_logloss: 0.306359\n",
      "[950]\ttraining's binary_logloss: 0.306359\n",
      "[975]\ttraining's binary_logloss: 0.306359\n",
      "[1000]\ttraining's binary_logloss: 0.306359\n",
      "  Continuing training … (batch 11)\n",
      "[825]\ttraining's binary_logloss: 0.76579\n",
      "[850]\ttraining's binary_logloss: 0.76579\n",
      "[875]\ttraining's binary_logloss: 0.76579\n",
      "[900]\ttraining's binary_logloss: 0.76579\n",
      "[925]\ttraining's binary_logloss: 0.76579\n",
      "[950]\ttraining's binary_logloss: 0.76579\n",
      "[975]\ttraining's binary_logloss: 0.76579\n",
      "[1000]\ttraining's binary_logloss: 0.76579\n",
      "\n",
      "Processing: D:\\Canada\\Subjects\\Semester -2\\AIDI-2005-02 CAPSTONE TERM ll\\Bot_detector\\data\\cleaned\\Wednesday-14-02-2018_TrafficForML_CICFlowMeter.parquet\n",
      "  Continuing training … (batch 1)\n",
      "[825]\ttraining's binary_logloss: 34.4997\n",
      "[850]\ttraining's binary_logloss: 34.4997\n",
      "[875]\ttraining's binary_logloss: 34.4997\n",
      "[900]\ttraining's binary_logloss: 34.4997\n",
      "[925]\ttraining's binary_logloss: 34.4997\n",
      "[950]\ttraining's binary_logloss: 34.4997\n",
      "[975]\ttraining's binary_logloss: 34.4997\n",
      "[1000]\ttraining's binary_logloss: 34.4997\n",
      "  Continuing training … (batch 2)\n",
      "[825]\ttraining's binary_logloss: 34.4883\n",
      "[850]\ttraining's binary_logloss: 34.4883\n",
      "[875]\ttraining's binary_logloss: 34.4883\n",
      "[900]\ttraining's binary_logloss: 34.4883\n",
      "[925]\ttraining's binary_logloss: 34.4883\n",
      "[950]\ttraining's binary_logloss: 34.4883\n",
      "[975]\ttraining's binary_logloss: 34.4883\n",
      "[1000]\ttraining's binary_logloss: 34.4883\n",
      "  Continuing training … (batch 3)\n",
      "[825]\ttraining's binary_logloss: 34.3616\n",
      "[850]\ttraining's binary_logloss: 34.3616\n",
      "[875]\ttraining's binary_logloss: 34.3616\n",
      "[900]\ttraining's binary_logloss: 34.3616\n",
      "[925]\ttraining's binary_logloss: 34.3616\n",
      "[950]\ttraining's binary_logloss: 34.3616\n",
      "[975]\ttraining's binary_logloss: 34.3616\n",
      "[1000]\ttraining's binary_logloss: 34.3616\n",
      "  Continuing training … (batch 4)\n",
      "[825]\ttraining's binary_logloss: 28.2755\n",
      "[850]\ttraining's binary_logloss: 28.2755\n",
      "[875]\ttraining's binary_logloss: 28.2755\n",
      "[900]\ttraining's binary_logloss: 28.2755\n",
      "[925]\ttraining's binary_logloss: 28.2755\n",
      "[950]\ttraining's binary_logloss: 28.2755\n",
      "[975]\ttraining's binary_logloss: 28.2755\n",
      "[1000]\ttraining's binary_logloss: 28.2755\n",
      "  Continuing training … (batch 5)\n",
      "[825]\ttraining's binary_logloss: 0.254896\n",
      "[850]\ttraining's binary_logloss: 0.254896\n",
      "[875]\ttraining's binary_logloss: 0.254896\n",
      "[900]\ttraining's binary_logloss: 0.254896\n",
      "[925]\ttraining's binary_logloss: 0.254896\n",
      "[950]\ttraining's binary_logloss: 0.254896\n",
      "[975]\ttraining's binary_logloss: 0.254896\n",
      "[1000]\ttraining's binary_logloss: 0.254896\n",
      "  Continuing training … (batch 6)\n",
      "[825]\ttraining's binary_logloss: 0.24937\n",
      "[850]\ttraining's binary_logloss: 0.24937\n",
      "[875]\ttraining's binary_logloss: 0.24937\n",
      "[900]\ttraining's binary_logloss: 0.24937\n",
      "[925]\ttraining's binary_logloss: 0.24937\n",
      "[950]\ttraining's binary_logloss: 0.24937\n",
      "[975]\ttraining's binary_logloss: 0.24937\n",
      "[1000]\ttraining's binary_logloss: 0.24937\n",
      "  Continuing training … (batch 7)\n",
      "[825]\ttraining's binary_logloss: 0.212759\n",
      "[850]\ttraining's binary_logloss: 0.212759\n",
      "[875]\ttraining's binary_logloss: 0.212759\n",
      "[900]\ttraining's binary_logloss: 0.212759\n",
      "[925]\ttraining's binary_logloss: 0.212759\n",
      "[950]\ttraining's binary_logloss: 0.212759\n",
      "[975]\ttraining's binary_logloss: 0.212759\n",
      "[1000]\ttraining's binary_logloss: 0.212759\n",
      "  Continuing training … (batch 8)\n",
      "[825]\ttraining's binary_logloss: 0.22152\n",
      "[850]\ttraining's binary_logloss: 0.22152\n",
      "[875]\ttraining's binary_logloss: 0.22152\n",
      "[900]\ttraining's binary_logloss: 0.22152\n",
      "[925]\ttraining's binary_logloss: 0.22152\n",
      "[950]\ttraining's binary_logloss: 0.22152\n",
      "[975]\ttraining's binary_logloss: 0.22152\n",
      "[1000]\ttraining's binary_logloss: 0.22152\n",
      "  Continuing training … (batch 9)\n",
      "[825]\ttraining's binary_logloss: 0.262495\n",
      "[850]\ttraining's binary_logloss: 0.262495\n",
      "[875]\ttraining's binary_logloss: 0.262495\n",
      "[900]\ttraining's binary_logloss: 0.262495\n",
      "[925]\ttraining's binary_logloss: 0.262495\n",
      "[950]\ttraining's binary_logloss: 0.262495\n",
      "[975]\ttraining's binary_logloss: 0.262495\n",
      "[1000]\ttraining's binary_logloss: 0.262495\n",
      "  Continuing training … (batch 10)\n",
      "[825]\ttraining's binary_logloss: 0.268712\n",
      "[850]\ttraining's binary_logloss: 0.268712\n",
      "[875]\ttraining's binary_logloss: 0.268712\n",
      "[900]\ttraining's binary_logloss: 0.268712\n",
      "[925]\ttraining's binary_logloss: 0.268712\n",
      "[950]\ttraining's binary_logloss: 0.268712\n",
      "[975]\ttraining's binary_logloss: 0.268712\n",
      "[1000]\ttraining's binary_logloss: 0.268712\n",
      "  Continuing training … (batch 11)\n",
      "[825]\ttraining's binary_logloss: 0.243176\n",
      "[850]\ttraining's binary_logloss: 0.243176\n",
      "[875]\ttraining's binary_logloss: 0.243176\n",
      "[900]\ttraining's binary_logloss: 0.243176\n",
      "[925]\ttraining's binary_logloss: 0.243176\n",
      "[950]\ttraining's binary_logloss: 0.243176\n",
      "[975]\ttraining's binary_logloss: 0.243176\n",
      "[1000]\ttraining's binary_logloss: 0.243176\n",
      "\n",
      "Processing: D:\\Canada\\Subjects\\Semester -2\\AIDI-2005-02 CAPSTONE TERM ll\\Bot_detector\\data\\cleaned\\Wednesday-21-02-2018_TrafficForML_CICFlowMeter.parquet\n",
      "  Continuing training … (batch 1)\n",
      "[825]\ttraining's binary_logloss: 33.7748\n",
      "[850]\ttraining's binary_logloss: 33.7748\n",
      "[875]\ttraining's binary_logloss: 33.7748\n",
      "[900]\ttraining's binary_logloss: 33.7748\n",
      "[925]\ttraining's binary_logloss: 33.7748\n",
      "[950]\ttraining's binary_logloss: 33.7748\n",
      "[975]\ttraining's binary_logloss: 33.7748\n",
      "[1000]\ttraining's binary_logloss: 33.7748\n",
      "  Continuing training … (batch 2)\n",
      "[825]\ttraining's binary_logloss: 34.5229\n",
      "[850]\ttraining's binary_logloss: 34.5229\n",
      "[875]\ttraining's binary_logloss: 34.5229\n",
      "[900]\ttraining's binary_logloss: 34.5229\n",
      "[925]\ttraining's binary_logloss: 34.5229\n",
      "[950]\ttraining's binary_logloss: 34.5229\n",
      "[975]\ttraining's binary_logloss: 34.5229\n",
      "[1000]\ttraining's binary_logloss: 34.5229\n",
      "  Continuing training … (batch 3)\n",
      "[825]\ttraining's binary_logloss: 34.4863\n",
      "[850]\ttraining's binary_logloss: 34.4863\n",
      "[875]\ttraining's binary_logloss: 34.4863\n",
      "[900]\ttraining's binary_logloss: 34.4863\n",
      "[925]\ttraining's binary_logloss: 34.4863\n",
      "[950]\ttraining's binary_logloss: 34.4863\n",
      "[975]\ttraining's binary_logloss: 34.4863\n",
      "[1000]\ttraining's binary_logloss: 34.4863\n",
      "  Continuing training … (batch 4)\n",
      "[825]\ttraining's binary_logloss: 22.5407\n",
      "[850]\ttraining's binary_logloss: 22.5407\n",
      "[875]\ttraining's binary_logloss: 22.5407\n",
      "[900]\ttraining's binary_logloss: 22.5407\n",
      "[925]\ttraining's binary_logloss: 22.5407\n",
      "[950]\ttraining's binary_logloss: 22.5407\n",
      "[975]\ttraining's binary_logloss: 22.5407\n",
      "[1000]\ttraining's binary_logloss: 22.5407\n",
      "  Continuing training … (batch 5)\n",
      "[825]\ttraining's binary_logloss: 17.1015\n",
      "[850]\ttraining's binary_logloss: 17.1015\n",
      "[875]\ttraining's binary_logloss: 17.1015\n",
      "[900]\ttraining's binary_logloss: 17.1015\n",
      "[925]\ttraining's binary_logloss: 17.1015\n",
      "[950]\ttraining's binary_logloss: 17.1015\n",
      "[975]\ttraining's binary_logloss: 17.1015\n",
      "[1000]\ttraining's binary_logloss: 17.1015\n",
      "  Continuing training … (batch 6)\n",
      "[825]\ttraining's binary_logloss: 17.1267\n",
      "[850]\ttraining's binary_logloss: 17.1267\n",
      "[875]\ttraining's binary_logloss: 17.1267\n",
      "[900]\ttraining's binary_logloss: 17.1267\n",
      "[925]\ttraining's binary_logloss: 17.1267\n",
      "[950]\ttraining's binary_logloss: 17.1267\n",
      "[975]\ttraining's binary_logloss: 17.1267\n",
      "[1000]\ttraining's binary_logloss: 17.1267\n",
      "  Continuing training … (batch 7)\n",
      "[825]\ttraining's binary_logloss: 17.0839\n",
      "[850]\ttraining's binary_logloss: 17.0839\n",
      "[875]\ttraining's binary_logloss: 17.0839\n",
      "[900]\ttraining's binary_logloss: 17.0839\n",
      "[925]\ttraining's binary_logloss: 17.0839\n",
      "[950]\ttraining's binary_logloss: 17.0839\n",
      "[975]\ttraining's binary_logloss: 17.0839\n",
      "[1000]\ttraining's binary_logloss: 17.0839\n",
      "  Continuing training … (batch 8)\n",
      "[825]\ttraining's binary_logloss: 16.9313\n",
      "[850]\ttraining's binary_logloss: 16.9313\n",
      "[875]\ttraining's binary_logloss: 16.9313\n",
      "[900]\ttraining's binary_logloss: 16.9313\n",
      "[925]\ttraining's binary_logloss: 16.9313\n",
      "[950]\ttraining's binary_logloss: 16.9313\n",
      "[975]\ttraining's binary_logloss: 16.9313\n",
      "[1000]\ttraining's binary_logloss: 16.9313\n",
      "  Continuing training … (batch 9)\n",
      "[825]\ttraining's binary_logloss: 16.8615\n",
      "[850]\ttraining's binary_logloss: 16.8615\n",
      "[875]\ttraining's binary_logloss: 16.8615\n",
      "[900]\ttraining's binary_logloss: 16.8615\n",
      "[925]\ttraining's binary_logloss: 16.8615\n",
      "[950]\ttraining's binary_logloss: 16.8615\n",
      "[975]\ttraining's binary_logloss: 16.8615\n",
      "[1000]\ttraining's binary_logloss: 16.8615\n",
      "  Continuing training … (batch 10)\n",
      "[825]\ttraining's binary_logloss: 17.0563\n",
      "[850]\ttraining's binary_logloss: 17.0563\n",
      "[875]\ttraining's binary_logloss: 17.0563\n",
      "[900]\ttraining's binary_logloss: 17.0563\n",
      "[925]\ttraining's binary_logloss: 17.0563\n",
      "[950]\ttraining's binary_logloss: 17.0563\n",
      "[975]\ttraining's binary_logloss: 17.0563\n",
      "[1000]\ttraining's binary_logloss: 17.0563\n",
      "  Continuing training … (batch 11)\n",
      "[825]\ttraining's binary_logloss: 17.2008\n",
      "[850]\ttraining's binary_logloss: 17.2008\n",
      "[875]\ttraining's binary_logloss: 17.2008\n",
      "[900]\ttraining's binary_logloss: 17.2008\n",
      "[925]\ttraining's binary_logloss: 17.2008\n",
      "[950]\ttraining's binary_logloss: 17.2008\n",
      "[975]\ttraining's binary_logloss: 17.2008\n",
      "[1000]\ttraining's binary_logloss: 17.2008\n",
      "\n",
      "Model saved as: final_lightgbm_tuned.txt\n",
      "\n",
      "Evaluating on: D:\\Canada\\Subjects\\Semester -2\\AIDI-2005-02 CAPSTONE TERM ll\\Bot_detector\\data\\cleaned\\Wednesday-28-02-2018_TrafficForML_CICFlowMeter.parquet\n",
      "\n",
      "=== Test Results (threshold=0.4) ===\n",
      "Accuracy: 0.8833038913926772\n",
      "Confusion Matrix:\n",
      " [[541220   2980]\n",
      " [ 68563    308]]\n",
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0     0.8876    0.9945    0.9380    544200\n",
      "           1     0.0937    0.0045    0.0085     68871\n",
      "\n",
      "    accuracy                         0.8833    613071\n",
      "   macro avg     0.4906    0.4995    0.4733    613071\n",
      "weighted avg     0.7984    0.8833    0.8336    613071\n",
      "\n",
      "\n",
      "Done! The tuned model and report are saved.\n"
     ]
    }
   ],
   "source": [
    "# try\n",
    "\n",
    "import pyarrow.parquet as pq\n",
    "import lightgbm as lgb\n",
    "import numpy as np\n",
    "import os\n",
    "from glob import glob\n",
    "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
    "\n",
    "# === CONFIGURATION ===\n",
    "CLEANED_DIR = r\"D:\\Canada\\Subjects\\Semester -2\\AIDI-2005-02 CAPSTONE TERM ll\\Bot_detector\\data\\cleaned\"\n",
    "MODEL_PATH = \"final_lightgbm_tuned.txt\"\n",
    "BATCH_SIZE = 100_000  # Tune for your RAM\n",
    "\n",
    "# 1. Auto-detect all cleaned parquet files, split train/test (last one as test for demo)\n",
    "file_list = sorted(glob(os.path.join(CLEANED_DIR, \"*.parquet\")))\n",
    "train_files = file_list[:-1]\n",
    "test_file = file_list[-1]\n",
    "\n",
    "print(\"Train files:\", train_files)\n",
    "print(\"Test file:\", test_file)\n",
    "\n",
    "# 2. Pick feature columns (from any file)\n",
    "sample = pq.read_table(train_files[0], columns=None).to_pandas().head(1)\n",
    "feature_cols = [col for col in sample.columns if col != 'Label']\n",
    "\n",
    "# 3. Compute global scale_pos_weight (class 0 / class 1)\n",
    "total_0 = 0\n",
    "total_1 = 0\n",
    "for f in train_files:\n",
    "    df = pq.read_table(f, columns=['Label']).to_pandas()\n",
    "    total_0 += (df['Label'] == 0).sum()\n",
    "    total_1 += (df['Label'] == 1).sum()\n",
    "scale_pos_weight = round(total_0 / total_1, 2)\n",
    "print(f\"scale_pos_weight: {scale_pos_weight}\")\n",
    "\n",
    "# 4. LightGBM parameters (tuned)\n",
    "params = {\n",
    "    'objective': 'binary',\n",
    "    'boosting_type': 'gbdt',\n",
    "    'metric': 'binary_logloss',\n",
    "    'learning_rate': 0.03,      # Slower learning for better results\n",
    "    'num_leaves': 40,           # Try between 32-64 for large data\n",
    "    'max_depth': 10,            # Max tree depth\n",
    "    'scale_pos_weight': scale_pos_weight,  # Handle imbalance\n",
    "    'feature_fraction': 0.8,    # Randomly select features at each tree\n",
    "    'bagging_fraction': 0.8,    # Randomly select data at each iteration\n",
    "    'bagging_freq': 2,          # Perform bagging every 2 iterations\n",
    "    'min_child_samples': 30,    # Minimum samples per leaf\n",
    "    'verbose': -1,\n",
    "    'n_jobs': -1\n",
    "}\n",
    "\n",
    "# === TRAINING ===\n",
    "booster = None\n",
    "for file_idx, file_path in enumerate(train_files):\n",
    "    print(f\"\\nProcessing: {file_path}\")\n",
    "    parquet_file = pq.ParquetFile(file_path)\n",
    "    batch_idx = 1\n",
    "    for batch in parquet_file.iter_batches(batch_size=BATCH_SIZE):\n",
    "        df = batch.to_pandas()\n",
    "        X = df[feature_cols].astype(np.float32)\n",
    "        y = df['Label'].astype(np.int32)\n",
    "        lgb_train = lgb.Dataset(X, y, free_raw_data=True)\n",
    "        if booster is None:\n",
    "            print(\"  Training new model …\")\n",
    "            booster = lgb.train(\n",
    "                params,\n",
    "                lgb_train,\n",
    "                num_boost_round=200,  # More boosting rounds\n",
    "                valid_sets=[lgb_train],\n",
    "                callbacks=[lgb.log_evaluation(period=25)],\n",
    "                keep_training_booster=True\n",
    "            )\n",
    "        else:\n",
    "            print(f\"  Continuing training … (batch {batch_idx})\")\n",
    "            booster = lgb.train(\n",
    "                params,\n",
    "                lgb_train,\n",
    "                num_boost_round=200,\n",
    "                valid_sets=[lgb_train],\n",
    "                init_model=booster,\n",
    "                callbacks=[lgb.log_evaluation(period=25)],\n",
    "                keep_training_booster=True\n",
    "            )\n",
    "        del df, X, y, lgb_train\n",
    "        batch_idx += 1\n",
    "\n",
    "# Save model\n",
    "booster.save_model(MODEL_PATH)\n",
    "print(\"\\nModel saved as:\", MODEL_PATH)\n",
    "\n",
    "# === EVALUATION ===\n",
    "print(f\"\\nEvaluating on: {test_file}\")\n",
    "test_parquet = pq.ParquetFile(test_file)\n",
    "y_true_all, y_pred_all = [], []\n",
    "for batch in test_parquet.iter_batches(batch_size=BATCH_SIZE):\n",
    "    df = batch.to_pandas()\n",
    "    X = df[feature_cols].astype(np.float32)\n",
    "    y = df['Label'].astype(np.int32)\n",
    "    y_pred_proba = booster.predict(X)\n",
    "    y_pred = (y_pred_proba > 0.4).astype(int)  # Lower threshold to catch more attacks (try 0.4, 0.3, etc.)\n",
    "    y_true_all.extend(y.tolist())\n",
    "    y_pred_all.extend(y_pred.tolist())\n",
    "    del df, X, y, y_pred\n",
    "\n",
    "# Results\n",
    "acc = accuracy_score(y_true_all, y_pred_all)\n",
    "cm = confusion_matrix(y_true_all, y_pred_all)\n",
    "cr = classification_report(y_true_all, y_pred_all, digits=4)\n",
    "\n",
    "print(\"\\n=== Test Results (threshold=0.4) ===\")\n",
    "print(\"Accuracy:\", acc)\n",
    "print(\"Confusion Matrix:\\n\", cm)\n",
    "print(\"Classification Report:\\n\", cr)\n",
    "\n",
    "with open(\"classification_report_lgbm_tuned.txt\", \"w\") as f:\n",
    "    f.write(\"Accuracy: {:.4f}\\n\".format(acc))\n",
    "    f.write(\"Confusion Matrix:\\n{}\\n\".format(cm))\n",
    "    f.write(\"Classification Report:\\n{}\\n\".format(cr))\n",
    "\n",
    "print(\"\\nDone! The tuned model and report are saved.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d836791",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sample try"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "25622dd6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Combined sampled shape: (100000, 79)\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import glob\n",
    "from sklearn.model_selection import train_test_split\n",
    "import lightgbm as lgb\n",
    "from sklearn.metrics import classification_report, confusion_matrix, roc_auc_score\n",
    "import optuna\n",
    "\n",
    "# File paths\n",
    "data_path = r'D:\\Canada\\Subjects\\Semester -2\\AIDI-2005-02 CAPSTONE TERM ll\\Bot_detector\\data\\cleaned\\*.parquet'\n",
    "all_files = glob.glob(data_path)\n",
    "\n",
    "# Sample data for prototyping/tuning (10,000 rows from each file)\n",
    "df_list = []\n",
    "for file in all_files:\n",
    "    df_temp = pd.read_parquet(file)\n",
    "    if len(df_temp) > 10000:\n",
    "        df_temp = df_temp.sample(n=10000, random_state=42)\n",
    "    df_list.append(df_temp)\n",
    "df = pd.concat(df_list, ignore_index=True)\n",
    "print(\"Combined sampled shape:\", df.shape)\n",
    "\n",
    "# Convert labels to binary\n",
    "df['Label'] = df['Label'].replace({'Benign': 0, 'Attack': 1}).astype(int)\n",
    "\n",
    "# Select numeric columns for features\n",
    "X = df.drop(columns=['Label']).select_dtypes(include=np.number)\n",
    "y = df['Label']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "4394e3cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.2, random_state=42, stratify=y\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "891e94eb",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-06-16 13:38:56,701] A new study created in memory with name: no-name-68c9f722-33ac-4d96-921c-b7c270b381fb\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training until validation scores don't improve for 20 rounds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-06-16 13:38:59,079] Trial 0 finished with value: 0.9808502718898113 and parameters: {'learning_rate': 0.18139066159802691, 'num_leaves': 117, 'max_depth': 11, 'feature_fraction': 0.888821100941702, 'bagging_fraction': 0.5528380922327147, 'bagging_freq': 1, 'min_data_in_leaf': 75, 'lambda_l1': 3.911840621886989, 'lambda_l2': 0.20239210830115262}. Best is trial 0 with value: 0.9808502718898113.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping, best iteration is:\n",
      "[49]\tvalid_0's auc: 0.98085\n",
      "Training until validation scores don't improve for 20 rounds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-06-16 13:39:00,545] Trial 1 finished with value: 0.9818915813012655 and parameters: {'learning_rate': 0.1254389560847368, 'num_leaves': 78, 'max_depth': 16, 'feature_fraction': 0.6147783077776022, 'bagging_fraction': 0.9960620687306792, 'bagging_freq': 9, 'min_data_in_leaf': 45, 'lambda_l1': 1.0974257818029893, 'lambda_l2': 2.726243314051949}. Best is trial 1 with value: 0.9818915813012655.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping, best iteration is:\n",
      "[52]\tvalid_0's auc: 0.981892\n",
      "Training until validation scores don't improve for 20 rounds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-06-16 13:39:03,541] Trial 2 finished with value: 0.980452426154311 and parameters: {'learning_rate': 0.05466076915383957, 'num_leaves': 18, 'max_depth': 12, 'feature_fraction': 0.846676028403428, 'bagging_fraction': 0.9015152466567351, 'bagging_freq': 7, 'min_data_in_leaf': 86, 'lambda_l1': 1.3696256733101726, 'lambda_l2': 3.0034629514711986}. Best is trial 1 with value: 0.9818915813012655.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Did not meet early stopping. Best iteration is:\n",
      "[198]\tvalid_0's auc: 0.980452\n",
      "Training until validation scores don't improve for 20 rounds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-06-16 13:39:09,263] Trial 3 finished with value: 0.9814388892420678 and parameters: {'learning_rate': 0.12668107092644357, 'num_leaves': 59, 'max_depth': 5, 'feature_fraction': 0.9870233597019524, 'bagging_fraction': 0.7548974698893183, 'bagging_freq': 10, 'min_data_in_leaf': 32, 'lambda_l1': 3.1359638460104855, 'lambda_l2': 0.3205208710185481}. Best is trial 1 with value: 0.9818915813012655.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Did not meet early stopping. Best iteration is:\n",
      "[189]\tvalid_0's auc: 0.981439\n",
      "Training until validation scores don't improve for 20 rounds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-06-16 13:39:14,197] Trial 4 finished with value: 0.9818694087975073 and parameters: {'learning_rate': 0.1078538405843952, 'num_leaves': 21, 'max_depth': 5, 'feature_fraction': 0.9940261550880798, 'bagging_fraction': 0.9913178669289349, 'bagging_freq': 7, 'min_data_in_leaf': 38, 'lambda_l1': 1.631114735664284, 'lambda_l2': 1.6329226194145197}. Best is trial 1 with value: 0.9818915813012655.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Did not meet early stopping. Best iteration is:\n",
      "[195]\tvalid_0's auc: 0.981869\n",
      "Training until validation scores don't improve for 20 rounds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-06-16 13:39:16,065] Trial 5 finished with value: 0.9809459123172579 and parameters: {'learning_rate': 0.16003308428037108, 'num_leaves': 36, 'max_depth': 6, 'feature_fraction': 0.5094051389903242, 'bagging_fraction': 0.5143950968826672, 'bagging_freq': 3, 'min_data_in_leaf': 25, 'lambda_l1': 4.639828000819839, 'lambda_l2': 4.768410781061782}. Best is trial 1 with value: 0.9818915813012655.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping, best iteration is:\n",
      "[114]\tvalid_0's auc: 0.980946\n",
      "Training until validation scores don't improve for 20 rounds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-06-16 13:39:18,886] Trial 6 finished with value: 0.9819046838609043 and parameters: {'learning_rate': 0.08751227127022969, 'num_leaves': 55, 'max_depth': 7, 'feature_fraction': 0.8603448670727547, 'bagging_fraction': 0.8097910918458999, 'bagging_freq': 7, 'min_data_in_leaf': 68, 'lambda_l1': 2.0689076521749348, 'lambda_l2': 3.6283066508439275}. Best is trial 6 with value: 0.9819046838609043.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping, best iteration is:\n",
      "[172]\tvalid_0's auc: 0.981905\n",
      "Training until validation scores don't improve for 20 rounds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-06-16 13:39:21,638] Trial 7 finished with value: 0.9807949094463798 and parameters: {'learning_rate': 0.054974295728494986, 'num_leaves': 19, 'max_depth': 13, 'feature_fraction': 0.591544542789908, 'bagging_fraction': 0.9238747807187657, 'bagging_freq': 7, 'min_data_in_leaf': 25, 'lambda_l1': 4.110391783839177, 'lambda_l2': 0.20515033896329138}. Best is trial 6 with value: 0.9819046838609043.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Did not meet early stopping. Best iteration is:\n",
      "[200]\tvalid_0's auc: 0.980795\n",
      "Training until validation scores don't improve for 20 rounds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-06-16 13:39:24,337] Trial 8 finished with value: 0.9814138402309938 and parameters: {'learning_rate': 0.08087063771328176, 'num_leaves': 25, 'max_depth': 7, 'feature_fraction': 0.9033554719908522, 'bagging_fraction': 0.5446654421821717, 'bagging_freq': 1, 'min_data_in_leaf': 63, 'lambda_l1': 0.5109435388406186, 'lambda_l2': 0.5362095134170819}. Best is trial 6 with value: 0.9819046838609043.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Did not meet early stopping. Best iteration is:\n",
      "[198]\tvalid_0's auc: 0.981414\n",
      "Training until validation scores don't improve for 20 rounds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-06-16 13:39:26,354] Trial 9 finished with value: 0.9818822292116915 and parameters: {'learning_rate': 0.09068827614757424, 'num_leaves': 81, 'max_depth': 8, 'feature_fraction': 0.9935886052673013, 'bagging_fraction': 0.7781696225467769, 'bagging_freq': 1, 'min_data_in_leaf': 50, 'lambda_l1': 0.5497677115841698, 'lambda_l2': 0.023711671984743}. Best is trial 6 with value: 0.9819046838609043.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping, best iteration is:\n",
      "[104]\tvalid_0's auc: 0.981882\n",
      "Training until validation scores don't improve for 20 rounds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-06-16 13:39:29,445] Trial 10 finished with value: 0.9814143976403061 and parameters: {'learning_rate': 0.04016974756186335, 'num_leaves': 53, 'max_depth': 9, 'feature_fraction': 0.745867005510173, 'bagging_fraction': 0.6653527345969691, 'bagging_freq': 4, 'min_data_in_leaf': 100, 'lambda_l1': 2.317069532485327, 'lambda_l2': 4.166689598789}. Best is trial 6 with value: 0.9819046838609043.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Did not meet early stopping. Best iteration is:\n",
      "[200]\tvalid_0's auc: 0.981414\n",
      "Training until validation scores don't improve for 20 rounds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-06-16 13:39:30,385] Trial 11 finished with value: 0.9812583780856168 and parameters: {'learning_rate': 0.13524362692589378, 'num_leaves': 91, 'max_depth': 16, 'feature_fraction': 0.6989966658279754, 'bagging_fraction': 0.8453047489183971, 'bagging_freq': 10, 'min_data_in_leaf': 54, 'lambda_l1': 1.7801938860113524, 'lambda_l2': 3.028358879137993}. Best is trial 6 with value: 0.9819046838609043.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping, best iteration is:\n",
      "[21]\tvalid_0's auc: 0.981258\n",
      "Training until validation scores don't improve for 20 rounds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-06-16 13:39:34,422] Trial 12 finished with value: 0.9815831619116189 and parameters: {'learning_rate': 0.01314133070863198, 'num_leaves': 103, 'max_depth': 16, 'feature_fraction': 0.6480809795038489, 'bagging_fraction': 0.9985490025082345, 'bagging_freq': 8, 'min_data_in_leaf': 67, 'lambda_l1': 2.821643537277791, 'lambda_l2': 3.7851851940844465}. Best is trial 6 with value: 0.9819046838609043.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Did not meet early stopping. Best iteration is:\n",
      "[200]\tvalid_0's auc: 0.981583\n",
      "Training until validation scores don't improve for 20 rounds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-06-16 13:39:35,893] Trial 13 finished with value: 0.9814948847923723 and parameters: {'learning_rate': 0.14028223243614393, 'num_leaves': 65, 'max_depth': 14, 'feature_fraction': 0.8059392830237418, 'bagging_fraction': 0.6595867831233237, 'bagging_freq': 9, 'min_data_in_leaf': 43, 'lambda_l1': 0.05090283176281618, 'lambda_l2': 2.0886602769087586}. Best is trial 6 with value: 0.9819046838609043.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping, best iteration is:\n",
      "[61]\tvalid_0's auc: 0.981495\n",
      "Training until validation scores don't improve for 20 rounds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-06-16 13:39:37,689] Trial 14 finished with value: 0.9818583019008387 and parameters: {'learning_rate': 0.10180027374468578, 'num_leaves': 43, 'max_depth': 10, 'feature_fraction': 0.5746500912412393, 'bagging_fraction': 0.8362518558549568, 'bagging_freq': 5, 'min_data_in_leaf': 75, 'lambda_l1': 1.1302891059837463, 'lambda_l2': 3.3718893900268916}. Best is trial 6 with value: 0.9819046838609043.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping, best iteration is:\n",
      "[94]\tvalid_0's auc: 0.981858\n",
      "Training until validation scores don't improve for 20 rounds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-06-16 13:39:39,770] Trial 15 finished with value: 0.9808819616414584 and parameters: {'learning_rate': 0.19534877448122945, 'num_leaves': 83, 'max_depth': 4, 'feature_fraction': 0.7673787868465876, 'bagging_fraction': 0.6939004297014277, 'bagging_freq': 8, 'min_data_in_leaf': 50, 'lambda_l1': 2.1992242946068887, 'lambda_l2': 2.3499983055169436}. Best is trial 6 with value: 0.9819046838609043.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping, best iteration is:\n",
      "[148]\tvalid_0's auc: 0.980882\n",
      "Training until validation scores don't improve for 20 rounds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-06-16 13:39:42,099] Trial 16 finished with value: 0.9819280468808482 and parameters: {'learning_rate': 0.07516228803155299, 'num_leaves': 73, 'max_depth': 14, 'feature_fraction': 0.6970798704970366, 'bagging_fraction': 0.9320033396410377, 'bagging_freq': 6, 'min_data_in_leaf': 72, 'lambda_l1': 3.308784229473275, 'lambda_l2': 1.3698182046203986}. Best is trial 16 with value: 0.9819280468808482.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping, best iteration is:\n",
      "[105]\tvalid_0's auc: 0.981928\n",
      "Training until validation scores don't improve for 20 rounds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-06-16 13:39:44,845] Trial 17 finished with value: 0.9821886460567716 and parameters: {'learning_rate': 0.07301812357461686, 'num_leaves': 48, 'max_depth': 14, 'feature_fraction': 0.7155135955928442, 'bagging_fraction': 0.9086445904209469, 'bagging_freq': 5, 'min_data_in_leaf': 89, 'lambda_l1': 3.4949973876244522, 'lambda_l2': 1.208701967098282}. Best is trial 17 with value: 0.9821886460567716.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping, best iteration is:\n",
      "[150]\tvalid_0's auc: 0.982189\n",
      "Training until validation scores don't improve for 20 rounds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-06-16 13:39:47,781] Trial 18 finished with value: 0.9821236631415047 and parameters: {'learning_rate': 0.07120926658823105, 'num_leaves': 128, 'max_depth': 14, 'feature_fraction': 0.6847552831015986, 'bagging_fraction': 0.9096579870016636, 'bagging_freq': 5, 'min_data_in_leaf': 89, 'lambda_l1': 3.553461754063702, 'lambda_l2': 1.1898074906131517}. Best is trial 17 with value: 0.9821886460567716.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping, best iteration is:\n",
      "[121]\tvalid_0's auc: 0.982124\n",
      "Training until validation scores don't improve for 20 rounds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-06-16 13:39:52,109] Trial 19 finished with value: 0.9816365837448516 and parameters: {'learning_rate': 0.0180536659820629, 'num_leaves': 128, 'max_depth': 14, 'feature_fraction': 0.6825316405094388, 'bagging_fraction': 0.880346398452855, 'bagging_freq': 3, 'min_data_in_leaf': 100, 'lambda_l1': 4.995757157326409, 'lambda_l2': 0.867643759262466}. Best is trial 17 with value: 0.9821886460567716.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Did not meet early stopping. Best iteration is:\n",
      "[194]\tvalid_0's auc: 0.981637\n",
      "Training until validation scores don't improve for 20 rounds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-06-16 13:39:55,230] Trial 20 finished with value: 0.981971655557545 and parameters: {'learning_rate': 0.06142619972849399, 'num_leaves': 104, 'max_depth': 11, 'feature_fraction': 0.7527198812349678, 'bagging_fraction': 0.9459183178103429, 'bagging_freq': 5, 'min_data_in_leaf': 89, 'lambda_l1': 3.7740883863086663, 'lambda_l2': 1.153374369782301}. Best is trial 17 with value: 0.9821886460567716.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping, best iteration is:\n",
      "[137]\tvalid_0's auc: 0.981972\n",
      "Training until validation scores don't improve for 20 rounds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-06-16 13:39:58,271] Trial 21 finished with value: 0.9821931053312705 and parameters: {'learning_rate': 0.06036389701688897, 'num_leaves': 108, 'max_depth': 12, 'feature_fraction': 0.7315057258291888, 'bagging_fraction': 0.9452917185973672, 'bagging_freq': 5, 'min_data_in_leaf': 86, 'lambda_l1': 3.9343696523899667, 'lambda_l2': 1.1977201179904}. Best is trial 21 with value: 0.9821931053312705.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping, best iteration is:\n",
      "[130]\tvalid_0's auc: 0.982193\n",
      "Training until validation scores don't improve for 20 rounds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-06-16 13:40:02,644] Trial 22 finished with value: 0.9819165202067963 and parameters: {'learning_rate': 0.03643383833661827, 'num_leaves': 128, 'max_depth': 13, 'feature_fraction': 0.6531814884699494, 'bagging_fraction': 0.8716271979954714, 'bagging_freq': 4, 'min_data_in_leaf': 84, 'lambda_l1': 3.4062835874945825, 'lambda_l2': 1.7874702913827083}. Best is trial 21 with value: 0.9821931053312705.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping, best iteration is:\n",
      "[175]\tvalid_0's auc: 0.981917\n",
      "Training until validation scores don't improve for 20 rounds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-06-16 13:40:05,193] Trial 23 finished with value: 0.9821404748816293 and parameters: {'learning_rate': 0.07071548249223611, 'num_leaves': 115, 'max_depth': 15, 'feature_fraction': 0.7252262801385408, 'bagging_fraction': 0.9422448745980904, 'bagging_freq': 4, 'min_data_in_leaf': 92, 'lambda_l1': 4.472783038988172, 'lambda_l2': 0.9227322752497279}. Best is trial 21 with value: 0.9821931053312705.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping, best iteration is:\n",
      "[91]\tvalid_0's auc: 0.98214\n",
      "Training until validation scores don't improve for 20 rounds\n",
      "Early stopping, best iteration is:\n",
      "[167]\tvalid_0's auc: 0.982151\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-06-16 13:40:18,672] Trial 24 finished with value: 0.982150618354795 and parameters: {'learning_rate': 0.03799223973194453, 'num_leaves': 105, 'max_depth': 15, 'feature_fraction': 0.8030930799039216, 'bagging_fraction': 0.9447810157081973, 'bagging_freq': 3, 'min_data_in_leaf': 93, 'lambda_l1': 4.326332010876492, 'lambda_l2': 0.6396657034399956}. Best is trial 21 with value: 0.9821931053312705.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training until validation scores don't improve for 20 rounds\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[185]\tvalid_0's auc: 0.982053\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-06-16 13:40:31,093] Trial 25 finished with value: 0.982052982264378 and parameters: {'learning_rate': 0.03496855995301919, 'num_leaves': 96, 'max_depth': 12, 'feature_fraction': 0.7965652868477087, 'bagging_fraction': 0.9617098769348824, 'bagging_freq': 3, 'min_data_in_leaf': 83, 'lambda_l1': 4.270230926544752, 'lambda_l2': 0.6858962674651341}. Best is trial 21 with value: 0.9821931053312705.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training until validation scores don't improve for 20 rounds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-06-16 13:40:40,615] Trial 26 finished with value: 0.9820149545624014 and parameters: {'learning_rate': 0.043125728856241205, 'num_leaves': 113, 'max_depth': 15, 'feature_fraction': 0.7888969219552971, 'bagging_fraction': 0.8199694075436176, 'bagging_freq': 2, 'min_data_in_leaf': 95, 'lambda_l1': 2.7607106996778215, 'lambda_l2': 1.9079997928032837}. Best is trial 21 with value: 0.9821931053312705.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping, best iteration is:\n",
      "[97]\tvalid_0's auc: 0.982015\n",
      "Training until validation scores don't improve for 20 rounds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-06-16 13:40:50,174] Trial 27 finished with value: 0.9808949128059752 and parameters: {'learning_rate': 0.023938042123110605, 'num_leaves': 43, 'max_depth': 12, 'feature_fraction': 0.826589014889469, 'bagging_fraction': 0.8795328082874658, 'bagging_freq': 6, 'min_data_in_leaf': 81, 'lambda_l1': 4.906562978629591, 'lambda_l2': 1.4654134904414526}. Best is trial 21 with value: 0.9821931053312705.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Did not meet early stopping. Best iteration is:\n",
      "[200]\tvalid_0's auc: 0.980895\n",
      "Training until validation scores don't improve for 20 rounds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-06-16 13:40:58,043] Trial 28 finished with value: 0.9820798136089323 and parameters: {'learning_rate': 0.10065188835482072, 'num_leaves': 91, 'max_depth': 13, 'feature_fraction': 0.9321457434267331, 'bagging_fraction': 0.9690578237315238, 'bagging_freq': 2, 'min_data_in_leaf': 78, 'lambda_l1': 3.74392153379715, 'lambda_l2': 2.255676162023393}. Best is trial 21 with value: 0.9821931053312705.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping, best iteration is:\n",
      "[103]\tvalid_0's auc: 0.98208\n",
      "Training until validation scores don't improve for 20 rounds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-06-16 13:41:05,759] Trial 29 finished with value: 0.9812666291197589 and parameters: {'learning_rate': 0.05208865322098913, 'num_leaves': 106, 'max_depth': 11, 'feature_fraction': 0.7193254471863219, 'bagging_fraction': 0.6116015187728692, 'bagging_freq': 4, 'min_data_in_leaf': 95, 'lambda_l1': 4.098335408378678, 'lambda_l2': 0.4990906467340883}. Best is trial 21 with value: 0.9821931053312705.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping, best iteration is:\n",
      "[99]\tvalid_0's auc: 0.981267\n",
      "Best params: {'learning_rate': 0.06036389701688897, 'num_leaves': 108, 'max_depth': 12, 'feature_fraction': 0.7315057258291888, 'bagging_fraction': 0.9452917185973672, 'bagging_freq': 5, 'min_data_in_leaf': 86, 'lambda_l1': 3.9343696523899667, 'lambda_l2': 1.1977201179904}\n"
     ]
    }
   ],
   "source": [
    "def objective(trial):\n",
    "    params = {\n",
    "        \"objective\": \"binary\",\n",
    "        \"metric\": \"auc\",\n",
    "        \"boosting_type\": \"gbdt\",\n",
    "        \"verbosity\": -1,\n",
    "        \"learning_rate\": trial.suggest_float(\"learning_rate\", 0.01, 0.2),\n",
    "        \"num_leaves\": trial.suggest_int(\"num_leaves\", 16, 128),\n",
    "        \"max_depth\": trial.suggest_int(\"max_depth\", 4, 16),\n",
    "        \"feature_fraction\": trial.suggest_float(\"feature_fraction\", 0.5, 1.0),\n",
    "        \"bagging_fraction\": trial.suggest_float(\"bagging_fraction\", 0.5, 1.0),\n",
    "        \"bagging_freq\": trial.suggest_int(\"bagging_freq\", 1, 10),\n",
    "        \"min_data_in_leaf\": trial.suggest_int(\"min_data_in_leaf\", 20, 100),\n",
    "        \"lambda_l1\": trial.suggest_float(\"lambda_l1\", 0.0, 5.0),\n",
    "        \"lambda_l2\": trial.suggest_float(\"lambda_l2\", 0.0, 5.0),\n",
    "        \"scale_pos_weight\": float(np.sum(y_train == 0)) / np.sum(y_train == 1),  # handle imbalance\n",
    "    }\n",
    "    dtrain = lgb.Dataset(X_train, label=y_train)\n",
    "    dval = lgb.Dataset(X_test, label=y_test)\n",
    "    booster = lgb.train(\n",
    "        params, dtrain, valid_sets=[dval],\n",
    "        num_boost_round=200,\n",
    "        callbacks=[lgb.early_stopping(stopping_rounds=20), lgb.log_evaluation(0)]  # disables logging, use 100 for every 100 rounds\n",
    "    )\n",
    "\n",
    "    preds = booster.predict(X_test)\n",
    "    auc = roc_auc_score(y_test, preds)\n",
    "    return auc\n",
    "\n",
    "# Run Optuna\n",
    "study = optuna.create_study(direction=\"maximize\")\n",
    "study.optimize(objective, n_trials=30)\n",
    "print(\"Best params:\", study.best_params)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "48950b4e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training until validation scores don't improve for 20 rounds\n",
      "[20]\tvalid_0's auc: 0.97785\n",
      "[40]\tvalid_0's auc: 0.979886\n",
      "[60]\tvalid_0's auc: 0.981022\n",
      "[80]\tvalid_0's auc: 0.981625\n",
      "[100]\tvalid_0's auc: 0.981836\n",
      "[120]\tvalid_0's auc: 0.982084\n",
      "[140]\tvalid_0's auc: 0.982041\n",
      "Early stopping, best iteration is:\n",
      "[122]\tvalid_0's auc: 0.982093\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<lightgbm.basic.Booster at 0x2052e62ca10>"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Use best params and train final model\n",
    "best_params = study.best_params\n",
    "best_params[\"objective\"] = \"binary\"\n",
    "best_params[\"metric\"] = \"auc\"\n",
    "best_params[\"verbosity\"] = -1\n",
    "\n",
    "dtrain = lgb.Dataset(X_train, label=y_train)\n",
    "dval = lgb.Dataset(X_test, label=y_test)\n",
    "booster = lgb.train(\n",
    "    best_params,\n",
    "    dtrain,\n",
    "    valid_sets=[dval],\n",
    "    num_boost_round=200,\n",
    "    callbacks=[\n",
    "        lgb.early_stopping(stopping_rounds=20),\n",
    "        lgb.log_evaluation(20)\n",
    "    ]\n",
    ")\n",
    "\n",
    "\n",
    "# Save the model\n",
    "booster.save_model('final_lightgbm_tuned.txt')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "39126cef",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AUC: 0.982093115734868\n",
      "Confusion Matrix:\n",
      " [[15148    81]\n",
      " [  655  4116]]\n",
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.96      0.99      0.98     15229\n",
      "           1       0.98      0.86      0.92      4771\n",
      "\n",
      "    accuracy                           0.96     20000\n",
      "   macro avg       0.97      0.93      0.95     20000\n",
      "weighted avg       0.96      0.96      0.96     20000\n",
      "\n"
     ]
    }
   ],
   "source": [
    "pred_probs = booster.predict(X_test)\n",
    "threshold = 0.5\n",
    "preds = (pred_probs > threshold).astype(int)\n",
    "\n",
    "print(\"AUC:\", roc_auc_score(y_test, pred_probs))\n",
    "print(\"Confusion Matrix:\\n\", confusion_matrix(y_test, preds))\n",
    "print(\"Classification Report:\\n\", classification_report(y_test, preds))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "2fbd8234",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<lightgbm.basic.Booster at 0x2052e62ca10>"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "booster.save_model('final_lightgbm_tuned.txt')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "c4e98e6c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AUC: 0.9999134772008106\n",
      "Confusion Matrix:\n",
      " [[762237    147]\n",
      " [   322 285869]]\n",
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00    762384\n",
      "           1       1.00      1.00      1.00    286191\n",
      "\n",
      "    accuracy                           1.00   1048575\n",
      "   macro avg       1.00      1.00      1.00   1048575\n",
      "weighted avg       1.00      1.00      1.00   1048575\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import lightgbm as lgb\n",
    "from sklearn.metrics import roc_auc_score, confusion_matrix, classification_report\n",
    "\n",
    "# 1. Load the test dataset\n",
    "df = pd.read_parquet('D:/Canada/Subjects/Semester -2/AIDI-2005-02 CAPSTONE TERM ll/Bot_detector/data/cleaned/Friday-02-03-2018_TrafficForML_CICFlowMeter.parquet')  # Change path as needed\n",
    "\n",
    "# 2. Encode the 'Label' column as 0 (Benign) and 1 (Attack)\n",
    "df['Label'] = df['Label'].replace({'Benign': 0, 'Attack': 1}).astype(int)\n",
    "\n",
    "# 3. Separate features and label\n",
    "X_test = df.drop(columns=['Label'])\n",
    "y_test = df['Label']\n",
    "\n",
    "# 4. Optional: Keep only numeric columns\n",
    "X_test = X_test.select_dtypes(include=np.number)\n",
    "\n",
    "# 5. Load the pretrained LightGBM model\n",
    "booster = lgb.Booster(model_file='final_lightgbm_tuned.txt')  # Change filename if different\n",
    "\n",
    "# 6. Predict probabilities and classes\n",
    "y_pred_probs = booster.predict(X_test)\n",
    "y_pred = (y_pred_probs >= 0.5).astype(int)  # You can adjust threshold if needed\n",
    "\n",
    "# 7. Evaluate the results\n",
    "auc = roc_auc_score(y_test, y_pred_probs)\n",
    "cm = confusion_matrix(y_test, y_pred)\n",
    "report = classification_report(y_test, y_pred)\n",
    "\n",
    "print(f\"AUC: {auc}\")\n",
    "print(\"Confusion Matrix:\\n\", cm)\n",
    "print(\"Classification Report:\\n\", report)\n",
    "\n",
    "# 8. (Optional) Save results to CSV for further analysis\n",
    "results = X_test.copy()\n",
    "results['Actual'] = y_test\n",
    "results['Predicted'] = y_pred\n",
    "results['Pred_Prob'] = y_pred_probs\n",
    "results.to_csv('test_results.csv', index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "877d3f4f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted probability of attack: 0.1843\n",
      "Predicted class: Benign\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import lightgbm as lgb\n",
    "\n",
    "# 1. Feature order (from your list)\n",
    "feature_names = [\n",
    "    'Dst Port', 'Protocol', 'Flow Duration', 'Tot Fwd Pkts', 'Tot Bwd Pkts', 'TotLen Fwd Pkts', 'TotLen Bwd Pkts',\n",
    "    'Fwd Pkt Len Max', 'Fwd Pkt Len Min', 'Fwd Pkt Len Mean', 'Fwd Pkt Len Std', 'Bwd Pkt Len Max', 'Bwd Pkt Len Min',\n",
    "    'Bwd Pkt Len Mean', 'Bwd Pkt Len Std', 'Flow Byts/s', 'Flow Pkts/s', 'Flow IAT Mean', 'Flow IAT Std',\n",
    "    'Flow IAT Max', 'Flow IAT Min', 'Fwd IAT Tot', 'Fwd IAT Mean', 'Fwd IAT Std', 'Fwd IAT Max', 'Fwd IAT Min',\n",
    "    'Bwd IAT Tot', 'Bwd IAT Mean', 'Bwd IAT Std', 'Bwd IAT Max', 'Bwd IAT Min', 'Fwd PSH Flags', 'Bwd PSH Flags',\n",
    "    'Fwd URG Flags', 'Bwd URG Flags', 'Fwd Header Len', 'Bwd Header Len', 'Fwd Pkts/s', 'Bwd Pkts/s', 'Pkt Len Min',\n",
    "    'Pkt Len Max', 'Pkt Len Mean', 'Pkt Len Std', 'Pkt Len Var', 'FIN Flag Cnt', 'SYN Flag Cnt', 'RST Flag Cnt',\n",
    "    'PSH Flag Cnt', 'ACK Flag Cnt', 'URG Flag Cnt', 'CWE Flag Count', 'ECE Flag Cnt', 'Down/Up Ratio', 'Pkt Size Avg',\n",
    "    'Fwd Seg Size Avg', 'Bwd Seg Size Avg', 'Fwd Byts/b Avg', 'Fwd Pkts/b Avg', 'Fwd Blk Rate Avg', 'Bwd Byts/b Avg',\n",
    "    'Bwd Pkts/b Avg', 'Bwd Blk Rate Avg', 'Subflow Fwd Pkts', 'Subflow Fwd Byts', 'Subflow Bwd Pkts', 'Subflow Bwd Byts',\n",
    "    'Init Fwd Win Byts', 'Init Bwd Win Byts', 'Fwd Act Data Pkts', 'Fwd Seg Size Min', 'Active Mean', 'Active Std',\n",
    "    'Active Max', 'Active Min', 'Idle Mean', 'Idle Std', 'Idle Max', 'Idle Min'\n",
    "]\n",
    "\n",
    "# 2. Example attack values (based on common attack patterns, fill the rest as zeros if unsure)\n",
    "attack_values = [\n",
    "    80,    # Dst Port (common HTTP port)\n",
    "    6,     # Protocol (TCP)\n",
    "    200000, # Flow Duration (long connection)\n",
    "    500,   # Tot Fwd Pkts\n",
    "    1200,  # Tot Bwd Pkts\n",
    "    100000, # TotLen Fwd Pkts\n",
    "    300000, # TotLen Bwd Pkts\n",
    "    1500,  # Fwd Pkt Len Max\n",
    "    0, 0, 0,  # ... Min, Mean, Std (defaults)\n",
    "    2000, 0, 0, 0,  # Bwd Pkt\n",
    "    1e6, 200, 5000, 500, 9000, 1000, 50000, 1000, 12000, 100, 250000, 5000, 9000, 100, 500, 0, 0, 0, 0, 40, 40,\n",
    "    10, 2000, 1200, 300, 80000, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 100, 500, 300, 200, 0, 0, 0, 0, 0, 0, 500, 50000, 200, 80000, 1024, 2048, 0, 40, 100, 0, 0, 0, 0, 0, 0\n",
    "]\n",
    "# Fill missing values with zeros if <78\n",
    "if len(attack_values) < 78:\n",
    "    attack_values += [0] * (78 - len(attack_values))\n",
    "\n",
    "# 3. Convert to DataFrame\n",
    "attack_df = pd.DataFrame([attack_values], columns=feature_names)\n",
    "\n",
    "# 4. Load your pre-trained model\n",
    "booster = lgb.Booster(model_file='final_lightgbm_tuned.txt')  # Change filename if needed\n",
    "\n",
    "# 5. Predict\n",
    "prob = booster.predict(attack_df)[0]\n",
    "pred = int(prob >= 0.5)\n",
    "\n",
    "print(f\"Predicted probability of attack: {prob:.4f}\")\n",
    "print(f\"Predicted class: {'Attack' if pred == 1 else 'Benign'}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "6c69fd59",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Load your data (already encoded with 0/1 in 'Label')\n",
    "df = pd.read_parquet('D:/Canada/Subjects/Semester -2/AIDI-2005-02 CAPSTONE TERM ll/Bot_detector/data/cleaned/Friday-02-03-2018_TrafficForML_CICFlowMeter.parquet')\n",
    "\n",
    "# Select feature columns (remove 'Label')\n",
    "feature_cols = [col for col in df.columns if col != 'Label']\n",
    "\n",
    "# 1. Sample a Benign flow\n",
    "benign_sample = df[df['Label'] == 0].sample(n=1, random_state=42)[feature_cols]\n",
    "\n",
    "# 2. Sample an Attack flow\n",
    "attack_sample = df[df['Label'] == 1].sample(n=1, random_state=24)[feature_cols]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "c2624d29",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Benign sample: Predicted probability of attack = 0.0088 | Predicted class: Benign\n",
      "Attack sample: Predicted probability of attack = 0.9967 | Predicted class: Attack\n"
     ]
    }
   ],
   "source": [
    "# Predict probability and class for Benign\n",
    "benign_prob = booster.predict(benign_sample)[0]\n",
    "benign_class = int(benign_prob >= 0.5)\n",
    "print(f\"Benign sample: Predicted probability of attack = {benign_prob:.4f} | Predicted class: {'Attack' if benign_class else 'Benign'}\")\n",
    "\n",
    "# Predict probability and class for Attack\n",
    "attack_prob = booster.predict(attack_sample)[0]\n",
    "attack_class = int(attack_prob >= 0.5)\n",
    "print(f\"Attack sample: Predicted probability of attack = {attack_prob:.4f} | Predicted class: {'Attack' if attack_class else 'Benign'}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "250cb32a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Benign (real) sample: Prob = 0.1488 | Predicted: Benign\n",
      "Attack (real) sample: Prob = 0.9980 | Predicted: Attack\n",
      "Attack (synthetic extreme) sample: Prob = 0.0381 | Predicted: Benign\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import lightgbm as lgb\n",
    "\n",
    "# -------- 1. Load Pretrained Model --------\n",
    "booster = lgb.Booster(model_file='final_lightgbm_tuned.txt')\n",
    "\n",
    "# -------- 2. Load Data --------\n",
    "# Use the same file you trained on, or another file in same format\n",
    "df = pd.read_parquet('D:/Canada/Subjects/Semester -2/AIDI-2005-02 CAPSTONE TERM ll/Bot_detector/data/cleaned/Friday-02-03-2018_TrafficForML_CICFlowMeter.parquet')\n",
    "\n",
    "# Make sure Label is encoded (for testing with real samples)\n",
    "df['Label'] = df['Label'].replace({'Benign': 0, 'Attack': 1}).astype(int)\n",
    "\n",
    "# Feature names as used for training\n",
    "feature_names = [\n",
    "    'Dst Port', 'Protocol', 'Flow Duration', 'Tot Fwd Pkts', 'Tot Bwd Pkts',\n",
    "    'TotLen Fwd Pkts', 'TotLen Bwd Pkts', 'Fwd Pkt Len Max', 'Fwd Pkt Len Min', 'Fwd Pkt Len Mean', 'Fwd Pkt Len Std',\n",
    "    'Bwd Pkt Len Max', 'Bwd Pkt Len Min', 'Bwd Pkt Len Mean', 'Bwd Pkt Len Std', 'Flow Byts/s', 'Flow Pkts/s',\n",
    "    'Flow IAT Mean', 'Flow IAT Std', 'Flow IAT Max', 'Flow IAT Min', 'Fwd IAT Tot', 'Fwd IAT Mean', 'Fwd IAT Std',\n",
    "    'Fwd IAT Max', 'Fwd IAT Min', 'Bwd IAT Tot', 'Bwd IAT Mean', 'Bwd IAT Std', 'Bwd IAT Max', 'Bwd IAT Min',\n",
    "    'Fwd PSH Flags', 'Bwd PSH Flags', 'Fwd URG Flags', 'Bwd URG Flags', 'Fwd Header Len', 'Bwd Header Len',\n",
    "    'Fwd Pkts/s', 'Bwd Pkts/s', 'Pkt Len Min', 'Pkt Len Max', 'Pkt Len Mean', 'Pkt Len Std', 'Pkt Len Var',\n",
    "    'FIN Flag Cnt', 'SYN Flag Cnt', 'RST Flag Cnt', 'PSH Flag Cnt', 'ACK Flag Cnt', 'URG Flag Cnt',\n",
    "    'CWE Flag Count', 'ECE Flag Cnt', 'Down/Up Ratio', 'Pkt Size Avg', 'Fwd Seg Size Avg', 'Bwd Seg Size Avg',\n",
    "    'Fwd Byts/b Avg', 'Fwd Pkts/b Avg', 'Fwd Blk Rate Avg', 'Bwd Byts/b Avg', 'Bwd Pkts/b Avg', 'Bwd Blk Rate Avg',\n",
    "    'Subflow Fwd Pkts', 'Subflow Fwd Byts', 'Subflow Bwd Pkts', 'Subflow Bwd Byts', 'Init Fwd Win Byts',\n",
    "    'Init Bwd Win Byts', 'Fwd Act Data Pkts', 'Fwd Seg Size Min', 'Active Mean', 'Active Std', 'Active Max',\n",
    "    'Active Min', 'Idle Mean', 'Idle Std', 'Idle Max', 'Idle Min'\n",
    "]\n",
    "\n",
    "# -------- 3. Sample Real Benign & Attack --------\n",
    "\n",
    "# Grab a real benign row\n",
    "benign_row = df[df['Label'] == 0].iloc[0][feature_names].to_frame().T\n",
    "\n",
    "# Grab a real attack row\n",
    "attack_row = df[df['Label'] == 1].iloc[0][feature_names].to_frame().T\n",
    "\n",
    "# -------- 4. Create Synthetic \"Extreme Attack\" --------\n",
    "\n",
    "extreme_attack = pd.DataFrame([[\n",
    "    8080,   # Dst Port (unusual)\n",
    "    17,     # Protocol (UDP)\n",
    "    800000, # Flow Duration\n",
    "    5000,   # Tot Fwd Pkts\n",
    "    9000,   # Tot Bwd Pkts\n",
    "    2_000_000, # TotLen Fwd Pkts\n",
    "    2_200_000, # TotLen Bwd Pkts\n",
    "    *([1000] * (78 - 7))  # Make other features large (or adjust as you wish)\n",
    "]], columns=feature_names)\n",
    "\n",
    "# -------- 5. Predict --------\n",
    "\n",
    "for name, sample in [('Benign (real)', benign_row),\n",
    "                     ('Attack (real)', attack_row),\n",
    "                     ('Attack (synthetic extreme)', extreme_attack)]:\n",
    "    prob = booster.predict(sample)[0]\n",
    "    pred = 'Attack' if prob >= 0.5 else 'Benign'\n",
    "    print(f\"{name} sample: Prob = {prob:.4f} | Predicted: {pred}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "cbda7ee9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Wrote app/models/classifier.pkl\n"
     ]
    }
   ],
   "source": [
    "#!/usr/bin/env python3\n",
    "\"\"\"\n",
    "scripts/make_classifier_pkl.py\n",
    "\n",
    "Loads your trained LightGBM model (final_lightgbm_tuned.txt),\n",
    "wraps it in an sklearn‐compatible interface, and saves it\n",
    "to app/models/classifier.pkl so your FastAPI and Kafka‐scorer\n",
    "can load it via joblib.load().\n",
    "\"\"\"\n",
    "\n",
    "import os\n",
    "import joblib\n",
    "import numpy as np\n",
    "import lightgbm as lgb\n",
    "\n",
    "# 1) Load your LightGBM booster from disk\n",
    "MODEL_FILE = \"final_lightgbm_tuned.txt\"\n",
    "booster = lgb.Booster(model_file=MODEL_FILE)\n",
    "\n",
    "# 2) Define the exact feature order you trained on:\n",
    "feature_names = [\n",
    "    'Dst Port', 'Protocol', 'Flow Duration', 'Tot Fwd Pkts', 'Tot Bwd Pkts',\n",
    "    'TotLen Fwd Pkts', 'TotLen Bwd Pkts', 'Fwd Pkt Len Max', 'Fwd Pkt Len Min',\n",
    "    'Fwd Pkt Len Mean', 'Fwd Pkt Len Std', 'Bwd Pkt Len Max', 'Bwd Pkt Len Min',\n",
    "    'Bwd Pkt Len Mean', 'Bwd Pkt Len Std', 'Flow Byts/s', 'Flow Pkts/s',\n",
    "    'Flow IAT Mean', 'Flow IAT Std', 'Flow IAT Max', 'Flow IAT Min',\n",
    "    'Fwd IAT Tot', 'Fwd IAT Mean', 'Fwd IAT Std', 'Fwd IAT Max', 'Fwd IAT Min',\n",
    "    'Bwd IAT Tot', 'Bwd IAT Mean', 'Bwd IAT Std', 'Bwd IAT Max', 'Bwd IAT Min',\n",
    "    'Fwd PSH Flags', 'Bwd PSH Flags', 'Fwd URG Flags', 'Bwd URG Flags',\n",
    "    'Fwd Header Len', 'Bwd Header Len', 'Fwd Pkts/s', 'Bwd Pkts/s',\n",
    "    'Pkt Len Min', 'Pkt Len Max', 'Pkt Len Mean', 'Pkt Len Std', 'Pkt Len Var',\n",
    "    'FIN Flag Cnt', 'SYN Flag Cnt', 'RST Flag Cnt', 'PSH Flag Cnt',\n",
    "    'ACK Flag Cnt', 'URG Flag Cnt', 'CWE Flag Count', 'ECE Flag Cnt',\n",
    "    'Down/Up Ratio', 'Pkt Size Avg', 'Fwd Seg Size Avg', 'Bwd Seg Size Avg',\n",
    "    'Fwd Byts/b Avg', 'Fwd Pkts/b Avg', 'Fwd Blk Rate Avg', 'Bwd Byts/b Avg',\n",
    "    'Bwd Pkts/b Avg', 'Bwd Blk Rate Avg', 'Subflow Fwd Pkts', 'Subflow Fwd Byts',\n",
    "    'Subflow Bwd Pkts', 'Subflow Bwd Byts', 'Init Fwd Win Byts', 'Init Bwd Win Byts',\n",
    "    'Fwd Act Data Pkts', 'Fwd Seg Size Min', 'Active Mean', 'Active Std',\n",
    "    'Active Max', 'Active Min', 'Idle Mean', 'Idle Std', 'Idle Max', 'Idle Min'\n",
    "]\n",
    "\n",
    "# 3) Wrap in an sklearn‐compatible interface\n",
    "class LGBMWrapper:\n",
    "    def __init__(self, booster, feature_names):\n",
    "        self.booster = booster\n",
    "        self.feature_names = feature_names\n",
    "\n",
    "    def predict_proba(self, X):\n",
    "        \"\"\"\n",
    "        X: pandas.DataFrame or 2D numpy array.\n",
    "        Returns an (n_samples, 2) array: [[P(class0), P(class1)], ...].\n",
    "        \"\"\"\n",
    "        # If it's a DataFrame, reorder columns\n",
    "        try:\n",
    "            Xmat = X[self.feature_names].values.astype(np.float32)\n",
    "        except Exception:\n",
    "            # assume it's already a numpy array\n",
    "            Xmat = np.asarray(X, dtype=np.float32)\n",
    "        p1 = self.booster.predict(Xmat)\n",
    "        p0 = 1.0 - p1\n",
    "        return np.vstack([p0, p1]).T\n",
    "\n",
    "    def predict(self, X, threshold=0.5):\n",
    "        \"\"\"\n",
    "        Returns binary predictions (0 or 1) for class1 at given threshold.\n",
    "        \"\"\"\n",
    "        proba = self.predict_proba(X)[:, 1]\n",
    "        return (proba >= threshold).astype(int)\n",
    "\n",
    "# 4) Instantiate and dump to classifier.pkl\n",
    "clf = LGBMWrapper(booster, feature_names)\n",
    "\n",
    "# Ensure output directory exists\n",
    "os.makedirs(\"app/models\", exist_ok=True)\n",
    "\n",
    "# Save with joblib so FastAPI can load via joblib.load()\n",
    "joblib.dump(clf, \"app/models/classifier.pkl\")\n",
    "print(\"✅ Wrote app/models/classifier.pkl\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8b2afc35",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   time_to_submit                                         user_agent  \\\n",
      "0        8.089360                                         curl/8.5.0   \n",
      "1        0.220738                                         curl/8.5.0   \n",
      "2        9.482393  Mozilla/5.0 (Windows NT 10.0; Win64; x64) Chro...   \n",
      "3        0.507608  Mozilla/5.0 (Windows NT 10.0; Win64; x64) Chro...   \n",
      "4        8.934762                                 go-http-client/1.1   \n",
      "\n",
      "   login_hour    client_ip  password_length  failed_login_count_last_10min  \\\n",
      "0          19   51.15.39.9                9                              0   \n",
      "1           8    45.77.1.3               14                              2   \n",
      "2          16   51.15.39.9                6                              0   \n",
      "3          12  203.0.113.2               14                              2   \n",
      "4           5   51.15.39.9                7                              0   \n",
      "\n",
      "   is_username_email  label  \n",
      "0                  1    0.0  \n",
      "1                  1    1.0  \n",
      "2                  0    0.0  \n",
      "3                  1    1.0  \n",
      "4                  1    0.0  \n",
      "\n",
      "✅ Saved to synthetic_login_data.csv\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "N = 2000  # number of synthetic logins\n",
    "\n",
    "np.random.seed(42)\n",
    "data = {\n",
    "    \"time_to_submit\": np.concatenate([\n",
    "        np.random.uniform(1.5, 12, N//2),     # humans: slower\n",
    "        np.random.uniform(0.02, 0.6, N//2)    # bots: very fast\n",
    "    ]),\n",
    "    \"user_agent\": np.random.choice([\n",
    "        \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) Chrome/119.0.0.0\",\n",
    "        \"Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) Firefox/118.0\",\n",
    "        \"curl/8.5.0\", \"python-requests/2.28.1\", \"go-http-client/1.1\"\n",
    "    ], N),\n",
    "    \"login_hour\": np.random.choice(range(24), N),\n",
    "    \"client_ip\": np.random.choice([\n",
    "        \"203.0.113.1\", \"203.0.113.2\", \"45.77.1.3\", \"51.15.39.9\", \"192.0.2.1\"\n",
    "    ], N),\n",
    "    \"password_length\": np.random.choice(range(6, 16), N),\n",
    "    \"failed_login_count_last_10min\": np.concatenate([\n",
    "        np.random.poisson(0.4, N//2),    # humans: low fails\n",
    "        np.random.poisson(3.5, N//2)     # bots: many fails\n",
    "    ]),\n",
    "    \"is_username_email\": np.random.choice([0, 1], N),\n",
    "    \"label\": np.concatenate([\n",
    "        np.zeros(N//2),  # human\n",
    "        np.ones(N//2)    # attack\n",
    "    ])\n",
    "}\n",
    "\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "# Optionally, shuffle the rows\n",
    "df = df.sample(frac=1).reset_index(drop=True)\n",
    "\n",
    "print(df.head())\n",
    "df.to_csv(\"synthetic_login_data.csv\", index=False)\n",
    "print(\"\\n✅ Saved to synthetic_login_data.csv\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
